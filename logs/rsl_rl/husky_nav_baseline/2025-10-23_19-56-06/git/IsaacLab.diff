--- git status ---
On branch main
Your branch is ahead of 'origin/main' by 44 commits.
  (use "git push" to publish your local commits)

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	deleted:    VERSION
	modified:   scripts/tutorials/01_assets/add_new_robot.py
	modified:   scripts/tutorials/01_assets/run_articulation.py
	modified:   source/isaaclab/isaaclab/envs/__pycache__/common.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/__pycache__/direct_rl_env.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/mdp/__pycache__/events.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/mdp/actions/__pycache__/actions_cfg.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/commands_cfg.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/pose_command.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/envs/mdp/commands/pose_command.py
	modified:   source/isaaclab/isaaclab/envs/mdp/events.py
	modified:   source/isaaclab/isaaclab/scene/__pycache__/interactive_scene.cpython-311.pyc
	modified:   source/isaaclab/isaaclab/scene/interactive_scene.py
	modified:   source/isaaclab_assets/isaaclab_assets/robots/__pycache__/__init__.cpython-311.pyc
	modified:   source/isaaclab_assets/isaaclab_assets/robots/aloha.py
	modified:   source/isaaclab_tasks/isaaclab_tasks.egg-info/PKG-INFO
	modified:   source/isaaclab_tasks/isaaclab_tasks.egg-info/SOURCES.txt
	modified:   source/isaaclab_tasks/isaaclab_tasks.egg-info/requires.txt
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__init__.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/__init__.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/aloha_env.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/asset_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager_cur.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/evaluation_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/graph_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/memory_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/obstacle_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/path_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies_for_pg.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager_for_pg.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__init__.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__pycache__/__init__.cpython-310.pyc
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_ppo_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy 2.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rsl_rl_ppo_cfg.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg_camera.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_sac_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_ppo_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg_old.yaml
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 2.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 3.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env_img.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/asset_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/config_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager copy.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/custom_runner.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/eval_scenes.json
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/evaluation_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/memory_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_generator.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies_for_pg.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items copy.json
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items_cutted.json
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager copy.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_for_pg.py
	deleted:    source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_img.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/direct/ant/__pycache__/__init__.cpython-311.pyc
	modified:   source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/__pycache__/__init__.cpython-311.pyc
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__init__.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__pycache__/__init__.cpython-311.pyc
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/__pycache__/__init__.cpython-311.pyc

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	Gripper_Robotiq_2F_140_physics_edit.usd
	Gripper_ur5e.usd
	Mobile_Husky_ur5.usd
	data/data/
	logs/rsl_rl/anymal_c_navigation/
	logs/rsl_rl/digit_loco_manip/
	logs/rsl_rl/husky_nav_baseline/
	logs/skrl/
	mm_Robotiq_2F_140_physics_edit.usd
	mobile_husky_ur5.usd
	mobile_ur5_ur5.usd
	mobile_ur5_ur5_01.usd
	models/__init__.py
	models/__pycache__/__init__.cpython-311.pyc
	models/__pycache__/custom_gmodel.cpython-311.pyc
	outputs/2025-09-17/20-22-02/
	outputs/2025-09-17/20-30-12/
	outputs/2025-09-17/20-39-54/
	outputs/2025-09-17/20-53-40/
	outputs/2025-09-17/20-57-18/
	outputs/2025-09-17/21-23-57/
	outputs/2025-09-18/
	outputs/2025-09-28/
	outputs/2025-10-16/
	outputs/2025-10-19/
	outputs/2025-10-20/
	outputs/2025-10-23/
	scripts/tutorials/01_assets/add_mm.py
	scripts/tutorials/01_assets/robots/
	scripts/tutorials/jetbot/
	skrl/__pycache__/__init__.cpython-311.pyc
	skrl/agents/__pycache__/__init__.cpython-311.pyc
	skrl/agents/torch/__pycache__/__init__.cpython-311.pyc
	skrl/agents/torch/__pycache__/base.cpython-311.pyc
	skrl/envs/__pycache__/__init__.cpython-311.pyc
	skrl/envs/wrappers/__pycache__/__init__.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/__init__.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/base.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/bidexhands_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/brax_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/deepmind_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/gym_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/gymnasium_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/isaacgym_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/isaaclab_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/omniverse_isaacgym_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/pettingzoo_envs.cpython-311.pyc
	skrl/envs/wrappers/torch/__pycache__/robosuite_envs.cpython-311.pyc
	skrl/memories/__pycache__/__init__.cpython-311.pyc
	skrl/memories/torch/__pycache__/__init__.cpython-311.pyc
	skrl/memories/torch/__pycache__/base.cpython-311.pyc
	skrl/memories/torch/__pycache__/random.cpython-311.pyc
	skrl/models/__pycache__/__init__.cpython-311.pyc
	skrl/models/torch/__pycache__/__init__.cpython-311.pyc
	skrl/models/torch/__pycache__/base.cpython-311.pyc
	skrl/models/torch/__pycache__/categorical.cpython-311.pyc
	skrl/models/torch/__pycache__/deterministic.cpython-311.pyc
	skrl/models/torch/__pycache__/gaussian.cpython-311.pyc
	skrl/models/torch/__pycache__/multicategorical.cpython-311.pyc
	skrl/models/torch/__pycache__/multivariate_gaussian.cpython-311.pyc
	skrl/models/torch/__pycache__/tabular.cpython-311.pyc
	skrl/resources/__pycache__/__init__.cpython-311.pyc
	skrl/resources/noises/__pycache__/__init__.cpython-311.pyc
	skrl/resources/noises/torch/__pycache__/__init__.cpython-311.pyc
	skrl/resources/noises/torch/__pycache__/base.cpython-311.pyc
	skrl/resources/noises/torch/__pycache__/gaussian.cpython-311.pyc
	skrl/resources/noises/torch/__pycache__/ornstein_uhlenbeck.cpython-311.pyc
	skrl/resources/preprocessors/__pycache__/__init__.cpython-311.pyc
	skrl/resources/preprocessors/torch/__pycache__/__init__.cpython-311.pyc
	skrl/resources/preprocessors/torch/__pycache__/running_standard_scaler.cpython-311.pyc
	skrl/resources/schedulers/__pycache__/__init__.cpython-311.pyc
	skrl/resources/schedulers/torch/__pycache__/__init__.cpython-311.pyc
	skrl/resources/schedulers/torch/__pycache__/kl_adaptive.cpython-311.pyc
	skrl/trainers/__pycache__/__init__.cpython-311.pyc
	skrl/trainers/torch/__pycache__/__init__.cpython-311.pyc
	skrl/trainers/torch/__pycache__/base.cpython-311.pyc
	skrl/trainers/torch/__pycache__/parallel.cpython-311.pyc
	skrl/trainers/torch/__pycache__/sequential.cpython-311.pyc
	skrl/trainers/torch/__pycache__/step.cpython-311.pyc
	skrl/utils/__pycache__/__init__.cpython-311.pyc
	skrl/utils/runner/__pycache__/__init__.cpython-311.pyc
	skrl/utils/runner/torch/__pycache__/__init__.cpython-311.pyc
	skrl/utils/runner/torch/__pycache__/runner.cpython-311.pyc
	skrl/utils/spaces/__pycache__/__init__.cpython-311.pyc
	skrl/utils/spaces/torch/__pycache__/__init__.cpython-311.pyc
	skrl/utils/spaces/torch/__pycache__/spaces.cpython-311.pyc
	source/isaaclab/isaaclab/terrains/config/__pycache__/
	source/isaaclab/isaaclab/utils/__pycache__/pretrained_checkpoint.cpython-311.pyc
	source/isaaclab_assets/data/aloha_assets/
	source/isaaclab_assets/data/husky_asset/
	source/isaaclab_assets/data/ur5_assets/
	source/isaaclab_assets/isaaclab_assets/robots/__pycache__/agility.cpython-311.pyc
	source/isaaclab_assets/isaaclab_assets/robots/__pycache__/aloha.cpython-311.pyc
	source/isaaclab_assets/isaaclab_assets/robots/__pycache__/ur5_husky.cpython-311.pyc
	source/isaaclab_assets/isaaclab_assets/robots/ur5_husky.py
	source/isaaclab_rl/isaaclab_rl/__pycache__/skrl.cpython-311.pyc
	source/isaaclab_rl/isaaclab_rl/rl_games/__pycache__/
	source/isaaclab_rl/isaaclab_rl/rl_games/pbt/__pycache__/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/loco_manipulation/tracking/config/digit/__pycache__/loco_manip_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/loco_manipulation/tracking/config/digit/agents/__pycache__/rsl_rl_ppo_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/__pycache__/velocity_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/anymal_c/__pycache__/flat_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/anymal_c/__pycache__/rough_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/digit/__pycache__/rough_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/__pycache__/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/lift/mdp/__pycache__/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/__pycache__/reach_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/config/ur5_with_base/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/__pycache__/__init__.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/__pycache__/rewards.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/__pycache__/navigation_env_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/agents/__pycache__/rsl_rl_ppo_cfg.cpython-311.pyc
	source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/husky/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/mdp/__pycache__/
	source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/mdp/custom_mdp.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/VERSION b/VERSION
deleted file mode 100644
index c043eea776..0000000000
--- a/VERSION
+++ /dev/null
@@ -1 +0,0 @@
-2.2.1
diff --git a/scripts/tutorials/01_assets/add_new_robot.py b/scripts/tutorials/01_assets/add_new_robot.py
index 7755caa678..fbab4fa3a1 100644
--- a/scripts/tutorials/01_assets/add_new_robot.py
+++ b/scripts/tutorials/01_assets/add_new_robot.py
@@ -80,7 +80,7 @@ DOFBOT_CONFIG = ArticulationCfg(
         ),
     },
 )
-
+            
 
 class NewRobotsSceneCfg(InteractiveSceneCfg):
     """Designs the scene."""
@@ -162,7 +162,7 @@ def main():
     # Initialize the simulation context
     sim_cfg = sim_utils.SimulationCfg(device=args_cli.device)
     sim = sim_utils.SimulationContext(sim_cfg)
-    sim.set_camera_view([3.5, 0.0, 3.2], [0.0, 0.0, 0.5])
+    sim.set_camera_view((3.5, 0.0, 3.2), (0.0, 0.0, 0.5))
     # Design scene
     scene_cfg = NewRobotsSceneCfg(args_cli.num_envs, env_spacing=2.0)
     scene = InteractiveScene(scene_cfg)
diff --git a/scripts/tutorials/01_assets/run_articulation.py b/scripts/tutorials/01_assets/run_articulation.py
index 433825e1a3..81110c16df 100644
--- a/scripts/tutorials/01_assets/run_articulation.py
+++ b/scripts/tutorials/01_assets/run_articulation.py
@@ -117,7 +117,6 @@ def run_simulator(sim: sim_utils.SimulationContext, entities: dict[str, Articula
         # Update buffers
         robot.update(sim_dt)
 
-
 def main():
     """Main function."""
     # Load kit helper
diff --git a/source/isaaclab/isaaclab/envs/__pycache__/common.cpython-311.pyc b/source/isaaclab/isaaclab/envs/__pycache__/common.cpython-311.pyc
index 67209c1fd6..4fad9a8321 100644
Binary files a/source/isaaclab/isaaclab/envs/__pycache__/common.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/__pycache__/common.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/__pycache__/direct_rl_env.cpython-311.pyc b/source/isaaclab/isaaclab/envs/__pycache__/direct_rl_env.cpython-311.pyc
index 24424c86f6..9f01988934 100644
Binary files a/source/isaaclab/isaaclab/envs/__pycache__/direct_rl_env.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/__pycache__/direct_rl_env.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/mdp/__pycache__/events.cpython-311.pyc b/source/isaaclab/isaaclab/envs/mdp/__pycache__/events.cpython-311.pyc
index bc1675cb78..ddda107ae5 100644
Binary files a/source/isaaclab/isaaclab/envs/mdp/__pycache__/events.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/mdp/__pycache__/events.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/mdp/actions/__pycache__/actions_cfg.cpython-311.pyc b/source/isaaclab/isaaclab/envs/mdp/actions/__pycache__/actions_cfg.cpython-311.pyc
index c853517912..768968b5d1 100644
Binary files a/source/isaaclab/isaaclab/envs/mdp/actions/__pycache__/actions_cfg.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/mdp/actions/__pycache__/actions_cfg.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/commands_cfg.cpython-311.pyc b/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/commands_cfg.cpython-311.pyc
index a04ab2eb99..95d776bfa7 100644
Binary files a/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/commands_cfg.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/commands_cfg.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/pose_command.cpython-311.pyc b/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/pose_command.cpython-311.pyc
index 5790c410c1..22fd0ee3c7 100644
Binary files a/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/pose_command.cpython-311.pyc and b/source/isaaclab/isaaclab/envs/mdp/commands/__pycache__/pose_command.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/envs/mdp/commands/pose_command.py b/source/isaaclab/isaaclab/envs/mdp/commands/pose_command.py
index af831d6f92..5c38a9f6cd 100644
--- a/source/isaaclab/isaaclab/envs/mdp/commands/pose_command.py
+++ b/source/isaaclab/isaaclab/envs/mdp/commands/pose_command.py
@@ -54,10 +54,28 @@ class UniformPoseCommand(CommandTerm):
         # initialize the base class
         super().__init__(cfg, env)
 
+
+
         # extract the robot and body index for which the command is generated
         self.robot: Articulation = env.scene[cfg.asset_name]
-        self.body_idx = self.robot.find_bodies(cfg.body_name)[0][0]
 
+        
+            # ========== LIST ALL AVAILABLE LINKS ==========
+        print("\n" + "="*70)
+        print("ALL AVAILABLE LINKS")
+        print("="*70)
+        
+        print(f"\nTotal links: {len(self.robot.body_names)}\n")
+        
+        for i, link_name in enumerate(self.robot.body_names):
+            link_pos = self.robot.data.body_link_pose_w[:, i, :3]
+            print(f"[{i:2d}] {link_name:<30} pos: {link_pos}")
+        
+        print("\n" + "="*70 + "\n")
+        # ========== END LIST ==========
+
+
+        self.body_idx = self.robot.find_bodies(cfg.body_name)[0][0]
         # create buffers
         # -- commands: (x, y, z, qw, qx, qy, qz) in root frame
         self.pose_command_b = torch.zeros(self.num_envs, 7, device=self.device)
diff --git a/source/isaaclab/isaaclab/envs/mdp/events.py b/source/isaaclab/isaaclab/envs/mdp/events.py
index 5c0c967e84..72f24417b5 100644
--- a/source/isaaclab/isaaclab/envs/mdp/events.py
+++ b/source/isaaclab/isaaclab/envs/mdp/events.py
@@ -1172,8 +1172,7 @@ def reset_joints_by_scale(
     velocity_range: tuple[float, float],
     asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
 ):
-    """Reset the robot joints by scaling the default position and velocity by the given ranges.
-
+    """
     This function samples random values from the given ranges and scales the default joint positions and velocities
     by these values. The scaled values are then set into the physics simulation.
     """
diff --git a/source/isaaclab/isaaclab/scene/__pycache__/interactive_scene.cpython-311.pyc b/source/isaaclab/isaaclab/scene/__pycache__/interactive_scene.cpython-311.pyc
index f6f8cddf7c..df23854f3f 100644
Binary files a/source/isaaclab/isaaclab/scene/__pycache__/interactive_scene.cpython-311.pyc and b/source/isaaclab/isaaclab/scene/__pycache__/interactive_scene.cpython-311.pyc differ
diff --git a/source/isaaclab/isaaclab/scene/interactive_scene.py b/source/isaaclab/isaaclab/scene/interactive_scene.py
index e12118a36d..7f3703a3d6 100644
--- a/source/isaaclab/isaaclab/scene/interactive_scene.py
+++ b/source/isaaclab/isaaclab/scene/interactive_scene.py
@@ -421,7 +421,7 @@ class InteractiveScene:
             These are not reset or updated by the scene. They are mainly other prims that are not necessarily
             handled by the interactive scene, but are useful to be accessed by the user.
 
-        .. _XFormPrim: https://docs.omniverse.nvidia.com/py/isaacsim/source/isaacsim.core/docs/index.html#isaacsim.core.prims.XFormPrim
+        .. _XFormPrim: https://docs.isaacsim.omniverse.nvidia.com/latest/py/source/extensions/isaacsim.core.prims/docs/index.html#isaacsim.core.prims.XFormPrim
 
         """
         return self._extras
@@ -787,4 +787,4 @@ class InteractiveScene:
             # store global collision paths
             if hasattr(asset_cfg, "collision_group") and asset_cfg.collision_group == -1:
                 asset_paths = sim_utils.find_matching_prim_paths(asset_cfg.prim_path)
-                self._global_prim_paths += asset_paths
+                self._global_prim_paths += asset_paths
\ No newline at end of file
diff --git a/source/isaaclab_assets/isaaclab_assets/robots/__pycache__/__init__.cpython-311.pyc b/source/isaaclab_assets/isaaclab_assets/robots/__pycache__/__init__.cpython-311.pyc
index 6aee332840..c23d9db85a 100644
Binary files a/source/isaaclab_assets/isaaclab_assets/robots/__pycache__/__init__.cpython-311.pyc and b/source/isaaclab_assets/isaaclab_assets/robots/__pycache__/__init__.cpython-311.pyc differ
diff --git a/source/isaaclab_assets/isaaclab_assets/robots/aloha.py b/source/isaaclab_assets/isaaclab_assets/robots/aloha.py
index 2f5b8b89d1..aaf76be706 100644
--- a/source/isaaclab_assets/isaaclab_assets/robots/aloha.py
+++ b/source/isaaclab_assets/isaaclab_assets/robots/aloha.py
@@ -84,5 +84,5 @@ ALOHA_CFG = ArticulationCfg(
             },
         ),
     },
-)
+)   
 """Configuration for the Aloha robot with velocity-driven wheels."""
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks.egg-info/PKG-INFO b/source/isaaclab_tasks/isaaclab_tasks.egg-info/PKG-INFO
index bb2ac97a7c..65a33e63c0 100644
--- a/source/isaaclab_tasks/isaaclab_tasks.egg-info/PKG-INFO
+++ b/source/isaaclab_tasks/isaaclab_tasks.egg-info/PKG-INFO
@@ -1,6 +1,6 @@
 Metadata-Version: 2.4
 Name: isaaclab_tasks
-Version: 0.10.36
+Version: 0.11.0
 Summary: Extension containing suite of environments for robot learning.
 Home-page: https://github.com/isaac-sim/IsaacLab
 Author: Isaac Lab Project Developers
@@ -8,12 +8,14 @@ Maintainer: Isaac Lab Project Developers
 Keywords: robotics,rl,il,learning
 Classifier: Natural Language :: English
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Classifier: Isaac Sim :: 4.5.0
+Classifier: Isaac Sim :: 5.0.0
 Requires-Python: >=3.10
 Requires-Dist: numpy<2
-Requires-Dist: torch>=2.5.1
+Requires-Dist: torch>=2.7
 Requires-Dist: torchvision>=0.14.1
-Requires-Dist: protobuf!=5.26.0,>=3.20.2
+Requires-Dist: protobuf!=5.26.0,>=4.25.8
 Requires-Dist: tensorboard
 Requires-Dist: scikit-learn
 Requires-Dist: numba
diff --git a/source/isaaclab_tasks/isaaclab_tasks.egg-info/SOURCES.txt b/source/isaaclab_tasks/isaaclab_tasks.egg-info/SOURCES.txt
index af902607c4..c1665f4ee8 100644
--- a/source/isaaclab_tasks/isaaclab_tasks.egg-info/SOURCES.txt
+++ b/source/isaaclab_tasks/isaaclab_tasks.egg-info/SOURCES.txt
@@ -9,7 +9,9 @@ isaaclab_tasks.egg-info/requires.txt
 isaaclab_tasks.egg-info/top_level.txt
 test/test_environment_determinism.py
 test/test_environments.py
+test/test_environments_with_stage_in_memory.py
 test/test_factory_environments.py
 test/test_hydra.py
+test/test_lift_teddy_bear.py
 test/test_multi_agent_environments.py
 test/test_record_video.py
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks.egg-info/requires.txt b/source/isaaclab_tasks/isaaclab_tasks.egg-info/requires.txt
index a3d0075d72..dd88aa8f54 100644
--- a/source/isaaclab_tasks/isaaclab_tasks.egg-info/requires.txt
+++ b/source/isaaclab_tasks/isaaclab_tasks.egg-info/requires.txt
@@ -1,7 +1,7 @@
 numpy<2
-torch>=2.5.1
+torch>=2.7
 torchvision>=0.14.1
-protobuf!=5.26.0,>=3.20.2
+protobuf!=5.26.0,>=4.25.8
 tensorboard
 scikit-learn
 numba
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__init__.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__init__.py
deleted file mode 100644
index 469ab002bf..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__init__.py
+++ /dev/null
@@ -1,29 +0,0 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-Quacopter environment.
-"""
-
-import gymnasium as gym
-
-from . import agents
-
-##
-# Register Gym environments.
-##
-
-gym.register(
-    id="Isaac-Aloha-Direct-v0",
-    entry_point=f"{__name__}.aloha_env:WheeledRobotEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": f"{__name__}.aloha_env:WheeledRobotEnvCfg",
-        "rl_games_cfg_entry_point": f"{agents.__name__}:rl_games_sac_cfg.yaml",
-        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:QuadcopterPPORunnerCfg",
-        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_sac_cfg.yaml",
-        "sb3_cfg_entry_point": f"{agents.__name__}:sb3_ppo_cfg.yaml",
-    },
-)
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/__init__.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 939cc91e64..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/aloha_env.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/aloha_env.cpython-310.pyc
deleted file mode 100644
index 4bb27b6f20..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/aloha_env.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/asset_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/asset_manager.cpython-310.pyc
deleted file mode 100644
index c0fe8ca6f9..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/asset_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager.cpython-310.pyc
deleted file mode 100644
index 9c9af434d9..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager_cur.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager_cur.cpython-310.pyc
deleted file mode 100644
index d7d63eb5dc..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/control_manager_cur.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/evaluation_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/evaluation_manager.cpython-310.pyc
deleted file mode 100644
index 10465e207f..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/evaluation_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/graph_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/graph_manager.cpython-310.pyc
deleted file mode 100644
index 8660a8cb9f..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/graph_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/memory_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/memory_manager.cpython-310.pyc
deleted file mode 100644
index 498eb864fe..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/memory_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/obstacle_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/obstacle_manager.cpython-310.pyc
deleted file mode 100644
index e219251239..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/obstacle_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/path_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/path_manager.cpython-310.pyc
deleted file mode 100644
index 971e3ccfa6..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/path_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies.cpython-310.pyc
deleted file mode 100644
index 7e867f4e3e..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies_for_pg.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies_for_pg.cpython-310.pyc
deleted file mode 100644
index e8c1d7407d..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/placement_strategies_for_pg.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager.cpython-310.pyc
deleted file mode 100644
index 7a5c1ba988..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager_for_pg.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager_for_pg.cpython-310.pyc
deleted file mode 100644
index bd0b8fb4db..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/__pycache__/scene_manager_for_pg.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__init__.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__init__.py
deleted file mode 100644
index e75ca2bc3f..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__init__.py
+++ /dev/null
@@ -1,4 +0,0 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__pycache__/__init__.cpython-310.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 3eb5b33a89..0000000000
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_ppo_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_ppo_cfg.yaml
deleted file mode 100644
index a741324741..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_ppo_cfg.yaml
+++ /dev/null
@@ -1,147 +0,0 @@
-params:
-  seed: 42
-
-  # environment wrapper clipping
-  env:
-    # added to the wrapper
-    # can make custom wrapper?
-    clip_actions: 1.0
-
-  algo:
-    name: a2c_continuous
-
-  model:
-    name: continuous_a2c_logstd
-
-  # doesn't have this fine grained control but made it close
-  network:
-    name: actor_critic
-    separate: False
-    space:
-      continuous:
-        mu_activation: None
-        sigma_activation: None
-
-        mu_init:
-          name: default
-        sigma_init:
-          name: const_initializer
-          val: 0
-        fixed_sigma: True
-    mlp:
-      units: [512]
-      activation: elu
-      initializer:
-          name: default
-
-  load_checkpoint: False # flag which sets whether to load the checkpoint
-  load_path: '' # path to the checkpoint to load
-
-  config:
-    name: aloha
-    env_name: rlgpu
-    device: 'cuda:0'
-    device_name: 'cuda:0'
-    multi_gpu: False
-    ppo: True
-    mixed_precision: False
-    normalize_input: False
-    normalize_value: True
-    num_actors: -1  # configured from the script (based on num_envs)
-    reward_shaper:
-      scale_value: 1.0
-    normalize_advantage: True
-    gamma: 0.99
-    tau : 0.95
-    learning_rate: 1e-4
-    lr_schedule: adaptive
-    kl_threshold: 0.008
-    score_to_win: 20000
-    max_epochs: 5000
-    save_best_after: 50
-    save_frequency: 25
-    grad_norm: 1.0
-    entropy_coef: 0.0
-    truncate_grads: True
-    e_clip: 0.2
-    horizon_length: 32
-    minibatch_size: 256
-    mini_epochs: 16
-    critic_coef: 2
-    clip_value: True
-    seq_length: 4
-    bounds_loss_coef: 0.0001
-    activate_contact_sensors: True
-
-# params:
-#   seed: 42
-#   algo:
-#     name: a2c_continuous
-
-#   model:
-#     name: continuous_a2c_logstd
-#   env:
-#     clip_actions: 1.0
-#   network:
-#     name: actor_critic
-#     separate: False
-
-#     space:
-#       continuous:
-#         mu_activation: None
-#         sigma_activation: None
-#         mu_init:
-#           name: default
-#         sigma_init:
-#           name: const_initializer
-#           val: 0
-#         fixed_sigma: True
-#     mlp:
-#       units: [256, 128]
-#       activation: elu
-#       d2rl: False
-
-#       initializer:
-#         name: default
-#       regularizer:
-#         name: None
-
-#   load_checkpoint: False # flag which sets whether to load the checkpoint
-#   load_path: '' # path to the checkpoint to load
-
-#   config:
-#     name: WheeledRobotEnv_PPO
-#     env_name: rlgpu
-#     device: 'cuda:0'
-#     device_name: 'cuda:0'
-#     multi_gpu: False
-#     ppo: True
-#     mixed_precision: False
-#     normalize_input: True
-#     normalize_value: True
-#     num_actors: 32 #${....task.env.numEnvs}
-#     reward_shaper:
-#       scale_value: 0.1
-#     normalize_advantage: True
-#     gamma: 0.99
-#     tau: 0.95
-#     learning_rate: 5e-4
-#     lr_schedule: adaptive
-#     kl_threshold: 0.008
-#     score_to_win: 1000000
-#     max_epochs: 100000
-#     save_best_after: 200
-#     save_frequency: 100
-#     print_stats: True
-#     grad_norm: 1.0
-#     entropy_coef: 0.0
-#     truncate_grads: True
-#     e_clip: 0.2
-#     horizon_length: 64 #64
-#     minibatch_size: 512
-#     mini_epochs: 8
-#     critic_coef: 4
-#     clip_value: True
-#     seq_length: 4
-#     bounds_loss_coef: 0.0001
-#     activate_contact_sensors: True
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy 2.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy 2.yaml
deleted file mode 100644
index bbc9cb0956..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy 2.yaml	
+++ /dev/null
@@ -1,56 +0,0 @@
-params:  
-
-  seed: 56
-
-  env:  
-    clip_actions: 1.0
-    max_episode_length: 512
-
-  algo:
-    name: sac
-
-  model:
-    name: soft_actor_critic
-
-  network:
-    name: soft_actor_critic
-    separate: True
-    space:
-      continuous:
-    mlp:
-      units: [512]
-      activation: elu
-      
-      initializer:
-        name: default
-    log_std_bounds: [-5, 2]
-
-  load_checkpoint: False
-  load_path: ''
-
-  config:
-    name: aloha_direct
-    env_name: rlgpu
-    sac: true
-    multi_gpu: False
-    normalize_input: False
-    reward_shaper:
-      scale_value: 1.0
-    max_epochs: 100000
-    num_steps_per_episode: 32
-    save_best_after: 20
-    save_frequency: 30
-    gamma: 0.99
-    init_alpha: 1.0
-    alpha_lr: 0.005
-    actor_lr: 0.0005
-    critic_lr: 0.0005
-    critic_tau: 0.005
-    batch_size: 256
-    learnable_temperature: true
-    num_warmup_steps: 20
-    replay_buffer_size: 200000
-    num_actors: 32
-    device: 'cuda:0'
-    device_name: 'cuda:0'
-    activate_contact_sensors: True
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy.yaml
deleted file mode 100644
index afb9c423ac..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg copy.yaml	
+++ /dev/null
@@ -1,55 +0,0 @@
-params:
-  seed: 100
-
-  # environment wrapper clipping
-  env:
-    clip_actions: 1.0
-
-  algo:
-    name: sac
-
-  model:
-    name: soft_actor_critic
-
-  network:
-    name: soft_actor_critic
-    separate: True
-    log_std_bounds: [-5, 2]  # Ограничения на логарифм стандартного отклонения
-    space:
-      continuous:
-        log_std_bounds: [-5, 2]  # ,skj -5 2
-    mlp:
-      units: [256, 128, 64] #, 64]  # ,skj 64 64
-      activation: relu
-      initializer:
-        name: default
-
-  load_checkpoint: False  # Флаг для загрузки чекпоинта
-  load_path: ''  # Путь к чекпоинту
-
-  config:
-    name: WheeledRobotEnv_SAC  # Имя эксперимента
-    activate_contact_sensors: True
-    env_name: rlgpu  # Используем rlgpu для Isaac Lab
-    device: 'cuda:0'
-    device_name: 'cuda:0'
-    multi_gpu: False
-    normalize_input: True
-    reward_shaper:
-      scale_value: 0.1
-    max_epochs: 150000
-    num_steps_per_episode: 8
-    save_best_after: 100
-    save_frequency: 250
-    gamma: 0.99
-    init_alpha: 0.5
-    alpha_lr: 0.0001
-    actor_lr: 0.0003
-    critic_lr: 0.0003
-    critic_tau: 0.005
-    batch_size: 512
-    learnable_temperature: true
-    num_warmup_steps: 512
-    replay_buffer_size: 100000
-    env_config:
-      env_name: WheeledRobotEnv  
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg.yaml
deleted file mode 100644
index ef5e72cd0a..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rl_games_sac_cfg.yaml
+++ /dev/null
@@ -1,56 +0,0 @@
-params:  
-
-  seed: 42
-
-  env:  
-    clip_actions: 1.0
-    max_episode_length: 512
-
-  algo:
-    name: sac
-
-  model:
-    name: soft_actor_critic
-
-  network:
-    name: soft_actor_critic
-    separate: True
-    space:
-      continuous:
-    mlp:
-      units: [512,256]
-      activation: elu
-      
-      initializer:
-        name: default
-    log_std_bounds: [-5, 2]
-
-  load_checkpoint: False
-  load_path: ''
-
-  config:
-    name: aloha_direct
-    env_name: rlgpu
-    sac: true
-    multi_gpu: False
-    normalize_input: False
-    reward_shaper:
-      scale_value: 1.0
-    max_epochs: 100000
-    num_steps_per_episode: 256
-    save_best_after: 20
-    save_frequency: 20
-    gamma: 0.99
-    init_alpha: 0.5
-    alpha_lr: 0.005
-    actor_lr: 0.0005
-    critic_lr: 0.0005
-    critic_tau: 0.005
-    batch_size: 256
-    learnable_temperature: true
-    num_warmup_steps: 4
-    replay_buffer_size: 500000
-    num_actors: 64
-    device: 'cuda:0'
-    device_name: 'cuda:0'
-    activate_contact_sensors: True
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rsl_rl_ppo_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rsl_rl_ppo_cfg.py
deleted file mode 100644
index 20e20d3c15..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/rsl_rl_ppo_cfg.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlPpoActorCriticCfg, RslRlPpoAlgorithmCfg
-
-from isaaclab.utils import configclass
-
-
-@configclass
-class QuadcopterPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 200
-    save_interval = 50
-    experiment_name = "quadcopter_direct"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[64, 64],
-        critic_hidden_dims=[64, 64],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.0,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=5.0e-4,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg.yaml
deleted file mode 100644
index 6db1ee32ea..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg.yaml
+++ /dev/null
@@ -1,21 +0,0 @@
-# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L32
-seed: 42
-sac: true
-n_timesteps: !!float 1e6
-policy: 'MlpPolicy'
-n_steps: 16
-batch_size: 256
-gae_lambda: 0.95
-gamma: 0.99
-n_epochs: 20
-ent_coef: 0.01
-learning_rate: !!float 3e-4
-clip_range: !!float 0.2
-policy_kwargs: "dict(
-                  activation_fn=nn.ELU,
-                  net_arch=[512],
-                  squash_output=False,
-                )"
-vf_coef: 1.0
-max_grad_norm: 1.0
-device: "cuda:0"
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg_camera.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg_camera.yaml
deleted file mode 100644
index 678fb527a9..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_ppo_cfg_camera.yaml
+++ /dev/null
@@ -1,98 +0,0 @@
-seed: 42
-
-
-# Models are instantiated using skrl's model instantiator utility
-# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
-models:
-  separate: False
-  policy:  # see gaussian_model parameters
-    class: GaussianMixin
-    clip_actions: False
-    clip_log_std: True
-    min_log_std: -20.0
-    max_log_std: 2.0
-    initial_log_std: 0.0
-    network:
-      - name: features_extractor
-        input: permute(STATES, (0, 3, 1, 2))  # PyTorch NHWC -> NCHW. Warning: don't permute for JAX since it expects NHWC
-        layers:
-          - conv2d: {out_channels: 32, kernel_size: 8, stride: 4, padding: 0}
-          - conv2d: {out_channels: 64, kernel_size: 4, stride: 2, padding: 0}
-          - conv2d: {out_channels: 64, kernel_size: 3, stride: 1, padding: 0}
-          - flatten
-        activations: relu
-      - name: net
-        input: features_extractor
-        layers: [512]
-        activations: elu
-    output: ACTIONS
-  value:  # see deterministic_model parameters
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: features_extractor
-        input: permute(STATES, (0, 3, 1, 2))  # PyTorch NHWC -> NCHW. Warning: don't permute for JAX since it expects NHWC
-        layers:
-          - conv2d: {out_channels: 32, kernel_size: 8, stride: 4, padding: 0}
-          - conv2d: {out_channels: 64, kernel_size: 4, stride: 2, padding: 0}
-          - conv2d: {out_channels: 64, kernel_size: 3, stride: 1, padding: 0}
-          - flatten
-        activations: relu
-      - name: net
-        input: features_extractor
-        layers: [512]
-        activations: elu
-    output: ONE
-
-
-# Rollout memory
-# https://skrl.readthedocs.io/en/latest/api/memories/random.html
-memory:
-  class: RandomMemory
-  memory_size: -1  # automatically determined (same as agent:rollouts)
-
-
-# PPO agent configuration (field names are from PPO_DEFAULT_CONFIG)
-# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html
-agent:
-  class: PPO
-  rollouts: 64
-  learning_epochs: 4
-  mini_batches: 32
-  discount_factor: 0.99
-  lambda: 0.95
-  learning_rate: 1.0e-04
-  learning_rate_scheduler: KLAdaptiveLR
-  learning_rate_scheduler_kwargs:
-    kl_threshold: 0.008
-  state_preprocessor: null
-  state_preprocessor_kwargs: null
-  value_preprocessor: RunningStandardScaler
-  value_preprocessor_kwargs: null
-  random_timesteps: 0
-  learning_starts: 0
-  grad_norm_clip: 1.0
-  ratio_clip: 0.2
-  value_clip: 0.2
-  clip_predicted_values: True
-  entropy_loss_scale: 0.0
-  value_loss_scale: 1.0
-  kl_threshold: 0.0
-  rewards_shaper_scale: 1.0
-  time_limit_bootstrap: False
-  # logging and checkpoint
-  experiment:
-    directory: "aloha"
-    experiment_name: ""
-    write_interval: auto
-    checkpoint_interval: auto
-
-
-# Sequential trainer
-# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
-trainer:
-  class: SequentialTrainer
-  timesteps: 32000
-  environment_info: log
-
-activate_contact_sensors: True
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_sac_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_sac_cfg.yaml
deleted file mode 100644
index 96a980c961..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/sb3_sac_cfg.yaml
+++ /dev/null
@@ -1,25 +0,0 @@
-# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L161
-seed: 42
-
-n_timesteps: !!float 1e7
-policy: 'MlpPolicy'
-batch_size: 256
-n_steps: 512
-gamma: 0.99
-gae_lambda: 0.9
-n_epochs: 20
-ent_coef: 0.0
-sde_sample_freq: 4
-max_grad_norm: 0.5
-vf_coef: 0.5
-learning_rate: !!float 3e-5
-use_sde: True
-clip_range: 0.4
-device: "cuda:0"
-activate_contact_sensors: True
-policy_kwargs: "dict(
-                  log_std_init=-1,
-                  ortho_init=False,
-                  activation_fn=nn.ReLU,
-                  net_arch=dict(pi=[256, 256], vf=[256, 256])
-                )"
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_ppo_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_ppo_cfg.yaml
deleted file mode 100644
index d633571aaf..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_ppo_cfg.yaml
+++ /dev/null
@@ -1,73 +0,0 @@
-seed: 42
-
-# Models configuration, mapped from rl_games model and network sections
-models:
-  separate: False  # matches rl_games network.separate: False
-  policy:
-    class: GaussianMixin
-    clip_actions: True  # matches rl_games env.clip_actions: 1.0
-    clip_log_std: True
-    min_log_std: -20.0
-    max_log_std: 2.0
-    initial_log_std: 0.0  # matches rl_games network.sigma_init.val: 0
-    fixed_log_std: True  # matches rl_games network.fixed_sigma: True
-    network:
-      - name: net
-        input: STATES
-        layers: [512]  # matches rl_games network.mlp.units: [512]
-        activations: elu  # matches rl_games network.mlp.activation: elu
-    output: ACTIONS
-  value:
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: net
-        input: STATES
-        layers: [512]  # matches rl_games network.mlp.units: [512]
-        activations: elu  # matches rl_games network.mlp.activation: elu
-    output: ONE
-
-# Rollout memory, configured to match rl_games horizon_length
-memory:
-  class: RandomMemory 
-  memory_size: 32  # matches rl_games config.horizon_length: 32
-
-# PPO agent configuration, mapped from rl_games config
-agent:
-  class: PPO
-  rollouts: 32  # matches rl_games config.horizon_length: 32
-  learning_epochs: 16  # matches rl_games config.mini_epochs: 16
-  mini_batches: 8  # derived from rl_games config.minibatch_size: 256 (assuming 2048 samples / 256 = 8 mini-batches)
-  discount_factor: 0.99  # matches rl_games config.gamma: 0.99
-  lambda: 0.95  # matches rl_games config.tau: 0.95
-  learning_rate: 1.0e-04  # matches rl_games config.learning_rate: 1e-4
-  learning_rate_scheduler: KLAdaptiveLR  # matches rl_games config.lr_schedule: adaptive
-  learning_rate_scheduler_kwargs:
-    kl_threshold: 0.008  # matches rl_games config.kl_threshold: 0.008
-  state_preprocessor: RunningStandardScaler  # matches rl_games config.normalize_input: False (RunningStandardScaler is default in skrl)
-  state_preprocessor_kwargs: null
-  value_preprocessor: RunningStandardScaler  # matches rl_games config.normalize_value: True
-  value_preprocessor_kwargs: null
-  random_timesteps: 0
-  learning_starts: 0
-  grad_norm_clip: 1.0  # matches rl_games config.grad_norm: 1.0
-  ratio_clip: 0.2  # matches rl_games config.e_clip: 0.2
-  value_clip: True  # matches rl_games config.clip_value: True
-  clip_predicted_values: True
-  entropy_loss_scale: 0.0  # matches rl_games config.entropy_coef: 0.0
-  value_loss_scale: 2.0  # matches rl_games config.critic_coef: 2
-  kl_threshold: 0.008  # matches rl_games config.kl_threshold: 0.008
-  rewards_shaper_scale: 1.0  # matches rl_games config.reward_shaper.scale_value: 1.0
-  time_limit_bootstrap: False
-  # logging and checkpoint, mapped from rl_games config
-  experiment:
-    directory: "aloha"  # matches rl_games config.name: aloha
-    experiment_name: ""
-    write_interval: 200  # matches rl_games config.save_frequency: 25
-    checkpoint_interval: 500  # matches rl_games config.save_best_after: 50
-
-# Sequential trainer, mapped from rl_games config
-trainer:
-  class: SequentialTrainer
-  timesteps: 160000000  # derived from rl_games config.max_epochs: 5000 * config.horizon_length: 32 * num_actors (assuming num_actors=1000 for approximation)
-  environment_info: log
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg.yaml
deleted file mode 100644
index 063ece0b09..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg.yaml
+++ /dev/null
@@ -1,80 +0,0 @@
-seed: 42
-
-
-# Models are instantiated using skrl's model instantiator utility
-# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
-models:
-  separate: True
-  policy:  # see gaussian_model parameters
-    # class: GaussianMixin # CategoricalMixin # 
-    # clip_actions: True
-    # clip_log_std: True
-    # min_log_std: -5.0
-    # max_log_std: 2.0
-    # initial_log_std: 0.2
-    # network:
-    #   - name: net
-    #     input: STATES
-    #     layers: [512,256]
-    #     activations: elu
-    # output: tanh(ACTIONS)
-
-    class: models.custom_gmodel.CustomActor # CategoricalMixin # 
-    clip_actions: True
-    clip_log_std: True
-    min_log_std: -5.0
-    max_log_std: 2.0
-  critic_1:  # see deterministic_model parameters
-    class: models.custom_gmodel.CustomCritic # CategoricalMixin # 
-    clip_actions: False
-  critic_2:  # see deterministic_model parameters
-    class: models.custom_gmodel.CustomCritic # CategoricalMixin # 
-    clip_actions: False
-  target_critic_1:  # see deterministic_model parameters
-    class: models.custom_gmodel.CustomCritic # CategoricalMixin # 
-    clip_actions: False
-  target_critic_2:  # see deterministic_model parameters
-    class: models.custom_gmodel.CustomCritic # CategoricalMixin # 
-    clip_actions: False
-
-memory:
-  class: RandomMemory
-  memory_size: 10000
-
-agent:
-  class: SAC
-  gradient_steps: 2
-  batch_size: 512
-
-  discount_factor: 0.99
-  polyak: 0.005
-
-  random_timesteps: 0
-  learning_starts: 3000
-  actor_learning_rate: 0.0003   # actor learning rate
-  critic_learning_rate: 0.0003  # critic learning rate
-  grad_norm_clip: 0
-
-  learn_entropy: True
-  initial_entropy_value: 0.5
-  activate_contact_sensors: True
-  state_preprocessor: RunningStandardScaler
-  state_preprocessor_kwargs:
-    size: auto
-    device: "cuda:0"
-  mixed_precision: True
-
-
-  experiment:
-    directory: 'aloha'
-    experiment_name: 'SAC'
-    write_interval: 200
-    checkpoint_interval: 500
-
-
-# Sequential trainer
-# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
-trainer:
-  class: SequentialTrainer
-  timesteps: 100000
-  environment_info: log
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg_old.yaml b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg_old.yaml
deleted file mode 100644
index 69b4632346..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/agents/skrl_sac_cfg_old.yaml
+++ /dev/null
@@ -1,98 +0,0 @@
-seed: 42
-
-
-# Models are instantiated using skrl's model instantiator utility
-# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
-models:
-  separate: True
-  policy:  # see gaussian_model parameters
-    class: GaussianMixin # CategoricalMixin # 
-    clip_actions: True
-    clip_log_std: True
-    min_log_std: -5.0
-    max_log_std: 2.0
-    initial_log_std: 0.2
-    network:
-      - name: net
-        input: STATES
-        layers: [512,256]
-        activations: elu
-    output: tanh(ACTIONS)
-  critic_1:  # see deterministic_model parameters
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: net
-        input: STATES_ACTIONS
-        layers: [512,256]
-        activations: elu
-    output: ONE
-  critic_2:  # see deterministic_model parameters
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: net
-        input: STATES_ACTIONS
-        layers: [512,256]
-        activations: elu
-    output: ONE
-  target_critic_1:  # see deterministic_model parameters
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: net
-        input: STATES_ACTIONS
-        layers: [512,256]
-        activations: elu
-    output: ONE
-  target_critic_2:  # see deterministic_model parameters
-    class: DeterministicMixin
-    clip_actions: False
-    network:
-      - name: net
-        input: STATES_ACTIONS
-        layers: [512,256]
-        activations: elu
-    output: ONE
-
-memory:
-  class: RandomMemory
-  memory_size: 10000
-
-agent:
-  class: SAC
-  gradient_steps: 2
-  batch_size: 512
-
-  discount_factor: 0.99
-  polyak: 0.005
-
-  random_timesteps: 0
-  learning_starts: 3000
-  actor_learning_rate: 0.0003   # actor learning rate
-  critic_learning_rate: 0.0003  # critic learning rate
-  grad_norm_clip: 0
-
-  learn_entropy: True
-  initial_entropy_value: 0.5
-  activate_contact_sensors: True
-  state_preprocessor: RunningStandardScaler
-  state_preprocessor_kwargs:
-    size: auto
-    device: "cuda:0"
-  mixed_precision: True
-
-
-  experiment:
-    directory: 'aloha'
-    experiment_name: 'SAC'
-    write_interval: 200
-    checkpoint_interval: 500
-
-
-# Sequential trainer
-# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
-trainer:
-  class: SequentialTrainer
-  timesteps: 70000
-  environment_info: log
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 2.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 2.py
deleted file mode 100644
index bf31335fb7..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 2.py	
+++ /dev/null
@@ -1,1158 +0,0 @@
-# env.py
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
- 
-import gymnasium as gym
-import torch
-import math
-import numpy as np
-import os
-import torchvision.models as models
-import torchvision.transforms as transforms
-from torch import nn
-import random
-
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation, ArticulationCfg, RigidObject, RigidObjectCfg
-from isaaclab.envs import DirectRLEnv, DirectRLEnvCfg
-from isaaclab.envs.ui import BaseEnvWindow
-from isaaclab.markers import VisualizationMarkers
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg, SimulationContext
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import subtract_frame_transforms
-from isaaclab.sensors import TiledCamera, TiledCameraCfg, ContactSensor, ContactSensorCfg
-from .scene_manager import SceneManager
-from .control_manager import VectorizedPurePursuit
-from .path_manager import Path_manager
-from .memory_manager import Memory_manager, PathTracker
-from .asset_manager import AssetManager
-import omni.kit.commands
-import omni.usd
-import datetime
-# from torch.utils.tensorboard import SummaryWriter
-##
-# Pre-defined configs
-##
-from isaaclab_assets.robots.aloha import ALOHA_CFG
-from isaaclab.markers import CUBOID_MARKER_CFG
-from transformers import CLIPProcessor, CLIPModel
-from PIL import Image
-import omni.kit.commands  # Уже импортировано в вашем коде
-from omni.usd import get_context  # Для доступа к stage
-from pxr import Gf
-
-class WheeledRobotEnvWindow(BaseEnvWindow):
-    def __init__(self, env: 'WheeledRobotEnv', window_name: str = "IsaacLab"):
-        super().__init__(env, window_name)
-        with self.ui_window_elements["main_vstack"]:
-            with self.ui_window_elements["debug_frame"]:
-                with self.ui_window_elements["debug_vstack"]:
-                    self._create_debug_vis_ui_element("targets", self.env)
-
-@configclass
-class WheeledRobotEnvCfg(DirectRLEnvCfg):
-    episode_length_s = 512.0
-    decimation = 8
-    action_space = gym.spaces.Box(
-        low=np.array([-1.0, -1.0], dtype=np.float32),
-        high=np.array([1.0, 1.0], dtype=np.float32),
-        shape=(2,)
-    )
-    # Observation space is now the ResNet18 embedding size (512)
-    m = 1  # Например, 3 эмбеддинга и действия
-    observation_space = gym.spaces.Box(
-        low=-float("inf"),
-        high=float("inf"),
-        shape=(m * (512 + 3),),  # m * (embedding_size + action_size) + 2 (скорости)
-        dtype="float32"
-    )
-    state_space = 0
-    debug_vis = False
-
-    ui_window_class_type = WheeledRobotEnvWindow
-
-    sim: SimulationCfg = SimulationCfg(
-        dt=1/60,
-        render_interval=decimation,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="min",
-            restitution_combine_mode="min",
-            static_friction=0.2,
-            dynamic_friction=0.15,
-            restitution=0.0,
-        ),
-    )
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="plane",
-        collision_group=-1,
-        # physics_material=sim_utils.RigidBodyMaterialCfg(
-        #     friction_combine_mode="min",
-        #     restitution_combine_mode="min",
-        #     static_friction=0.8,
-        #     dynamic_friction=0.6,
-        #     restitution=0.0,
-        # ),
-        debug_vis=False,
-    )
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=64, env_spacing=18, replicate_physics=True)
-    robot: ArticulationCfg = ALOHA_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    wheel_radius = 0.068
-    wheel_distance = 0.34
-    tiled_camera: TiledCameraCfg = TiledCameraCfg(
-        prim_path="/World/envs/env_.*/Robot/box2_Link/Camera",
-        offset=TiledCameraCfg.OffsetCfg(pos=(-0.35, 0, 1.1), rot=(0.99619469809,0,0.08715574274,0), convention="world"),
-        # offset=TiledCameraCfg.OffsetCfg(pos=(0.0, 0, 0.9), rot=(1,0,0,0), convention="world"),
-        data_types=["rgb"],
-        spawn=sim_utils.PinholeCameraCfg(
-            focal_length=35.0, focus_distance=2.0, horizontal_aperture=36, clipping_range=(0.2, 10.0)
-        ),
-        width=224,
-        height=224,
-    )
-    current_dir = os.getcwd()
-    kitchen = sim_utils.UsdFileCfg(
-        usd_path=os.path.join(current_dir, "source/isaaclab_assets/data/aloha_assets", "scenes/scenes_sber_kitchen_for_BBQ/kitchen_new_simple.usd"),
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-    contact_sensor = ContactSensorCfg(
-        prim_path="/World/envs/env_.*/Robot/.*",
-        update_period=0.1,
-        history_length=1,
-        debug_vis=True,
-        filter_prim_paths_expr=["/World/envs/env_.*"],
-    )
-
-class WheeledRobotEnv(DirectRLEnv):
-    cfg: WheeledRobotEnvCfg
-
-    def __init__(self, cfg: WheeledRobotEnvCfg, render_mode: str | None = None, **kwargs):
-        self._super_init = True
-        self.config_path="/home/xiso/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json"
-        super().__init__(cfg, render_mode, **kwargs)
-        self._super_init = False
-        
-        self.scene_manager = SceneManager(self.num_envs, self.config_path, self.device)
-        self.use_controller = True
-        self.imitation = False
-        if self.imitation:
-            self.use_controller = True
-        if self.use_controller:
-            self.path_manager = Path_manager(scene_manager=self.scene_manager, ratio=8.0, shift=[5, 4], device=self.device)
-            self.control_module = VectorizedPurePursuit(num_envs=self.num_envs, device=self.device)
-        self.memory_on = False
-        self.tracker = PathTracker(num_envs=self.num_envs, device=self.device)
-        if self.memory_on:
-            self.memory_manager = Memory_manager(
-                num_envs=self.num_envs,
-                embedding_size=512,  # Размер эмбеддинга ResNet18
-                action_size=2,      # Размер действия (линейная и угловая скорость)
-                history_length=25,  # n = 10, можно настроить
-                device=self.device
-            )
-
-        self._actions = torch.zeros((self.num_envs, 2), device=self.device)
-        self._actions[:, 1] = 0.0
-        self._left_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._right_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._desired_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self._episode_sums = {
-            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-            for key in ["moves"]
-        }
-        self._left_wheel_id = self._robot.find_joints("left_wheel")[0]
-        self._right_wheel_id = self._robot.find_joints("right_wheel")[0]
-
-        self.set_debug_vis(self.cfg.debug_vis)
-        self.Debug = True
-        self.event_update_counter = 0
-        self.episode_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.success_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.step_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.possible_goal_position = []
-        
-        self.delete = 1
-        self.count = 0
-        self._debug_log_enabled = True
-        self._debug_envs_to_log = list(range(min(5, self.num_envs)))
-        self._inconsistencies = []
-        self._debug_step_counter = 0
-        self._debug_log_frequency = 10
-        self.turn_on_controller = False #it is not use or not use controller, it is flag for the first step
-        self.turn_on_controller_step = 0
-        self.my_episode_lenght = 256
-        self.turn_off_controller_step = 0
-        self.use_obstacles = True
-        self.turn_on_obstacles = False
-        self.turn_on_obstacles_always = False
-        if self.use_obstacles:
-            self.use_obstacles = True
-        self.previous_distance_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_angle_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_lin_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_ang_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.angular_speed = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        # Initialize ResNet18 for image embeddings
-        # self.resnet18 = models.resnet18(pretrained=True).to(self.device)
-        # self.resnet18.eval()  # Set to evaluation mode
-        # # Remove the final fully connected layer to get embeddings
-        # self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])
-        # # Image preprocessing for ResNet18
-        # transforms.ToTensor()
-        # self.transform = transforms.Compose([
-        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
-        # ])
-        self.success_rate = 0
-        self.sr_stack_capacity = 0
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-        self._step_update_counter = 0
-        self.mean_radius = 2.3
-        self.max_angle_error = torch.pi / 3
-        self.cur_angle_error = torch.pi / 4
-        self.warm = True
-        self.warm_len = 2048
-        self.without_imitation = self.warm_len / 2
-        self._obstacle_update_counter = 0
-        self.has_contact = torch.full((self.num_envs,), True, dtype=torch.bool, device=self.device)
-        self.sim = SimulationContext.instance()
-        self.obstacle_positions = None
-        self.key = None
-        self.success_ep_num = 0
-        # self.run = wandb.init(project="aloha_direct")
-        self.first_ep = [True, True]
-        self.first_ep_step = 0
-        self.second_ep = True
-        timestamp = datetime.datetime.now().strftime("%m_%d_%H_%M")
-        name = "dev"
-        self.episode_lengths = torch.zeros(self.num_envs, device=self.device)
-        self.episode_count = 0
-        self.total_episode_length = 0.0
-        # self.tensorboard_writer = SummaryWriter(log_dir=f"/home/xiso/IsaacLab/logs/tensorboard/navigation_rl_{name}_{timestamp}")
-        self.tensorboard_step = 0
-        self.cur_step = 0
-        self.velocities = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-        
-        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
-        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
-        self.clip_model.eval()  # Установить в режим оценки
-        self.second_try = 0
-        self.foult_ep_num = 0
-        # Инициализация стеков для хранения успехов (1 - успех, 0 - неуспех)
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.max_stack_size = 20  # Максимальный размер стека
-        self.sr_stack_full = False
-        self.start_mean_radius = 0
-        self.min_level_radius = 0
-        self.sr_treshhold = 85
-        self.LOG = False
-        
-        if self.LOG:
-            from comet_ml import start
-            from comet_ml.integration.pytorch import log_model
-            self.experiment = start(
-                api_key="DRYfW6B6VtUQr9llvf3jup57R",
-                project_name="general",
-                workspace="xisonik"
-            )
-        self.print_config_info()
-        self._setup_scene()
-        self.prim_paths = self.asset_manager.all_prim_paths
-        # сразу после создания scene_manager
-        self._material_cache = {}        # key -> material prim path (строка), key = "r_g_b"
-        self._applied_color_map = {}     # obj_index (int) -> color_key (str), чтобы не биндим повторно
-
-
-    def print_config_info(self):
-        print("__________[ CONGIFG INFO ]__________")
-        print(f"|")
-        print(f"| Start mean radius is: {self.mean_radius}")
-        print(f"|")
-        print(f"| Start amx angle is: {self.max_angle_error}")
-        print(f"|")
-        print(f"| Use controller: {self.use_controller}")
-        print(f"|")
-        print(f"| Full imitation: {self.imitation}")
-        print(f"|")
-        print(f"| Use memory: {self.memory_on}")
-        print(f"|")
-        print(f"| Use obstacles: {self.use_obstacles}")
-        print(f"|")
-        print(f"| Start radius: {self.start_mean_radius}, min: {self.min_level_radius}")
-        print(f"|")
-        print(f"| Warm len: {self.warm_len}")
-        print(f"|")
-        print(f"| Turn on obstacles always: {self.turn_on_obstacles_always}")
-        print(f"|")
-        print(f"_______[ CONGIFG INFO CLOSE ]_______")
-
-    def _setup_scene(self):
-        from isaaclab.sensors import ContactSensor
-        import time
-        from pxr import Usd
-        from isaaclab.sim.spawners.from_files import spawn_from_usd
-        import random
-        if self._super_init:
-            self._robot = Articulation(self.cfg.robot)
-            self.scene.articulations["robot"] = self._robot
-            self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-            self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
-            self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
-            self.scene.clone_environments(copy_from_source=True)
-            self._tiled_camera = TiledCamera(self.cfg.tiled_camera)
-            self.scene.sensors["tiled_camera"] = self._tiled_camera
-            # Спавн кухни (статический элемент)
-            spawn_from_usd(
-                prim_path="/World/envs/env_.*/Kitchen",
-                cfg=self.cfg.kitchen,
-                translation=(5.0, 4.0, 0.0),
-                orientation=(0.0, 0.0, 0.0, 1.0),
-            )
-            self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-            self.scene.sensors["contact_sensor"] = self._contact_sensor
-            self.asset_manager = AssetManager(config_path=self.config_path)
-            self.scene_objects = self.asset_manager.spawn_assets_in_scene()
-            
-            # self.scene_manager.update_prims(prim_path)
-            
-
-        # light_cfg = sim_utils.DomeLightCfg(intensity=300.0, color=(0.75, 0.75, 0.75))
-        # light_cfg.func("/World/Light", light_cfg)
-
-    def _get_observations(self) -> dict:
-        self.tensorboard_step += 1
-        self.cur_step += 1
-        self.episode_lengths += 1
-        import os
-        from PIL import Image, ImageDraw, ImageFont
-        # Получение RGB изображений с камеры
-        camera_data = self._tiled_camera.data.output["rgb"].clone()  # Shape: (num_envs, 224, 224, 3)
-        
-        # Преобразование изображений для CLIP
-        # CLIP ожидает изображения в формате PIL или тензоры с правильной нормализацией
-        images = camera_data.cpu().numpy().astype(np.uint8)  # Конвертация в numpy uint8
-        # inputs = self.clip_processor(images=images, return_tensors="pt", padding=True).to(self.device)
-        images_list = [Image.fromarray(im) for im in images]  # если images shape (N,H,W,3) numpy, это даёт список 2D-arrays
-        inputs = self.clip_processor(images=images_list, return_tensors="pt", padding=True)
-        for k, v in inputs.items():
-            inputs[k] = v.to(self.device)
-        # Получение эмбеддингов изображений
-        with torch.no_grad():
-            image_embeddings = self.clip_model.get_image_features(**inputs)  # Shape: (num_envs, 512)
-            image_embeddings = image_embeddings / (image_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        
-        # Получение скоростей робота
-        root_lin_vel_w = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1).unsqueeze(-1)
-        root_ang_vel_w = self._robot.data.root_ang_vel_w[:, 2].unsqueeze(-1)
-        
-        # Обновление памяти, если используется
-        # if self.memory_on:
-        #     velocities = torch.cat([root_lin_vel_w, root_ang_vel_w], dim=-1)
-        #     self.memory_manager.update(image_embeddings, velocities)
-        #     memory_data = self.memory_manager.get_observations() 
-        #     obs = torch.cat([memory_data], dim=-1)
-        # else:
-        
-         # только в первом шаге
-        # if self.tensorboard_step % 50 == 0:
-        #     save_dir = "/home/xiso/Downloads/assets/tmp"
-        #     os.makedirs(save_dir, exist_ok=True)
-
-        #     for i in range(min(4, self.num_envs)):  # первые 4 среды
-        #         img_np = camera_data[i].cpu().numpy().astype(np.uint8)  # (H,W,3)
-        #         img_pil = Image.fromarray(img_np)
-        #         pos = self.to_local(self._robot.data.root_pos_w[:, :2])
-        #         # Добавим подпись с позициями
-        #         draw = ImageDraw.Draw(img_pil)
-        #         text = f"env {i}\nroot_pos_w: {pos[i, :2].cpu().numpy()}\n" \
-        #             f"goal_pos: {self._desired_pos_w[i, :2].cpu().numpy()}"
-        #         draw.text((5, 5), text, fill=(255, 255, 255))
-
-        #         img_pil.save(os.path.join(save_dir, f"env_{i}_step_{self.tensorboard_step}.png"))
-
-        #     print(f"[DEBUG] Saved first 4 env images with positions to {save_dir} {self.tensorboard_step}")
-            # Получаем полный эмбеддинг сцены из графа для каждой среды
-        # scene_embeddings = [self.scene_manager.graphs[i].graph_to_tensor() for i in range(self.num_envs)]
-        # scene_embeddings = torch.stack(scene_embeddings, dim=0)
-        # print(image_embeddings.shape)
-        scene_embeddings = self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-
-        obs = torch.cat([image_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        # obs = torch.cat([image_embeddings, scene_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-
-        self.previous_ang_vel = self.angular_speed
-        # log_embedding_stats(image_embeddings)
-        
-        observations = {"policy": obs}       
-        return observations
-
-    # as they are not affected by the observation space change.
-
-    def _pre_physics_step(self, actions: torch.Tensor):
-        if self.cur_step % 256 == 0:
-            print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-        env_ids = self._robot._ALL_INDICES.clone()
-        self._actions = actions.clone().clamp(-1.0, 1.0)
-
-        nan_mask = torch.isnan(self._actions) | torch.isinf(self._actions)
-        nan_indices = torch.nonzero(nan_mask.any(dim=1), as_tuple=False).squeeze()  # env_ids где любой action NaN/inf
-        if nan_indices.numel() > 0:
-            print(f"[WARNING] NaN/Inf in actions for envs: {nan_indices.tolist()}. Attempting recovery...")
-        r = self.cfg.wheel_radius
-        L = self.cfg.wheel_distance
-        self._step_update_counter += 1
-        if self.turn_on_controller or self.imitation:
-            self.turn_on_controller_step += 1
-            # Получаем текущую ориентацию (yaw) из кватерниона
-            quat = self._robot.data.root_quat_w
-            siny_cosp = 2 * (quat[:, 0] * quat[:, 3] + quat[:, 1] * quat[:, 2])
-            cosy_cosp = 1 - 2 * (quat[:, 2] * quat[:, 2] + quat[:, 3] * quat[:, 3])
-            yaw = torch.atan2(siny_cosp, cosy_cosp)
-            linear_speed, angular_speed = self.control_module.compute_controls(
-                self.to_local(self._robot.data.root_pos_w[:, :2],env_ids),
-                yaw
-            )
-            # angular_speed = -angular_speed 
-            self._actions[:, 0] = (linear_speed / 0.6) - 1
-            self._actions[:, 1] = angular_speed / 2
-            actions.copy_(self._actions.clamp(-1.0, 1.0))
-        else:
-            self.turn_off_controller_step += 1
-            linear_speed = 0.6*(self._actions[:, 0] + 1.0) # [num_envs], всегда > 0
-            angular_speed = 2*self._actions[:, 1]  # [num_envs], оставляем как есть от RL
-        # linear_speed = torch.tensor([0], device=self.device)
-        self.angular_speed = angular_speed
-        self.velocities = torch.stack([linear_speed, angular_speed], dim=1)
-        # if self.tensorboard_step % 4 ==0:
-        # self.delete = -1 * self.delete 
-        # angular_speed = torch.tensor([0], device=self.device)
-        # print("vel is: ", linear_speed, angular_speed)
-        self._left_wheel_vel = (linear_speed - (angular_speed * L / 2)) / r
-        self._right_wheel_vel = (linear_speed + (angular_speed * L / 2)) / r
-        # self._left_wheel_vel = torch.clamp(self._left_wheel_vel, -10, 10)
-        # self._right_wheel_vel = torch.clamp(self._right_wheel_vel, -10, 10)
-
-    def _apply_action(self):
-        wheel_velocities = torch.stack([self._left_wheel_vel, self._right_wheel_vel], dim=1).unsqueeze(-1).to(dtype=torch.float32)
-        self._robot.set_joint_velocity_target(wheel_velocities, joint_ids=[self._left_wheel_id, self._right_wheel_id])
-
-    def _get_rewards(self) -> torch.Tensor:
-        # env_ids = self._robot._ALL_INDICES.clone()
-        # # Сначала размещаем все объекты
-        # num_envs = len(env_ids)
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        # joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        # joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        # default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        # default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        # default_root_state[:, 2] = 0.1
-        # self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        # self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        # self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-        lin_vel = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1)
-        
-        lin_vel_reward = torch.clamp(lin_vel*0.02, min=0, max=0.15)
-        ang_vel = self._robot.data.root_ang_vel_w[:, 2]
-        ang_vel_reward = torch.abs(self.angular_speed) * 0.1
-        a_penalty = 0.1 * torch.abs(self.angular_speed - self.previous_ang_vel) #+ torch.abs(lin_vel - self.previous_lin_vel))
-        # print("a_penalty ", -a_penalty, self.angular_speed, self.previous_ang_vel )
-        # self.previous_lin_vel = lin_vel
-
-        goal_reached, num_subs, r_error, a_error = self.goal_reached(get_num_subs=True)
-
-        moves = torch.clamp(5 * (self.previous_distance_error - r_error), min=0, max=1) + \
-                    torch.clamp(5 * (self.previous_angle_error - a_error), min=0, max=1)
-        env_ids = self._robot._ALL_INDICES.clone()
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        self.tracker.add_step(env_ids, self.to_local(root_pos_w, env_ids), self.velocities)
-        path_lengths = self.tracker.compute_path_lengths(env_ids)
-
-        moves_reward = moves * 0.1
-        
-        self.previous_distance_error = r_error
-        self.previous_angle_error = a_error
-
-        has_contact = self.get_contact()
-
-        time_out = self.is_time_out(self.my_episode_lenght-1)
-        time_out_penalty = -5 * time_out.float()
-
-        vel_penalty = -1 * (ang_vel_reward + lin_vel_reward)
-        mask = ~goal_reached
-        vel_penalty[mask] = 0
-        lin_vel_reward[goal_reached] = 0
-
-        paths = self.tracker.get_paths(env_ids)
-        # jerk_counts = self.tracker.compute_jerk(env_ids, threshold=0.2)
-
-        # print(jerk_counts)
-        
-        if self.turn_on_controller:
-            IL_reward = 0.5
-            punish = 0
-        else:
-            IL_reward = 0
-            punish = (
-                - 0.1
-                - ang_vel_reward / (1 + 2 * self.mean_radius)
-                + lin_vel_reward / (1 + 2 * self.mean_radius)
-            )
-        reward = (
-            IL_reward + punish #* r_error
-            # + torch.clamp(goal_reached.float() * 7 * (1 + self.scene_manager.get_start_dist_error()) / (1 + path_lengths), min=0, max=15)
-            - torch.clamp(has_contact.float() * (7 + lin_vel_reward), min=0, max=10)
-        )
-
-        if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-            sr = self.update_success_rate(goal_reached)
-
-        # if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-        # print("path info")
-        # print(path_lengths)
-        # print(r_error)
-        # print("reward ", reward)
-        # print("- 0.1 - 0.05 * r_error ", - 0.1 - 0.05 * r_error)
-        # print("IL_reward * r_error ", IL_reward * r_error)
-        # print("goal_reached ", goal_reached)
-        # print("lin_vel_reward ", lin_vel_reward)
-        # print("torch.clamp(goal_reached ", torch.clamp(goal_reached.float() * 7 * (1 + self.mean_radius) / (1 + path_lengths), min=0, max=10))
-        # print("torch.clamp(has_contact ", torch.clamp(has_contact.float() * (3 + lin_vel_reward), min=0, max=6))
-        # print("ang_vel_reward ", -ang_vel_reward)
-        # print("___________")
-        check = {
-            "moves":moves,
-        }
-        for key, value in check.items():
-            self._episode_sums[key] += value
-
-        # if self.tensorboard_step % 100 == 0:
-        #     self.tensorboard_writer.add_scalar("Metrics/reward", torch.sum(reward), self.tensorboard_step)
-        return reward
-    
-    def quat_rotate(self, quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
-        """
-        Вращение вектора vec кватернионом quat.
-        quat: [N, 4] (w, x, y, z)
-        vec: [N, 3]
-        Возвращает: [N, 3] - вектор vec, повернутый кватернионом quat
-        """
-        w, x, y, z = quat.unbind(dim=1)
-        vx, vy, vz = vec.unbind(dim=1)
-
-        # Кватернионное умножение q * v
-        qw = -x*vx - y*vy - z*vz
-        qx = w*vx + y*vz - z*vy
-        qy = w*vy + z*vx - x*vz
-        qz = w*vz + x*vy - y*vx
-
-        # Обратный кватернион q*
-        rw = w
-        rx = -x
-        ry = -y
-        rz = -z
-
-        # Результат (q * v) * q*
-        rx_new = qw*rx + qx*rw + qy*rz - qz*ry
-        ry_new = qw*ry - qx*rz + qy*rw + qz*rx
-        rz_new = qw*rz + qx*ry - qy*rx + qz*rw
-
-        return torch.stack([rx_new, ry_new, rz_new], dim=1)
-
-
-    def goal_reached(self, angle_threshold: float = 17, radius_threshold: float = 1.2, get_num_subs=False):
-        """
-        Проверяет достижение цели с учётом расстояния и направления взгляда робота.
-        distance_to_goal: [N] расстояния до цели
-        angle_threshold: максимально допустимый угол в радианах между направлением взгляда и вектором на цель
-        Возвращает: [N] булев тензор, True если цель достигнута
-        """
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root: ", self.to_local(root_pos_w))
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # Проверка по расстоянию (например, радиус достижения stored в self.radius)
-        close_enough = distance_to_goal <= radius_threshold
-
-        # Получаем ориентацию робота в виде кватерниона (w, x, y, z)
-        root_quat_w = self._robot.data.root_quat_w  # shape [N, 4]
-
-        # Локальный вектор взгляда робота (вперёд по оси X)
-        local_forward = torch.tensor([1.0, 0.0, 0.0], device=root_quat_w.device, dtype=root_quat_w.dtype)
-        local_forward = local_forward.unsqueeze(0).repeat(root_quat_w.shape[0], 1)  # [N, 3]
-
-        # Вектор взгляда в мировых координатах
-        forward_w = self.quat_rotate(root_quat_w, local_forward)  # [N, 3]
-
-        # Вектор от робота к цели
-        root_pos_w = self._robot.data.root_pos_w  # [N, 3]
-        to_goal = self._desired_pos_w - root_pos_w  # [N, 3]
-
-        # Нормализуем векторы
-        forward_w_norm = torch.nn.functional.normalize(forward_w[:, :2] , dim=1)
-        to_goal_norm = torch.nn.functional.normalize(to_goal[:, :2] , dim=1)
-
-        # Косинус угла между векторами взгляда и направления на цель
-        cos_angle = torch.sum(forward_w_norm * to_goal_norm, dim=1)
-        cos_angle = torch.clamp(cos_angle, -1.0, 1.0)  # для безопасности
-        # direction_to_goal = to_goal
-        # yaw_g = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-
-        # Вычисляем угол между векторами
-        angle = torch.acos(cos_angle)
-        angle_degrees = torch.abs(angle) * 180.0 / 3.141592653589793
-        # Проверяем, что угол меньше порога
-        facing_goal = angle_degrees < angle_threshold
-
-        # Итоговое условие: близко к цели и смотрит в её сторону
-        # print(distance_to_goal, angle_degrees)
-        
-        conditions = torch.stack([close_enough, facing_goal], dim=1)  # shape [N, M]
-        num_conditions_met = conditions.sum(dim=1)  # shape [N], количество True в каждой строк
-
-        # self.step_counter += torch.ones_like(self.step_counter) #torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        # enouth_steps = self.step_counter > 4
-        # returns = torch.logical_and(torch.logical_and(close_enough, facing_goal), enouth_steps)
-        # self.step_counter = torch.where(returns, torch.zeros_like(self.step_counter), self.step_counter)
-        returns = torch.logical_and(close_enough, facing_goal)
-        # if torch.any(returns):
-        #     print(close_enough, facing_goal)
-        # print("returns", returns)
-        if get_num_subs == False:
-            return returns
-        return returns, num_conditions_met, distance_to_goal+0.1-radius_threshold, angle_degrees
-
-    def get_contact(self):
-        force_matrix = self.scene["contact_sensor"].data.net_forces_w
-        force_matrix[..., 2] = 0
-        forces_magnitude = torch.norm(torch.norm(force_matrix, dim=2), dim=1)  # shape: [batch_size, num_contacts]
-        # вычисляем модуль силы для каждого контакта
-        if force_matrix is not None and force_matrix.numel() > 0:
-            contact_forces = torch.norm(force_matrix, dim=-1)
-            if contact_forces.dim() >= 3:
-                has_contact = torch.any(contact_forces > 0.1, dim=(1, 2))
-            else:
-                has_contact = torch.any(contact_forces > 0.1, dim=1) if contact_forces.dim() == 2 else torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-            # print("c ", has_contact)
-            num_contacts_per_env = torch.sum(contact_forces > 0.1, dim=1)
-            high_contact_envs = num_contacts_per_env >= 1
-        else:
-            print("force_matrix_w is None or empty")
-            high_contact_envs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-        # if torch.any(high_contact_envs):
-        #     print("high_contact_envs ", high_contact_envs)
-        return high_contact_envs
-
-    def update_SR_history(self):
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-
-    def update_success_rate(self, goal_reached):
-        if self.turn_on_controller:
-            return torch.tensor(self.success_rate, device=self.device)
-        
-        # Получаем завершенные эпизоды
-        died, time_out = self._get_dones(self.my_episode_lenght - 1, inner=True)
-        completed = died | time_out
-        
-        if torch.any(completed):
-            # Получаем релевантные среды среди завершенных
-            # Фильтруем завершенные среды, оставляя только релевантные
-            relevant_completed = self._robot._ALL_INDICES[completed] #relevant_env_ids[(relevant_env_ids.view(1, -1) == self._robot._ALL_INDICES[completed].view(-1, 1)).any(dim=0)]
-            success = goal_reached.clone()
-            # Обновляем стеки для релевантных завершенных сред
-            for env_id in self._robot._ALL_INDICES.clone()[completed]:
-                env_id = env_id.item()
-                if not success[env_id]:#here idia is colulate all fault and sucess only on relative envs
-                    self.success_stacks[env_id].append(0)
-                elif env_id in relevant_completed:
-                    self.success_stacks[env_id].append(1)
-                
-                if len(self.success_stacks[env_id]) > self.max_stack_size:
-                    self.success_stacks[env_id].pop(0)
-            # print("self.success_stacks ", self.success_stacks)
-        # Вычисляем процент успеха для всех сред с непустыми стеками
-        # Подсчитываем общий процент успеха по всем релевантным средам
-        total_successes = 0
-        total_elements = 0
-        # print(self.success_stacks)
-        for env_id in range(self.num_envs):
-            stack = self.success_stacks[env_id]
-            if len(stack) == 0:
-                continue
-            total_successes += sum(stack)
-            total_elements += len(stack)
-        # print("update ", self.success_stacks)
-        # Вычисляем процент успеха
-        # print("total_successes ", total_successes, total_elements)
-        self.sr_stack_capacity = total_elements
-        if total_elements > 0:
-            self.success_rate = (total_successes / total_elements) * 100.0
-        else:
-            self.success_rate = 0.0
-        # print(total_elements)
-        if total_elements >= self.num_envs * self.max_stack_size * 0.9:
-            self.sr_stack_full = True
-        # print(success_rates, self.success_rate)
-        return self.success_rate
-    
-    def update_sr_stack(self):
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.sr_stack_full = False
-
-    def _get_dones(self, my_episode_lenght = 256, inner=False) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.is_time_out(my_episode_lenght)
-        
-        has_contact = self.get_contact()
-        self.has_contact = has_contact
-        died = torch.logical_or(
-            torch.logical_or(self.goal_reached(), has_contact),
-            time_out,
-        )
-        if torch.any(died):
-            goal_reached = self.goal_reached()
-            self.episode_counter += died.long()
-            self.success_counter += goal_reached.long()
-            self.event_update_counter += torch.sum(died).item()
-        
-        if not inner:
-            self.episode_length_buf[died] = 0
-        # print("died ", time_out, self.episode_length_buf)
-        return died, time_out
-    
-    def is_time_out(self, max_episode_length=256):
-        if self.first_ep[1]:
-            self.first_ep_step += 1
-            if self.first_ep_step > 1:
-                self.first_ep[1] = False
-            max_episode_length = 4
-        time_out = self.episode_length_buf >= max_episode_length
-        return time_out
-
-    def _reset_idx(self, env_ids: torch.Tensor | None):
-        super()._reset_idx(env_ids)
-        if self.first_ep[0] or env_ids is None or len(env_ids) == self.num_envs:
-            env_ids = self._robot._ALL_INDICES.clone()
-        # Сначала размещаем все объекты
-        if self.first_ep[0]:
-            self.first_ep[0] = False
-            self.scene_manager.randomize_scene(
-                env_ids,
-                mess=False, # или False, в зависимости от режима
-                use_obstacles=self.turn_on_obstacles,
-                all_defoult=True
-            )
-            return
-        num_envs = len(env_ids)
-
-        self.scene_manager.randomize_scene(
-            env_ids,
-            mess=False, # или False, в зависимости от режима
-            use_obstacles=self.turn_on_obstacles,
-            all_defoult=False
-        )
-        self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        goal_pos_local  = self.scene_manager.get_active_goal_state(env_ids)
-        self._desired_pos_w[env_ids, :3] = goal_pos_local 
-        self._desired_pos_w[env_ids, :2] = self.to_global(goal_pos_local , env_ids)
-        # бновляем все объекты на сцене одним махом
-        self.curriculum_learning_module(env_ids) 
-
-        if self.turn_on_controller_step > self.my_episode_lenght and self.turn_on_controller:
-            self.turn_on_controller_step = 0
-            self.turn_on_controller = False
-
-        cond_imitation = (
-            not self.warm and
-            self.sr_stack_full and
-            self.mean_radius != 0 and
-            self.use_controller and
-            not self.turn_on_controller and
-            not self.first_ep[0] and
-            self.turn_off_controller_step > self.my_episode_lenght
-        )
-        if cond_imitation: 
-            self.turn_on_controller_step = 0
-            self.turn_off_controller_step = 0
-            prob = lambda x: torch.rand(1).item() <= x
-            self.turn_on_controller = prob(0.01 * max(10, min(40, 100 - self.success_rate)))
-            print(f"turn controller: {self.turn_on_controller} with SR {self.success_rate}")
-        elif self.cur_step < self.warm_len:
-                if self.cur_step < self.without_imitation:
-                    self.turn_on_controller = False
-                else:
-                    self.turn_on_controller = True
-            
-        
-        if (self.mean_radius >= 2.3 and self.use_obstacles) or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-            if self.turn_on_obstacles_always and self.cur_step % 300:
-                print("[ WARNING ] ostacles allways turn on")
-
-            self.turn_on_obstacles = True
-            if not self.turn_on_obstacles_always:
-                self.min_level_radius = max(2.3, self.mean_radius - 0.3)
-        else:
-            self.turn_on_obstacles = False
-        env_ids = env_ids.to(dtype=torch.long)
-
-        final_distance_to_goal = torch.linalg.norm(
-            self._desired_pos_w[env_ids, :2] - self._robot.data.root_pos_w[env_ids, :2], dim=1
-        ).mean()
-        self._robot.reset(env_ids)
-        if len(env_ids) == self.num_envs:
-            self.episode_length_buf = torch.zeros_like(self.episode_length_buf) #, high=int(self.max_episode_length))
-        self._actions[env_ids] = 0.0
-        min_radius = 1.2
-        robot_pos_local, robot_quats = self.scene_manager.place_robot_for_goal(
-            env_ids,
-            mean_dist=self.mean_radius,
-            min_dist=1.2,
-            max_dist=4.0,
-            angle_error=self.cur_angle_error,
-        )
-        robot_pos  = robot_pos_local
-
-        # print("i'm in path_manager")
-        if self.turn_on_controller or self.imitation:
-            if self.imitation:
-                print("[ WARNING ] imitation mode on")
-            if self.turn_on_controller_step == 0:
-                env_ids_for_control = self._robot._ALL_INDICES.clone()
-                robot_pos_for_control = self._robot.data.default_root_state[env_ids_for_control, :2].clone()
-                robot_pos_for_control[env_ids, :2] = robot_pos
-                goal_pos_for_control = self._desired_pos_w[env_ids_for_control, :2].clone()
-                goal_pos_for_control[env_ids, :2] = goal_pos_local[:, :2]
-            else:
-                env_ids_for_control = env_ids
-                robot_pos_for_control = robot_pos
-                goal_pos_for_control = goal_pos_local[:, :2]
-            paths = None
-            possible_try_steps = 3
-            obstacle_positions_list = self.scene_manager.get_active_obstacle_positions_for_path_planning(env_ids)
-
-            for i in range(possible_try_steps):
-                paths = self.path_manager.get_paths( # Используем старый get_paths
-                    env_ids=env_ids_for_control,
-                    # Передаем данные для генерации ключа
-                    active_obstacle_positions_list=obstacle_positions_list,
-                    start_positions=robot_pos_local,
-                    target_positions=goal_pos_local[:, :2]
-                )
-                if paths is None:
-                    print(f"[ ERROR ] GET NONE PATH {i + 1} times")
-                    self.scene_manager.randomize_scene(
-                        env_ids_for_control,
-                        mess=False, # или False, в зависимости от режима
-                        use_obstacles=self.turn_on_obstacles,
-                    )
-                    goal_pos_local = self.scene_manager.get_active_goal_state(env_ids_for_control)
-                    self._desired_pos_w[env_ids_for_control, :3] = goal_pos_local
-                    self._desired_pos_w[env_ids_for_control, :2] = self.to_global(goal_pos_local, env_ids_for_control)
-                else:
-                    break
-            # print("out path_manager, paths: ", paths, self.turn_on_controller_step)
-            # print(len(paths), len(env_ids_for_control), len(goal_pos_for_control))
-            self.control_module.update_paths(env_ids_for_control, paths, goal_pos_for_control)
-        if self.memory_on:
-            self.memory_manager.reset()
-        # print("in reset robot pose ", robot_pos, goal_pos)
-        
-        self._update_scene_objects(env_ids) #self._robot._ALL_INDICES.clone())
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        default_root_state[:, 2] = 0.1
-        default_root_state[:, 3:7] = robot_quats
-        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-        # Логируем длину эпизодов для сброшенных сред
-        self.total_episode_length += torch.sum(self.episode_lengths[env_ids]).item()
-        self.episode_count += len(env_ids)
-        mean_episode_length = self.total_episode_length / self.episode_count if self.episode_count > 0 else 0.0
-        # self.tensorboard_writer.add_scalar("Metrics/Mean_episode_length", mean_episode_length, self.tensorboard_step)
-        # Сбрасываем счетчик длины для сброшенных сред
-        self.episode_lengths[env_ids] = 0
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # print("distance_to_goal ", distance_to_goal)
-        _, _, r_error, a_error = self.goal_reached(get_num_subs=True)
-        self.previous_distance_error[env_ids] = r_error[env_ids]
-        self.previous_angle_error[env_ids] = a_error[env_ids]
-        self.first_ep[0] = False
-        self.tracker.reset(env_ids)
-        env_ids_for_scene_embeddings = self._robot._ALL_INDICES.clone()
-        # scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        if self.LOG:
-            self.experiment.log_metric("success_rate", self.success_rate, step=self.tensorboard_step)
-            self.experiment.log_metric("mean_radius", self.mean_radius, step=self.tensorboard_step)
-            self.experiment.log_metric("max_angle", self.max_angle_error, step=self.tensorboard_step)
-            # self.experiment.log_metric("use obstacles", self.turn_on_obstacles.float(), step=self.tensorboard_step)
-
-    def to_local(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] - env_origins[env_ids, :2]
-    
-    def to_global(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] + env_origins[env_ids, :2]
-
-    def curriculum_learning_module(self, env_ids: torch.Tensor):
-        # print("self.success_rate ", self.success_rate)
-        if self.warm and self.cur_step >= self.warm_len:
-            self.warm = False
-            self.mean_radius = self.start_mean_radius
-            self.cur_angle_error = 0
-            self._step_update_counter = 0
-            print(f"end worm stage r: {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-        elif not self.warm and not self.turn_on_controller and self.sr_stack_full:
-            if self.success_rate >= self.sr_treshhold:
-                self.success_ep_num += 1
-                self.foult_ep_num = 0
-                if self.success_ep_num > self.num_envs:
-                    self.second_try = max(self.mean_radius, self.second_try)
-                    self.success_ep_num = 0
-                    old_mr = self.mean_radius
-                    old_a = self.cur_angle_error
-                    self.cur_angle_error += self.max_angle_error / 2
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    if self.cur_angle_error > self.max_angle_error:
-                        self.cur_angle_error = 0
-                        if self.mean_radius == 0:
-                            self.mean_radius += 0.3
-                        else:
-                            self.mean_radius += 1
-                        print(f"udate [ UP ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}")
-                    else:
-                        print(f"udate [ UP ] r: {round(self.mean_radius, 2)} a: from {round(old_a, 2)} to {round(self.cur_angle_error, 2)}")
-                    self._step_update_counter = 0
-                    self.update_sr_stack()
-            elif self.success_rate <= 10 or (self._step_update_counter >= 4000 and self.success_rate <= self.sr_treshhold):
-                self.foult_ep_num += 1
-                if self.foult_ep_num > 2000:
-                    self.success_ep_num = 0
-                    self.foult_ep_num = 0
-                    old_mr = self.mean_radius
-                    if self.cur_angle_error == 0:
-                        self.mean_radius += -0.1
-                        self.mean_radius = max(self.min_level_radius, self.mean_radius)
-                    self.cur_angle_error = 0
-                   
-                    self._step_update_counter = 0
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    print(f"udate [ DOWN ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-                    self.update_sr_stack()
-
-        self._obstacle_update_counter += 1
-        return None
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        pass
-
-    def _debug_vis_callback(self, event):
-        pass
-
-    def close(self):
-        # self.tensorboard_writer.close()
-        super().close()
-
-    def _update_scene_objects(self, env_ids: torch.Tensor):
-        """Векторизованное обновление позиций всех объектов в симуляторе."""
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-
-        # Получаем все локальные позиции из scene_manager'а
-        all_local_positions = self.scene_manager.positions
-        
-        # Конвертируем в глобальные координаты
-        # Это может быть медленно, лучше делать это только для нужных env_ids
-        env_origins_expanded = self._terrain.env_origins.unsqueeze(1).expand_as(all_local_positions)
-        all_global_positions = all_local_positions + env_origins_expanded
-        
-        # Создаем тензор для ориентации (по умолчанию Y-up: w=1)
-        all_quats = torch.zeros(self.num_envs, self.scene_manager.num_total_objects, 4, device=self.device)
-        all_quats[..., 0] = 1.0
-        
-        # Собираем полные состояния (поза + ориентация)
-        all_root_states = torch.cat([all_global_positions, all_quats], dim=-1)
-
-        # Итерируемся по объектам, управляемым симулятором
-        for name, object_instances in self.scene_objects.items():
-            # Используем новый атрибут 'object_map'
-            if name not in self.scene_manager.object_map:
-                continue
-            
-            # Получаем индексы для данного типа объектов из object_map
-            indices = self.scene_manager.object_map[name]['indices']
-            
-            # Собираем состояния только для этих объектов
-            object_root_states = all_root_states[:, indices, :]
-            
-            # Обновляем каждый экземпляр этого типа (например, chair_0, chair_1, ...)
-            for i, instance in enumerate(object_instances):
-                # Выбираем срез для i-го экземпляра по всем окружениям
-                instance_states = object_root_states[:, i, :]
-                # Применяем маску: неактивные объекты перемещаем далеко
-                active_mask = self.scene_manager.active[:, indices[i]]
-                inactive_pos = torch.tensor([20.0 + indices[i], 20.0, 0.0], device=self.device)
-                
-                # Используем torch.where для векторизованного обновления позиций
-                final_positions = torch.where(
-                    active_mask.unsqueeze(-1), 
-                    instance_states[:, :3], 
-                    inactive_pos
-                )
-                instance_states[:, :3] = final_positions
-                if name == "bowl":
-                    # Для миски используем Z-up ориентацию (кватернион [1, 0, 0, 0])
-                    rot = torch.tensor([0.0, 0.0, 0.7071, 0.7071],device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                # Записываем состояния в симулятор для всех окружений сразу
-                instance.write_root_pose_to_sim(instance_states, env_ids=self._robot._ALL_INDICES.clone())
-                changeable_indices = self.scene_manager.type_map.get("changeable_color", torch.tensor([], dtype=torch.long))
-                if len(changeable_indices) > 0:
-                    obj_global_idx = indices[i].item()  # глобальный индекс объекта
-                    if len(changeable_indices) > 0:
-                        obj_global_idx = indices[i].item()
-                        is_changeable = bool(((changeable_indices == obj_global_idx).any()).item())
-                        if is_changeable:
-                            color_vec = self.scene_manager.colors[0, obj_global_idx].cpu().tolist()
-                            prim_path_template = self.asset_manager.all_prim_paths[obj_global_idx]
-
-                            # Преобразуем шаблон env_.* -> env_{idx}
-                            for env_id in env_ids.tolist():
-                                prim_path = prim_path_template.replace("env_.*", f"env_{env_id}")
-                                if prim_path is None:
-                                    continue
-                                self._set_object_color(prim_path, obj_global_idx, color_vec)
-
-
-    def _prepare_color_material(self, color_vec: list[float]):
-        """Создаёт OmniPBR-материал для цвета и сохраняет в кеш (если ещё нет)."""
-        from pxr import Gf
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        if color_key in self._material_cache:
-            return self._material_cache[color_key]
-
-        mtl_created = []
-        omni.kit.commands.execute(
-            "CreateAndBindMdlMaterialFromLibrary",
-            mdl_name="OmniPBR.mdl",
-            mtl_name=f"mat_{color_key}",
-            mtl_created_list=mtl_created
-        )
-        if not mtl_created:
-            return None
-
-        mtl_path = mtl_created[0]
-        stage = omni.usd.get_context().get_stage()
-        shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-        try:
-            shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-        except Exception:
-            try:
-                shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                pass
-
-        self._material_cache[color_key] = mtl_path
-        return mtl_path
-
-    def _set_object_color(self, prim_path: str, obj_idx: int, color_vec: list[float]):
-        """Назначает цвет объекту (только если он изменился)."""
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        prev_key = self._applied_color_map.get(obj_idx)
-        if prev_key == color_key:
-            return  # цвет уже установлен
-
-        mtl_path = self._prepare_color_material(color_vec)
-        if mtl_path is None:
-            return
-
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=mtl_path
-        )
-        self._applied_color_map[obj_idx] = color_key
-
-
-    def _apply_color_to_prim(self, prim_path: str, color_vec: list[float]):
-        """
-        Создаёт (или берёт из кеша) материал OmniPBR для указанного цвета и привязывает его к prim_path.
-        color_vec — список/кортеж из 3 чисел [r,g,b] (0..1).
-        """
-        if prim_path is None:
-            return
-
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        # если уже применён к этому объекту — пропускаем
-        # (в _update_scene_objects мы будем сравнивать с self._applied_color_map[obj_idx])
-        print("material cache: ", self._material_cache)
-        if color_key not in self._material_cache:
-            mtl_created = []
-            omni.kit.commands.execute(
-                "CreateAndBindMdlMaterialFromLibrary",
-                mdl_name="OmniPBR.mdl",
-                mtl_name=f"mat_{color_key}",
-                mtl_created_list=mtl_created
-            )
-            if len(mtl_created) == 0:
-                # если по какой-то причине не создалось — выходим
-                return
-            mtl_path = mtl_created[0]
-            # попытка поменять параметр цвета внутри шейдера
-            stage = omni.usd.get_context().get_stage()
-            shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-            try:
-                shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                # у разных материалов имя порта может отличаться
-                try:
-                    shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-                except Exception:
-                    pass
-            self._material_cache[color_key] = mtl_path
-
-        # Привязываем материал (bind) к приму
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=self._material_cache[color_key]
-        )
-
-
-def log_embedding_stats(embedding):
-    mean_val = embedding.mean().item()
-    std_val = embedding.std().item()
-    min_val = embedding.min().item()
-    max_val = embedding.max().item()
-    print(f"[ EM ] mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 3.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 3.py
deleted file mode 100644
index 7d6afe913b..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy 3.py	
+++ /dev/null
@@ -1,1217 +0,0 @@
-# env.py
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
- 
-import gymnasium as gym
-import torch
-import math
-import numpy as np
-import os
-import torchvision.models as models
-import torchvision.transforms as transforms
-from torch import nn
-import random
-
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation, ArticulationCfg, RigidObject, RigidObjectCfg
-from isaaclab.envs import DirectRLEnv, DirectRLEnvCfg
-from isaaclab.envs.ui import BaseEnvWindow
-from isaaclab.markers import VisualizationMarkers
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg, SimulationContext
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import subtract_frame_transforms
-from isaaclab.sensors import TiledCamera, TiledCameraCfg, ContactSensor, ContactSensorCfg
-from .scene_manager import SceneManager
-from .evaluation_manager import EvaluationManager
-from .control_manager import VectorizedPurePursuit
-from .path_manager import Path_manager
-from .memory_manager import Memory_manager, PathTracker
-from .asset_manager import AssetManager
-import omni.kit.commands
-import omni.usd
-import datetime
-# from torch.utils.tensorboard import SummaryWriter
-##
-# Pre-defined configs
-##
-from isaaclab_assets.robots.aloha import ALOHA_CFG
-from isaaclab.markers import CUBOID_MARKER_CFG
-from transformers import CLIPProcessor, CLIPModel
-from PIL import Image
-import omni.kit.commands  # Уже импортировано в вашем коде
-from omni.usd import get_context  # Для доступа к stage
-from pxr import Gf
-
-class WheeledRobotEnvWindow(BaseEnvWindow):
-    def __init__(self, env: 'WheeledRobotEnv', window_name: str = "IsaacLab"):
-        super().__init__(env, window_name)
-        with self.ui_window_elements["main_vstack"]:
-            with self.ui_window_elements["debug_frame"]:
-                with self.ui_window_elements["debug_vstack"]:
-                    self._create_debug_vis_ui_element("targets", self.env)
-
-@configclass
-class WheeledRobotEnvCfg(DirectRLEnvCfg):
-    episode_length_s = 512.0
-    decimation = 8
-    action_space = gym.spaces.Box(
-        low=np.array([-1.0, -1.0], dtype=np.float32),
-        high=np.array([1.0, 1.0], dtype=np.float32),
-        shape=(2,)
-    )
-    # Observation space is now the ResNet18 embedding size (512)
-    m = 1  # Например, 3 эмбеддинга и действия
-    # observation_space = gym.spaces.Box(
-    #     low=-float("inf"),
-    #     high=float("inf"),
-    #     shape=(m * (512 + 3),),  # m * (embedding_size + action_size) + 2 (скорости)
-    #     dtype="float32"
-    # )
-    # TODO automat compute num_total_objects
-    num_total_objects = 10
-
-    observation_space = gym.spaces.Dict({
-        "img": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(512 + 3,), dtype=np.float32),
-        "graph": gym.spaces.Dict({
-            "node_features": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 14), dtype=np.float32),
-            "edge_features": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 6), dtype=np.float32),
-        })
-    })
-    state_space = 0
-    debug_vis = False
-
-    ui_window_class_type = WheeledRobotEnvWindow
-
-    sim: SimulationCfg = SimulationCfg(
-        dt=1/60,
-        render_interval=decimation,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="min",
-            restitution_combine_mode="min",
-            static_friction=0.2,
-            dynamic_friction=0.15,
-            restitution=0.0,
-        ),
-    )
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="plane",
-        collision_group=-1,
-        # physics_material=sim_utils.RigidBodyMaterialCfg(
-        #     friction_combine_mode="min",
-        #     restitution_combine_mode="min",
-        #     static_friction=0.8,
-        #     dynamic_friction=0.6,
-        #     restitution=0.0,
-        # ),
-        debug_vis=False,
-    )
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=64, env_spacing=18, replicate_physics=True)
-    robot: ArticulationCfg = ALOHA_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    wheel_radius = 0.068
-    wheel_distance = 0.34
-    tiled_camera: TiledCameraCfg = TiledCameraCfg(
-        prim_path="/World/envs/env_.*/Robot/box2_Link/Camera",
-        offset=TiledCameraCfg.OffsetCfg(pos=(-0.35, 0, 1.1), rot=(0.99619469809,0,0.08715574274,0), convention="world"),
-        # offset=TiledCameraCfg.OffsetCfg(pos=(0.0, 0, 0.9), rot=(1,0,0,0), convention="world"),
-        data_types=["rgb"],
-        spawn=sim_utils.PinholeCameraCfg(
-            focal_length=35.0, focus_distance=2.0, horizontal_aperture=36, clipping_range=(0.2, 10.0)
-        ),
-        width=224,
-        height=224,
-    )
-    current_dir = os.getcwd()
-    kitchen = sim_utils.UsdFileCfg(
-        usd_path=os.path.join(current_dir, "source/isaaclab_assets/data/aloha_assets", "scenes/scenes_sber_kitchen_for_BBQ/kitchen_new_simple.usd"),
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-    contact_sensor = ContactSensorCfg(
-        prim_path="/World/envs/env_.*/Robot/.*",
-        update_period=0.1,
-        history_length=1,
-        debug_vis=True,
-        filter_prim_paths_expr=["/World/envs/env_.*"],
-    )
-
-class WheeledRobotEnv(DirectRLEnv):
-    cfg: WheeledRobotEnvCfg
-
-    def __init__(self, cfg: WheeledRobotEnvCfg, render_mode: str | None = None, **kwargs):
-        self._super_init = True
-        self.current_dir = os.getcwd()
-        self.config_path=os.path.join(self.current_dir, "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json")
-        super().__init__(cfg, render_mode, **kwargs)
-        self._super_init = False
-        self.eval = False
-        self.eval_name = "CI"
-
-        self.eval_printed = False
-        self.scene_manager = SceneManager(self.num_envs, self.config_path, self.device)
-        self.eval_manager = EvaluationManager(self.num_envs)
-        self.eval_manager.set_task_lists(
-            robot_positions=[[0.0, -1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0],
-                             [1.0, -1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0],
-                             [2.0, -1.0, 0.0], [2.0, 0.0, 0.0], [2.0, 1.0, 0.0]],  # список стартовых позиций
-            angle_errors=[torch.pi, torch.pi*0.9, torch.pi*0.8]                         # список ошибок угла
-        )
-        self.use_controller = True
-        self.imitation = False
-        if self.imitation:
-            self.use_controller = True
-        if self.use_controller:
-            self.path_manager = Path_manager(scene_manager=self.scene_manager, ratio=8.0, shift=[5, 4], device=self.device)
-            self.control_module = VectorizedPurePursuit(num_envs=self.num_envs, device=self.device)
-        self.memory_on = False
-        self.tracker = PathTracker(num_envs=self.num_envs, device=self.device)
-        if self.memory_on:
-            self.memory_manager = Memory_manager(
-                num_envs=self.num_envs,
-                embedding_size=512,  # Размер эмбеддинга ResNet18
-                action_size=2,      # Размер действия (линейная и угловая скорость)
-                history_length=25,  # n = 10, можно настроить
-                device=self.device
-            )
-
-        self._actions = torch.zeros((self.num_envs, 2), device=self.device)
-        self._actions[:, 1] = 0.0
-        self._left_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._right_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._desired_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self._episode_sums = {
-            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-            for key in ["moves"]
-        }
-        self._left_wheel_id = self._robot.find_joints("left_wheel")[0]
-        self._right_wheel_id = self._robot.find_joints("right_wheel")[0]
-
-        self.set_debug_vis(self.cfg.debug_vis)
-        self.Debug = True
-        self.event_update_counter = 0
-        self.episode_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.success_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.step_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.possible_goal_position = []
-        
-        self.delete = 1
-        self.count = 0
-        self._debug_log_enabled = True
-        self._debug_envs_to_log = list(range(min(5, self.num_envs)))
-        self._inconsistencies = []
-        self._debug_step_counter = 0
-        self._debug_log_frequency = 10
-        self.turn_on_controller = False #it is not use or not use controller, it is flag for the first step
-        self.turn_on_controller_step = 0
-        self.my_episode_lenght = 256
-        self.turn_off_controller_step = 0
-        self.use_obstacles = True
-        self.turn_on_obstacles = False
-        self.turn_on_obstacles_always = False
-        if self.use_obstacles:
-            self.use_obstacles = True
-        self.previous_distance_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_angle_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_lin_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_ang_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.angular_speed = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        # Initialize ResNet18 for image embeddings
-        # self.resnet18 = models.resnet18(pretrained=True).to(self.device)
-        # self.resnet18.eval()  # Set to evaluation mode
-        # # Remove the final fully connected layer to get embeddings
-        # self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])
-        # # Image preprocessing for ResNet18
-        # transforms.ToTensor()
-        # self.transform = transforms.Compose([
-        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
-        # ])
-        self.success_rate = 0
-        self.sr_stack_capacity = 0
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-        self._step_update_counter = 0
-        self.mean_radius = 4.3
-        self.max_angle_error = torch.pi / 6
-        self.cur_angle_error = torch.pi / 12
-        self.warm = True
-        self.warm_len = 2500
-        self.without_imitation = self.warm_len / 2
-        self._obstacle_update_counter = 0
-        self.has_contact = torch.full((self.num_envs,), True, dtype=torch.bool, device=self.device)
-        self.sim = SimulationContext.instance()
-        self.obstacle_positions = None
-        self.key = None
-        self.success_ep_num = 0
-        # self.run = wandb.init(project="aloha_direct")
-        self.first_ep = [True, True]
-        self.first_ep_step = 0
-        self.second_ep = True
-        timestamp = datetime.datetime.now().strftime("%m_%d_%H_%M")
-        name = "dev"
-        self.episode_lengths = torch.zeros(self.num_envs, device=self.device)
-        self.episode_count = 0
-        self.total_episode_length = 0.0
-        # self.tensorboard_writer = SummaryWriter(log_dir=f"/home/xiso/IsaacLab/logs/tensorboard/navigation_rl_{name}_{timestamp}")
-        self.tensorboard_step = 0
-        self.cur_step = 0
-        self.velocities = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-        
-        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
-        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
-        self.clip_model.eval()  # Установить в режим оценки
-        self.second_try = 0
-        self.foult_ep_num = 0
-        # Инициализация стеков для хранения успехов (1 - успех, 0 - неуспех)
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.max_stack_size = 20  # Максимальный размер стека
-        self.sr_stack_full = False
-        self.start_mean_radius = 0
-        self.min_level_radius = 0
-        self.sr_treshhold = 85
-        self.LOG = False
-        self.text_embeddings = torch.zeros((self.num_envs, 512), device=self.device)
-        if self.LOG:
-            from comet_ml import start
-            from comet_ml.integration.pytorch import log_model
-            self.experiment = start(
-                api_key="DRYfW6B6VtUQr9llvf3jup57R",
-                project_name="general",
-                workspace="xisonik"
-            )
-        self.print_config_info()
-        self._setup_scene()
-        self.prim_paths = self.asset_manager.all_prim_paths
-        # сразу после создания scene_manager
-        self._material_cache = {}        # key -> material prim path (строка), key = "r_g_b"
-        self._applied_color_map = {}     # obj_index (int) -> color_key (str), чтобы не биндим повторно
-
-
-    def print_config_info(self):
-        print("__________[ CONGIFG INFO ]__________")
-        print(f"|")
-        print(f"| Start mean radius is: {self.mean_radius}")
-        print(f"|")
-        print(f"| Start amx angle is: {self.max_angle_error}")
-        print(f"|")
-        print(f"| Use controller: {self.use_controller}")
-        print(f"|")
-        print(f"| Full imitation: {self.imitation}")
-        print(f"|")
-        print(f"| Use memory: {self.memory_on}")
-        print(f"|")
-        print(f"| Use obstacles: {self.use_obstacles}")
-        print(f"|")
-        print(f"| Start radius: {self.start_mean_radius}, min: {self.min_level_radius}")
-        print(f"|")
-        print(f"| Warm len: {self.warm_len}")
-        print(f"|")
-        print(f"| Turn on obstacles always: {self.turn_on_obstacles_always}")
-        print(f"|")
-        print(f"_______[ CONGIFG INFO CLOSE ]_______")
-
-    def _setup_scene(self):
-        from isaaclab.sensors import ContactSensor
-        import time
-        from pxr import Usd
-        from isaaclab.sim.spawners.from_files import spawn_from_usd
-        import random
-        if self._super_init:
-            self._robot = Articulation(self.cfg.robot)
-            self.scene.articulations["robot"] = self._robot
-            self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-            self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
-            self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
-            self.scene.clone_environments(copy_from_source=True)
-            self._tiled_camera = TiledCamera(self.cfg.tiled_camera)
-            self.scene.sensors["tiled_camera"] = self._tiled_camera
-            # Спавн кухни (статический элемент)
-            spawn_from_usd(
-                prim_path="/World/envs/env_.*/Kitchen",
-                cfg=self.cfg.kitchen,
-                translation=(5.0, 4.0, 0.0),
-                orientation=(0.0, 0.0, 0.0, 1.0),
-            )
-            self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-            self.scene.sensors["contact_sensor"] = self._contact_sensor
-            self.asset_manager = AssetManager(config_path=self.config_path)
-            self.scene_objects = self.asset_manager.spawn_assets_in_scene()
-            
-            # self.scene_manager.update_prims(prim_path)
-            
-
-        # light_cfg = sim_utils.DomeLightCfg(intensity=300.0, color=(0.75, 0.75, 0.75))
-        # light_cfg.func("/World/Light", light_cfg)
-
-    def _get_observations(self) -> dict:
-        self.tensorboard_step += 1
-        self.cur_step += 1
-        self.episode_lengths += 1
-        import os
-        from PIL import Image, ImageDraw, ImageFont
-        # Получение RGB изображений с камеры
-        camera_data = self._tiled_camera.data.output["rgb"].clone()  # Shape: (num_envs, 224, 224, 3)
-        
-        # Преобразование изображений для CLIP
-        # CLIP ожидает изображения в формате PIL или тензоры с правильной нормализацией
-        images = camera_data.cpu().numpy().astype(np.uint8)  # Конвертация в numpy uint8
-        # inputs = self.clip_processor(images=images, return_tensors="pt", padding=True).to(self.device)
-        images_list = [Image.fromarray(im) for im in images]  # если images shape (N,H,W,3) numpy, это даёт список 2D-arrays
-        inputs = self.clip_processor(images=images_list, return_tensors="pt", padding=True)
-        for k, v in inputs.items():
-            inputs[k] = v.to(self.device)
-        # Получение эмбеддингов изображений
-        with torch.no_grad():
-            image_embeddings = self.clip_model.get_image_features(**inputs)  # Shape: (num_envs, 512)
-            image_embeddings = image_embeddings / (image_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        
-        # Получение скоростей робота
-        root_lin_vel_w = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1).unsqueeze(-1)
-        root_ang_vel_w = self._robot.data.root_ang_vel_w[:, 2].unsqueeze(-1)
-        
-        scene_embeddings = self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        scene_embeddings_dict = self.scene_manager.get_graph_obs(self._robot._ALL_INDICES.clone())
-        # obs = torch.cat([image_embeddings, scene_embeddings, text_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs_img = torch.cat([image_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs = {
-            "img": obs_img,
-            "graph": scene_embeddings_dict
-        }
-        self.previous_ang_vel = self.angular_speed
-        # log_embedding_stats(image_embeddings)
-        
-        observations = {"policy": obs}       
-        return observations
-
-    # as they are not affected by the observation space change.
-
-    def _pre_physics_step(self, actions: torch.Tensor):
-        if self.cur_step % 256 == 0:
-            print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-        env_ids = self._robot._ALL_INDICES.clone()
-        self._actions = actions.clone().clamp(-1.0, 1.0)
-
-        nan_mask = torch.isnan(self._actions) | torch.isinf(self._actions)
-        nan_indices = torch.nonzero(nan_mask.any(dim=1), as_tuple=False).squeeze()  # env_ids где любой action NaN/inf
-        if nan_indices.numel() > 0:
-            print(f"[WARNING] NaN/Inf in actions for envs: {nan_indices.tolist()}. Attempting recovery...")
-        r = self.cfg.wheel_radius
-        L = self.cfg.wheel_distance
-        self._step_update_counter += 1
-        if self.turn_on_controller or self.imitation:
-            self.turn_on_controller_step += 1
-            # Получаем текущую ориентацию (yaw) из кватерниона
-            quat = self._robot.data.root_quat_w
-            siny_cosp = 2 * (quat[:, 0] * quat[:, 3] + quat[:, 1] * quat[:, 2])
-            cosy_cosp = 1 - 2 * (quat[:, 2] * quat[:, 2] + quat[:, 3] * quat[:, 3])
-            yaw = torch.atan2(siny_cosp, cosy_cosp)
-            linear_speed, angular_speed = self.control_module.compute_controls(
-                self.to_local(self._robot.data.root_pos_w[:, :2],env_ids),
-                yaw
-            )
-            # angular_speed = -angular_speed 
-            self._actions[:, 0] = (linear_speed / 0.6) - 1
-            self._actions[:, 1] = angular_speed / 2
-            actions.copy_(self._actions.clamp(-1.0, 1.0))
-        else:
-            self.turn_off_controller_step += 1
-            linear_speed = 0.6*(self._actions[:, 0] + 1.0) # [num_envs], всегда > 0
-            angular_speed = 2*self._actions[:, 1]  # [num_envs], оставляем как есть от RL
-        # linear_speed = torch.tensor([0], device=self.device)
-        self.angular_speed = angular_speed
-        self.velocities = torch.stack([linear_speed, angular_speed], dim=1)
-        # if self.tensorboard_step % 4 ==0:
-        # self.delete = -1 * self.delete 
-        # angular_speed = torch.tensor([0], device=self.device)
-        # print("vel is: ", linear_speed, angular_speed)
-        self._left_wheel_vel = (linear_speed - (angular_speed * L / 2)) / r
-        self._right_wheel_vel = (linear_speed + (angular_speed * L / 2)) / r
-        # self._left_wheel_vel = torch.clamp(self._left_wheel_vel, -10, 10)
-        # self._right_wheel_vel = torch.clamp(self._right_wheel_vel, -10, 10)
-
-    def _apply_action(self):
-        wheel_velocities = torch.stack([self._left_wheel_vel, self._right_wheel_vel], dim=1).unsqueeze(-1).to(dtype=torch.float32)
-        self._robot.set_joint_velocity_target(wheel_velocities, joint_ids=[self._left_wheel_id, self._right_wheel_id])
-
-    def _get_rewards(self) -> torch.Tensor:
-        # env_ids = self._robot._ALL_INDICES.clone()
-        # num_envs = len(env_ids)
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        # joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        # joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        # default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        # default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        # default_root_state[:, 2] = 0.1
-        # self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        # self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        # self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-        lin_vel = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1)
-        
-        lin_vel_reward = torch.clamp(lin_vel*0.02, min=0, max=0.15)
-        ang_vel = self._robot.data.root_ang_vel_w[:, 2]
-        ang_vel_reward = torch.abs(self.angular_speed) * 0.1
-        a_penalty = 0.1 * torch.abs(self.angular_speed - self.previous_ang_vel) #+ torch.abs(lin_vel - self.previous_lin_vel))
-        # print("a_penalty ", -a_penalty, self.angular_speed, self.previous_ang_vel )
-        # self.previous_lin_vel = lin_vel
-
-        goal_reached, num_subs, r_error, a_error = self.goal_reached(get_num_subs=True)
-
-        moves = torch.clamp(5 * (self.previous_distance_error - r_error), min=0, max=1) + \
-                    torch.clamp(5 * (self.previous_angle_error - a_error), min=0, max=1)
-        env_ids = self._robot._ALL_INDICES.clone()
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        self.tracker.add_step(env_ids, self.to_local(root_pos_w, env_ids), self.velocities)
-        path_lengths = self.tracker.compute_path_lengths(env_ids)
-
-        moves_reward = moves * 0.1
-        
-        self.previous_distance_error = r_error
-        self.previous_angle_error = a_error
-
-        has_contact = self.get_contact()
-
-        time_out = self.is_time_out(self.my_episode_lenght-1)
-        time_out_penalty = -5 * time_out.float()
-
-        vel_penalty = -1 * (ang_vel_reward + lin_vel_reward)
-        mask = ~goal_reached
-        vel_penalty[mask] = 0
-        lin_vel_reward[goal_reached] = 0
-
-        paths = self.tracker.get_paths(env_ids)
-        # jerk_counts = self.tracker.compute_jerk(env_ids, threshold=0.2)
-        # print(jerk_counts)
-        start_dists = self.eval_manager.get_start_dists(env_ids)
-        if self.turn_on_controller:
-            IL_reward = 0.5
-            punish = - 0.05
-        else:
-            IL_reward = 0
-            punish = (
-                - 0.1
-                - ang_vel_reward / (1 + 2 * self.mean_radius)
-                + lin_vel_reward / (1 + 2 * self.mean_radius)
-            )
-        reward = (
-            IL_reward + punish #* r_error
-            + torch.clamp(goal_reached.float() * 7, min=0, max=15) #* (1 + start_dists) / (1 + path_lengths)
-            - torch.clamp(has_contact.float() * (5 + lin_vel_reward), min=0, max=10)
-        )
-
-        if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-            sr = self.update_success_rate(goal_reached)
-
-        # if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-        # print("path info")
-        # print(path_lengths)
-        # print(r_error)
-        # print("reward ", reward)
-        # print("- 0.1 - 0.05 * r_error ", - 0.1 - 0.05 * r_error)
-        # print("IL_reward * r_error ", IL_reward * r_error)
-        # print("goal_reached ", goal_reached)
-        # print("lin_vel_reward ", lin_vel_reward)
-        # print("torch.clamp(goal_reached ", torch.clamp(goal_reached.float() * 7 * (1 + self.mean_radius) / (1 + path_lengths), min=0, max=10))
-        # print("torch.clamp(has_contact ", torch.clamp(has_contact.float() * (3 + lin_vel_reward), min=0, max=6))
-        # print("ang_vel_reward ", -ang_vel_reward)
-        # print("___________")
-        check = {
-            "moves":moves,
-        }
-        for key, value in check.items():
-            self._episode_sums[key] += value
-
-        # if self.tensorboard_step % 100 == 0:
-        #     self.tensorboard_writer.add_scalar("Metrics/reward", torch.sum(reward), self.tensorboard_step)
-        return reward
-    
-    def quat_rotate(self, quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
-        """
-        Вращение вектора vec кватернионом quat.
-        quat: [N, 4] (w, x, y, z)
-        vec: [N, 3]
-        Возвращает: [N, 3] - вектор vec, повернутый кватернионом quat
-        """
-        w, x, y, z = quat.unbind(dim=1)
-        vx, vy, vz = vec.unbind(dim=1)
-
-        # Кватернионное умножение q * v
-        qw = -x*vx - y*vy - z*vz
-        qx = w*vx + y*vz - z*vy
-        qy = w*vy + z*vx - x*vz
-        qz = w*vz + x*vy - y*vx
-
-        # Обратный кватернион q*
-        rw = w
-        rx = -x
-        ry = -y
-        rz = -z
-
-        # Результат (q * v) * q*
-        rx_new = qw*rx + qx*rw + qy*rz - qz*ry
-        ry_new = qw*ry - qx*rz + qy*rw + qz*rx
-        rz_new = qw*rz + qx*ry - qy*rx + qz*rw
-
-        return torch.stack([rx_new, ry_new, rz_new], dim=1)
-
-
-    def goal_reached(self, angle_threshold: float = 17, radius_threshold: float = 1.3, get_num_subs=False):
-        """
-        Проверяет достижение цели с учётом расстояния и направления взгляда робота.
-        distance_to_goal: [N] расстояния до цели
-        angle_threshold: максимально допустимый угол в радианах между направлением взгляда и вектором на цель
-        Возвращает: [N] булев тензор, True если цель достигнута
-        """
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root: ", self.to_local(root_pos_w))
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # Проверка по расстоянию (например, радиус достижения stored в self.radius)
-        close_enough = distance_to_goal <= radius_threshold
-
-        # Получаем ориентацию робота в виде кватерниона (w, x, y, z)
-        root_quat_w = self._robot.data.root_quat_w  # shape [N, 4]
-
-        # Локальный вектор взгляда робота (вперёд по оси X)
-        local_forward = torch.tensor([1.0, 0.0, 0.0], device=root_quat_w.device, dtype=root_quat_w.dtype)
-        local_forward = local_forward.unsqueeze(0).repeat(root_quat_w.shape[0], 1)  # [N, 3]
-
-        # Вектор взгляда в мировых координатах
-        forward_w = self.quat_rotate(root_quat_w, local_forward)  # [N, 3]
-
-        # Вектор от робота к цели
-        root_pos_w = self._robot.data.root_pos_w  # [N, 3]
-        to_goal = self._desired_pos_w - root_pos_w  # [N, 3]
-
-        # Нормализуем векторы
-        forward_w_norm = torch.nn.functional.normalize(forward_w[:, :2] , dim=1)
-        to_goal_norm = torch.nn.functional.normalize(to_goal[:, :2] , dim=1)
-
-        # Косинус угла между векторами взгляда и направления на цель
-        cos_angle = torch.sum(forward_w_norm * to_goal_norm, dim=1)
-        cos_angle = torch.clamp(cos_angle, -1.0, 1.0)  # для безопасности
-        # direction_to_goal = to_goal
-        # yaw_g = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-
-        # Вычисляем угол между векторами
-        angle = torch.acos(cos_angle)
-        angle_degrees = torch.abs(angle) * 180.0 / 3.141592653589793
-        # Проверяем, что угол меньше порога
-        facing_goal = angle_degrees < angle_threshold
-
-        # Итоговое условие: близко к цели и смотрит в её сторону
-        # print(distance_to_goal, angle_degrees)
-        
-        conditions = torch.stack([close_enough, facing_goal], dim=1)  # shape [N, M]
-        num_conditions_met = conditions.sum(dim=1)  # shape [N], количество True в каждой строк
-
-        # self.step_counter += torch.ones_like(self.step_counter) #torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        # enouth_steps = self.step_counter > 4
-        # returns = torch.logical_and(torch.logical_and(close_enough, facing_goal), enouth_steps)
-        # self.step_counter = torch.where(returns, torch.zeros_like(self.step_counter), self.step_counter)
-        returns = torch.logical_and(close_enough, facing_goal)
-        # if torch.any(returns):
-        #     print(close_enough, facing_goal)
-        # print("returns", returns)
-        if get_num_subs == False:
-            return returns
-        return returns, num_conditions_met, distance_to_goal+0.1-radius_threshold, angle_degrees
-
-    def get_contact(self):
-        force_matrix = self.scene["contact_sensor"].data.net_forces_w
-        force_matrix[..., 2] = 0
-        forces_magnitude = torch.norm(torch.norm(force_matrix, dim=2), dim=1)  # shape: [batch_size, num_contacts]
-        # вычисляем модуль силы для каждого контакта
-        if force_matrix is not None and force_matrix.numel() > 0:
-            contact_forces = torch.norm(force_matrix, dim=-1)
-            if contact_forces.dim() >= 3:
-                has_contact = torch.any(contact_forces > 0.1, dim=(1, 2))
-            else:
-                has_contact = torch.any(contact_forces > 0.1, dim=1) if contact_forces.dim() == 2 else torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-            # print("c ", has_contact)
-            num_contacts_per_env = torch.sum(contact_forces > 0.1, dim=1)
-            high_contact_envs = num_contacts_per_env >= 1
-        else:
-            print("force_matrix_w is None or empty")
-            high_contact_envs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-        # if torch.any(high_contact_envs):
-        #     print("high_contact_envs ", high_contact_envs)
-        return high_contact_envs
-
-    def update_SR_history(self):
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-
-    def update_success_rate(self, goal_reached):
-        if self.turn_on_controller:
-            return torch.tensor(self.success_rate, device=self.device)
-        
-        # Получаем завершенные эпизоды
-        died, time_out = self._get_dones(self.my_episode_lenght - 1, inner=True)
-        completed = died | time_out
-        
-        if torch.any(completed):
-            # Получаем релевантные среды среди завершенных
-            # Фильтруем завершенные среды, оставляя только релевантные
-            relevant_completed = self._robot._ALL_INDICES[completed] #relevant_env_ids[(relevant_env_ids.view(1, -1) == self._robot._ALL_INDICES[completed].view(-1, 1)).any(dim=0)]
-            success = goal_reached.clone()
-            # Обновляем стеки для релевантных завершенных сред
-            for env_id in self._robot._ALL_INDICES.clone()[completed]:
-                env_id = env_id.item()
-                if not success[env_id]:#here idia is colulate all fault and sucess only on relative envs
-                    self.success_stacks[env_id].append(0)
-                elif env_id in relevant_completed:
-                    self.success_stacks[env_id].append(1)
-                
-                if len(self.success_stacks[env_id]) > self.max_stack_size:
-                    self.success_stacks[env_id].pop(0)
-            # print("self.success_stacks ", self.success_stacks)
-        # Вычисляем процент успеха для всех сред с непустыми стеками
-        # Подсчитываем общий процент успеха по всем релевантным средам
-        total_successes = 0
-        total_elements = 0
-        # print(self.success_stacks)
-        for env_id in range(self.num_envs):
-            stack = self.success_stacks[env_id]
-            if len(stack) == 0:
-                continue
-            total_successes += sum(stack)
-            total_elements += len(stack)
-        # print("update ", self.success_stacks)
-        # Вычисляем процент успеха
-        # print("total_successes ", total_successes, total_elements)
-        self.sr_stack_capacity = total_elements
-        if total_elements > 0:
-            self.success_rate = (total_successes / total_elements) * 100.0
-        else:
-            self.success_rate = 0.0
-        # print(total_elements)
-        if total_elements >= self.num_envs * self.max_stack_size * 0.9:
-            self.sr_stack_full = True
-        # print(success_rates, self.success_rate)
-        return self.success_rate
-    
-    def update_sr_stack(self):
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.sr_stack_full = False
-
-    def _get_dones(self, my_episode_lenght = 256, inner=False) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.is_time_out(my_episode_lenght)
-        
-        has_contact = self.get_contact()
-        self.has_contact = has_contact
-        died = torch.logical_or(
-            torch.logical_or(self.goal_reached(), has_contact),
-            time_out,
-        )
-        env_ids = self._robot._ALL_INDICES[died]
-        if torch.any(died):
-            goal_reached = self.goal_reached()
-            if self.eval:
-                # === EVAL LOGGING ===
-                envs_finished = torch.where(died)[0]  # индексы завершившихся эпизодов
-                # успехи конкретно для них
-                successes = goal_reached[envs_finished].float()
-                traj_lens = self.tracker.compute_path_lengths(envs_finished)  # [K]
-                durations = self.tracker.lengths[envs_finished].float()       # [K]
-                start_dists = self.eval_manager.get_start_dists(envs_finished) # [K]
-                # логируем (ВАЖНО: log_results -> ДО next_episode)
-                self.eval_manager.log_results(envs_finished, successes, traj_lens, start_dists, durations)
-                self.eval_manager.next_episode(envs_finished)
-
-                if self.eval_manager.is_all_done() and not self.eval_printed:
-                    import pandas as pd
-                    df, global_stats, pos_stats = self.eval_manager.summarize()
-                    print("=== FINAL EVAL SUMMARY ===")
-                    print(global_stats)
-                    self.eval_printed = True
-
-                    # создаём директорию logs если её нет
-                    log_dir = os.path.join(self.current_dir, "logs/skrl/results")
-                    os.makedirs(log_dir, exist_ok=True)
-
-                    # 1. Сохраняем сырые результаты
-                    save_path = os.path.join(log_dir, f"eval_results_{self.eval_name}.csv")
-                    df.to_csv(save_path, index=False)
-
-                    # 2. Сохраняем агрегированную информацию
-                    summary_path = os.path.join(log_dir, f"eval_summary_{self.eval_name}.csv")
-
-                    # превращаем global_stats и pos_stats в один DataFrame
-                    summary_df = pos_stats.copy()
-                    summary_df["position_idx"] = summary_df.index
-                    summary_df.reset_index(drop=True, inplace=True)
-
-                    # добавляем глобальные метрики как отдельную строку
-                    global_row = global_stats.to_dict()
-                    global_row["position_idx"] = "ALL"
-                    summary_df = pd.concat([summary_df, pd.DataFrame([global_row])], ignore_index=True)
-
-                    summary_df.to_csv(summary_path, index=False)
-
-                    print(f"[EVAL] Results saved to {save_path}")
-                    print(f"[EVAL] Summary saved to {summary_path}")
-
-                
-        if not inner:
-            self.episode_length_buf[died] = 0
-        # print("died ", time_out, self.episode_length_buf)
-        return died, time_out
-    
-    def is_time_out(self, max_episode_length=256):
-        if self.first_ep[1]:
-            self.first_ep[1] = False
-            max_episode_length = 2
-        time_out = self.episode_length_buf >= max_episode_length
-        return time_out
-
-    def _reset_idx(self, env_ids: torch.Tensor | None):
-        super()._reset_idx(env_ids)
-        if self.first_ep[0] or env_ids is None or len(env_ids) == self.num_envs:
-            env_ids = self._robot._ALL_INDICES.clone()
-
-        num_envs = len(env_ids)
-        if self.eval:
-            positions = self.eval_manager.get_positions()
-            self.scene_manager.apply_fixed_positions(env_ids, positions)
-        else:
-            self.scene_manager.randomize_scene(
-                env_ids,
-                mess=False, # или False, в зависимости от режима
-                use_obstacles=self.turn_on_obstacles,
-                all_defoult=False
-            )
-        self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        goal_pos_local  = self.scene_manager.get_active_goal_state(env_ids)
-        # BLOCK TEXT_EMB
-        # colors = ["red" if x.item() > 0 else "green" for x in goal_pos_local[:, 0]]
-        # text_prompts = [f"move to bowl near {c} wall" for c in colors]
-
-        # text_inputs = self.clip_processor(
-        #     text=text_prompts, return_tensors="pt", padding=True
-        # ).to(self.device)
-        # with torch.no_grad():
-        #     text_embeddings = self.clip_model.get_text_features(**text_inputs)
-        #     text_embeddings = text_embeddings / (text_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        # self.text_embeddings[env_ids] = text_embeddings
-
-        # print("goal_pos_local ", goal_pos_local)
-        self._desired_pos_w[env_ids, :3] = goal_pos_local 
-        self._desired_pos_w[env_ids, :2] = self.to_global(goal_pos_local , env_ids)
-
-        self.curriculum_learning_module(env_ids) 
-
-        if self.turn_on_controller_step > self.my_episode_lenght and self.turn_on_controller:
-            self.turn_on_controller_step = 0
-            self.turn_on_controller = False
-        
-        if not self.eval:
-            cond_imitation = (
-                not self.warm and
-                # self.mean_radius >= 3.3 and
-                self.sr_stack_full and
-                self.mean_radius != 0 and
-                self.use_controller and
-                not self.turn_on_controller and
-                not self.first_ep[0] and
-                self.turn_off_controller_step > self.my_episode_lenght
-            )
-            if cond_imitation: 
-                self.turn_on_controller_step = 0
-                self.turn_off_controller_step = 0
-                prob = lambda x: torch.rand(1).item() <= x
-                self.turn_on_controller = prob(0.01 * max(10, min(40, 100 - self.success_rate)))
-                print(f"turn controller: {self.turn_on_controller} with SR {self.success_rate}")
-            elif self.cur_step < self.warm_len:
-                if self.cur_step < self.without_imitation:
-                    self.turn_on_controller = False
-                else:
-                    self.turn_on_controller = True
-                
-        
-        if (self.mean_radius >= 3.3 and self.use_obstacles) or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-        # if self.use_obstacles or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-            if self.turn_on_obstacles_always and self.cur_step % 300:
-                print("[ WARNING ] ostacles allways turn on")
-
-            self.turn_on_obstacles = True
-            if not self.turn_on_obstacles_always and not self.warm and self.min_level_radius < 3.3:
-                print("level_up min_level_radius to: ", 3.3)
-                self.min_level_radius = 3.3
-        else:
-            self.turn_on_obstacles = False
-        env_ids = env_ids.to(dtype=torch.long)
-
-        final_distance_to_goal = torch.linalg.norm(
-            self._desired_pos_w[env_ids, :2] - self._robot.data.root_pos_w[env_ids, :2], dim=1
-        ).mean()
-        self._robot.reset(env_ids)
-        if len(env_ids) == self.num_envs:
-            self.episode_length_buf = torch.zeros_like(self.episode_length_buf) #, high=int(self.max_episode_length))
-        self._actions[env_ids] = 0.0
-        min_radius = 1.2
-        if self.eval:
-            robot_pos_local, robot_quats = self.eval_manager.get_current_tasks(env_ids)
-            # здесь angle_errors можно применить для ориентации робота
-            # вычислим стартовую евклидову дистанцию в локальных координатах
-            start_dists_local = torch.linalg.norm(goal_pos_local[:, :2] - robot_pos_local[:, :2], dim=1)
-            # сохраним стартовые дистанции в eval_manager
-            self.eval_manager.set_start_dists(env_ids, start_dists_local)
-        else:
-            robot_pos_local, robot_quats = self.scene_manager.place_robot_for_goal(
-                env_ids,
-                mean_dist=self.mean_radius,
-                min_dist=1.2,
-                max_dist=4.0,
-                angle_error=self.cur_angle_error,
-            )
-        robot_pos  = robot_pos_local
-        # print("robot_pos_local ", robot_pos_local)
-        # print("bounds ", self.scene_manager.room_bounds)
-        # print("i'm in path_manager")
-        if self.turn_on_controller or self.imitation:
-            if self.imitation:
-                print("[ WARNING ] imitation mode on")
-            if self.turn_on_controller_step == 0:
-                env_ids_for_control = self._robot._ALL_INDICES.clone()
-                robot_pos_for_control = self._robot.data.default_root_state[env_ids_for_control, :2].clone()
-                robot_pos_for_control[env_ids, :2] = robot_pos[:, :2]
-                goal_pos_for_control = self._desired_pos_w[env_ids_for_control, :2].clone()
-                goal_pos_for_control[env_ids, :2] = goal_pos_local[:, :2]
-            else:
-                env_ids_for_control = env_ids
-                robot_pos_for_control = robot_pos
-                goal_pos_for_control = goal_pos_local[:, :2]
-            paths = None
-            possible_try_steps = 3
-            obstacle_positions_list = self.scene_manager.get_active_obstacle_positions_for_path_planning(env_ids)
-
-            for i in range(possible_try_steps):
-                paths = self.path_manager.get_paths( # Используем старый get_paths
-                    env_ids=env_ids_for_control,
-                    # Передаем данные для генерации ключа
-                    active_obstacle_positions_list=obstacle_positions_list,
-                    start_positions=robot_pos_local,
-                    target_positions=goal_pos_local[:, :2]
-                )
-                if paths is None:
-                    print(f"[ ERROR ] GET NONE PATH {i + 1} times")
-                    self.scene_manager.randomize_scene(
-                        env_ids_for_control,
-                        mess=False, # или False, в зависимости от режима
-                        use_obstacles=self.turn_on_obstacles,
-                    )
-                    goal_pos_local = self.scene_manager.get_active_goal_state(env_ids_for_control)
-                    self._desired_pos_w[env_ids_for_control, :3] = goal_pos_local
-                    self._desired_pos_w[env_ids_for_control, :2] = self.to_global(goal_pos_local, env_ids_for_control)
-                else:
-                    break
-            # print("out path_manager, paths: ", paths)
-            # print(len(paths), len(env_ids_for_control), len(goal_pos_for_control))
-            self.control_module.update_paths(env_ids_for_control, paths, goal_pos_for_control)
-        if self.memory_on:
-            self.memory_manager.reset()
-        # print(f"in reset envs: {env_ids} goals:", goal_pos_local[:, :2])
-        
-        self._update_scene_objects(env_ids) #self._robot._ALL_INDICES.clone())
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        default_root_state[:, 2] = 0.1
-        default_root_state[:, 3:7] = robot_quats
-        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-        # Логируем длину эпизодов для сброшенных сред
-        self.total_episode_length += torch.sum(self.episode_lengths[env_ids]).item()
-        self.episode_count += len(env_ids)
-        mean_episode_length = self.total_episode_length / self.episode_count if self.episode_count > 0 else 0.0
-        # self.tensorboard_writer.add_scalar("Metrics/Mean_episode_length", mean_episode_length, self.tensorboard_step)
-        # Сбрасываем счетчик длины для сброшенных сред
-        self.episode_lengths[env_ids] = 0
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # print("distance_to_goal ", distance_to_goal)
-        _, _, r_error, a_error = self.goal_reached(get_num_subs=True)
-        self.previous_distance_error[env_ids] = r_error[env_ids]
-        self.previous_angle_error[env_ids] = a_error[env_ids]
-        self.first_ep[0] = False
-        self.tracker.reset(env_ids)
-        env_ids_for_scene_embeddings = self._robot._ALL_INDICES.clone()
-        # scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        # for i in env_ids_for_scene_embeddings:
-        #     self.scene_manager.print_graph_info(i)
-        if self.LOG and self.sr_stack_full:
-            self.experiment.log_metric("success_rate", self.success_rate, step=self.tensorboard_step)
-            self.experiment.log_metric("mean_radius", self.mean_radius, step=self.tensorboard_step)
-            self.experiment.log_metric("max_angle", self.max_angle_error, step=self.tensorboard_step)
-            # self.experiment.log_metric("use obstacles", self.turn_on_obstacles.float(), step=self.tensorboard_step)
-
-    def to_local(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] - env_origins[env_ids, :2]
-    
-    def to_global(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] + env_origins[env_ids, :2]
-
-    def curriculum_learning_module(self, env_ids: torch.Tensor):
-        # print("self.success_rate ", self.success_rate)
-        # if self.mean_radius > 3.3:
-        #     max_angle_error = torch.pi * 0.8
-        if self.warm and self.cur_step >= self.warm_len:
-            self.warm = False
-            self.mean_radius = self.start_mean_radius
-            self.cur_angle_error = 0
-            self._step_update_counter = 0
-            print(f"end worm stage r: {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-        elif not self.warm and not self.turn_on_controller and self.sr_stack_full:
-            if self.success_rate >= self.sr_treshhold:
-                self.success_ep_num += 1
-                self.foult_ep_num = 0
-                if self.success_ep_num > self.num_envs:
-                    self.second_try = max(self.mean_radius, self.second_try)
-                    self.success_ep_num = 0
-                    old_mr = self.mean_radius
-                    old_a = self.cur_angle_error
-                    self.cur_angle_error += self.max_angle_error / 2
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    if self.cur_angle_error > self.max_angle_error:
-                        self.cur_angle_error = 0
-                        if self.mean_radius == 0:
-                            self.mean_radius += 0.3
-                        else:
-                            self.mean_radius += 1
-                        print(f"udate [ UP ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}")
-                    else:
-                        print(f"udate [ UP ] r: {round(self.mean_radius, 2)} a: from {round(old_a, 2)} to {round(self.cur_angle_error, 2)}")
-                    self._step_update_counter = 0
-                    self.update_sr_stack()
-            elif self.success_rate <= 10 or (self._step_update_counter >= 4000 and self.success_rate <= self.sr_treshhold):
-                self.foult_ep_num += 1
-                if self.foult_ep_num > 2000:
-                    self.success_ep_num = 0
-                    self.foult_ep_num = 0
-                    old_mr = self.mean_radius
-                    if self.cur_angle_error == 0:
-                        self.mean_radius += -0.1
-                        self.mean_radius = max(self.min_level_radius, self.mean_radius)
-                    self.cur_angle_error = 0
-                   
-                    self._step_update_counter = 0
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    print(f"udate [ DOWN ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-                    self.update_sr_stack()
-
-        self._obstacle_update_counter += 1
-        return None
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        pass
-
-    def _debug_vis_callback(self, event):
-        pass
-
-    def close(self):
-        # self.tensorboard_writer.close()
-        super().close()
-
-    def _update_scene_objects(self, env_ids: torch.Tensor):
-        """Векторизованное обновление позиций всех объектов в симуляторе."""
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        # Получаем все локальные позиции из scene_manager'а
-        all_local_positions = self.scene_manager.positions
-        
-        # Конвертируем в глобальные координаты
-        env_origins_expanded = self._terrain.env_origins.unsqueeze(1).expand_as(all_local_positions)
-        all_global_positions = all_local_positions + env_origins_expanded
-        
-        # Создаем тензор для ориентации (по умолчанию Y-up: w=1)
-        all_quats = torch.zeros(self.num_envs, self.scene_manager.num_total_objects, 4, device=self.device)
-        all_quats[..., 0] = 1.0
-        
-        # Собираем полные состояния (поза + ориентация)
-        all_root_states = torch.cat([all_global_positions, all_quats], dim=-1)
-        
-        # Итерируемся по объектам, управляемым симулятором
-        for name, object_instances in self.scene_objects.items():
-            if name not in self.scene_manager.object_map:
-                continue
-            
-            # Получаем индексы для данного типа объектов
-            indices = self.scene_manager.object_map[name]['indices']
-            
-            # Собираем состояния только для этих объектов
-            object_root_states = all_root_states[:, indices, :]
-            
-            # Обновляем каждый экземпляр этого типа
-            for i, instance in enumerate(object_instances):
-                # Выбираем срез для i-го экземпляра по всем окружениям
-                instance_states = object_root_states[:, i, :]
-                # Применяем маску: неактивные объекты берём из default_positions
-                active_mask = self.scene_manager.active[:, indices[i]]
-                # Используем дефолтные позиции из SceneManager
-                inactive_pos = self.scene_manager.default_positions[0, indices[i]]  # (3,)
-                inactive_pos = inactive_pos.expand(self.num_envs, -1)  # (num_envs, 3)
-                # Конвертируем в глобальные координаты
-                inactive_pos_global = inactive_pos + env_origins_expanded[:, indices[i], :]
-                # Векторизованное обновление позиций
-                final_positions = torch.where(
-                    active_mask.unsqueeze(-1),
-                    instance_states[:, :3],
-                    inactive_pos_global
-                )
-                instance_states[:, :3] = final_positions
-                if name == "bowl":
-                    rot = torch.tensor([0.0, 0.0, 0.7071, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                if name == "cabinet":
-                    rot = torch.tensor([0.7071, 0.0, 0.0, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                # Записываем состояния в симулятор
-                instance.write_root_pose_to_sim(instance_states, env_ids=self._robot._ALL_INDICES.clone())
-                # changeable_indices = self.scene_manager.type_map.get("changeable_color", torch.tensor([], dtype=torch.long))
-                # if len(changeable_indices) > 0:
-                #     obj_global_idx = indices[i].item()  # глобальный индекс объекта
-                #     if len(changeable_indices) > 0:
-                #         obj_global_idx = indices[i].item()
-                #         is_changeable = bool(((changeable_indices == obj_global_idx).any()).item())
-                #         if is_changeable:
-                #             color_vec = self.scene_manager.colors[0, obj_global_idx].cpu().tolist()
-                #             prim_path_template = self.asset_manager.all_prim_paths[obj_global_idx]
-
-                #             # Преобразуем шаблон env_.* -> env_{idx}
-                #             for env_id in env_ids.tolist():
-                #                 prim_path = prim_path_template.replace("env_.*", f"env_{env_id}")
-                #                 if prim_path is None:
-                #                     continue
-                #                 self._set_object_color(prim_path, obj_global_idx, color_vec)
-
-    def _prepare_color_material(self, color_vec: list[float]):
-        """Создаёт OmniPBR-материал для цвета и сохраняет в кеш (если ещё нет)."""
-        from pxr import Gf
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        if color_key in self._material_cache:
-            return self._material_cache[color_key]
-
-        mtl_created = []
-        omni.kit.commands.execute(
-            "CreateAndBindMdlMaterialFromLibrary",
-            mdl_name="OmniPBR.mdl",
-            mtl_name=f"mat_{color_key}",
-            mtl_created_list=mtl_created
-        )
-        if not mtl_created:
-            return None
-
-        mtl_path = mtl_created[0]
-        stage = omni.usd.get_context().get_stage()
-        shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-        try:
-            shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-        except Exception:
-            try:
-                shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                pass
-
-        self._material_cache[color_key] = mtl_path
-        return mtl_path
-
-    def _set_object_color(self, prim_path: str, obj_idx: int, color_vec: list[float]):
-        """Назначает цвет объекту (только если он изменился)."""
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        prev_key = self._applied_color_map.get(obj_idx)
-        if prev_key == color_key:
-            return  # цвет уже установлен
-
-        mtl_path = self._prepare_color_material(color_vec)
-        if mtl_path is None:
-            return
-
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=mtl_path
-        )
-        self._applied_color_map[obj_idx] = color_key
-
-
-    def _apply_color_to_prim(self, prim_path: str, color_vec: list[float]):
-        """
-        Создаёт (или берёт из кеша) материал OmniPBR для указанного цвета и привязывает его к prim_path.
-        color_vec — список/кортеж из 3 чисел [r,g,b] (0..1).
-        """
-        if prim_path is None:
-            return
-
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        # если уже применён к этому объекту — пропускаем
-        # (в _update_scene_objects мы будем сравнивать с self._applied_color_map[obj_idx])
-        print("material cache: ", self._material_cache)
-        if color_key not in self._material_cache:
-            mtl_created = []
-            omni.kit.commands.execute(
-                "CreateAndBindMdlMaterialFromLibrary",
-                mdl_name="OmniPBR.mdl",
-                mtl_name=f"mat_{color_key}",
-                mtl_created_list=mtl_created
-            )
-            if len(mtl_created) == 0:
-                # если по какой-то причине не создалось — выходим
-                return
-            mtl_path = mtl_created[0]
-            # попытка поменять параметр цвета внутри шейдера
-            stage = omni.usd.get_context().get_stage()
-            shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-            try:
-                shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                # у разных материалов имя порта может отличаться
-                try:
-                    shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-                except Exception:
-                    pass
-            self._material_cache[color_key] = mtl_path
-
-        # Привязываем материал (bind) к приму
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=self._material_cache[color_key]
-        )
-
-
-def log_embedding_stats(embedding):
-    mean_val = embedding.mean().item()
-    std_val = embedding.std().item()
-    min_val = embedding.min().item()
-    max_val = embedding.max().item()
-    print(f"[ EM ] mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy.py
deleted file mode 100644
index 8c1d7ac158..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env copy.py	
+++ /dev/null
@@ -1,1137 +0,0 @@
-# env.py
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
- 
-import gymnasium as gym
-import torch
-import math
-import numpy as np
-import os
-import torchvision.models as models
-import torchvision.transforms as transforms
-from torch import nn
-import random
-
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation, ArticulationCfg, RigidObject, RigidObjectCfg
-from isaaclab.envs import DirectRLEnv, DirectRLEnvCfg
-from isaaclab.envs.ui import BaseEnvWindow
-from isaaclab.markers import VisualizationMarkers
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg, SimulationContext
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import subtract_frame_transforms
-from isaaclab.sensors import TiledCamera, TiledCameraCfg, ContactSensor, ContactSensorCfg
-from .scene_manager import Scene_manager
-from .control_manager import VectorizedPurePursuit
-from .path_manager import Path_manager
-from .memory_manager import Memory_manager, PathTracker
-from .asset_manager import Asset_paths
-import omni.kit.commands
-import omni.usd
-import datetime
-# from torch.utils.tensorboard import SummaryWriter
-##
-# Pre-defined configs
-##
-from isaaclab_assets.robots.aloha import ALOHA_CFG
-from isaaclab.markers import CUBOID_MARKER_CFG
-from transformers import CLIPProcessor, CLIPModel
-Asset_paths_manager = Asset_paths()
-from PIL import Image
-import omni.kit.commands  # Уже импортировано в вашем коде
-from omni.usd import get_context  # Для доступа к stage
-
-class WheeledRobotEnvWindow(BaseEnvWindow):
-    def __init__(self, env: 'WheeledRobotEnv', window_name: str = "IsaacLab"):
-        super().__init__(env, window_name)
-        with self.ui_window_elements["main_vstack"]:
-            with self.ui_window_elements["debug_frame"]:
-                with self.ui_window_elements["debug_vstack"]:
-                    self._create_debug_vis_ui_element("targets", self.env)
-
-@configclass
-class WheeledRobotEnvCfg(DirectRLEnvCfg):
-    episode_length_s = 512.0
-    decimation = 8
-    action_space = gym.spaces.Box(
-        low=np.array([-1.0, -1.0], dtype=np.float32),
-        high=np.array([1.0, 1.0], dtype=np.float32),
-        shape=(2,)
-    )
-    # Observation space is now the ResNet18 embedding size (512)
-    m = 1  # Например, 3 эмбеддинга и действия
-    observation_space = gym.spaces.Box(
-        low=-float("inf"),
-        high=float("inf"),
-        shape=(m * (512 + 3 + 8),),  # m * (embedding_size + action_size) + 2 (скорости)
-        dtype="float32"
-    )
-    state_space = 0
-    debug_vis = False
-
-    ui_window_class_type = WheeledRobotEnvWindow
-
-    sim: SimulationCfg = SimulationCfg(
-        dt=1/60,
-        render_interval=decimation,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="min",
-            restitution_combine_mode="min",
-            static_friction=0.2,
-            dynamic_friction=0.15,
-            restitution=0.0,
-        ),
-    )
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="plane",
-        collision_group=-1,
-        # physics_material=sim_utils.RigidBodyMaterialCfg(
-        #     friction_combine_mode="min",
-        #     restitution_combine_mode="min",
-        #     static_friction=0.8,
-        #     dynamic_friction=0.6,
-        #     restitution=0.0,
-        # ),
-        debug_vis=False,
-    )
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=64, env_spacing=15, replicate_physics=True)
-    robot: ArticulationCfg = ALOHA_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    wheel_radius = 0.068
-    wheel_distance = 0.34
-    tiled_camera: TiledCameraCfg = TiledCameraCfg(
-        prim_path="/World/envs/env_.*/Robot/box2_Link/Camera",
-        offset=TiledCameraCfg.OffsetCfg(pos=(-0.35, 0, 1.1), rot=(0.99619469809,0,0.08715574274,0), convention="world"),
-        # offset=TiledCameraCfg.OffsetCfg(pos=(0.0, 0, 0.9), rot=(1,0,0,0), convention="world"),
-        data_types=["rgb"],
-        spawn=sim_utils.PinholeCameraCfg(
-            focal_length=35.0, focus_distance=2.0, horizontal_aperture=36, clipping_range=(0.2, 10.0)
-        ),
-        width=224,
-        height=224,
-    )
-    kitchen = sim_utils.UsdFileCfg(
-        usd_path=Asset_paths_manager.kitchen_usd_path,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-    contact_sensor = ContactSensorCfg(
-        prim_path="/World/envs/env_.*/Robot/.*",
-        update_period=0.1,
-        history_length=1,
-        debug_vis=True,
-        filter_prim_paths_expr=["/World/envs/env_.*"],
-    )
-
-    table = sim_utils.UsdFileCfg(
-        usd_path=Asset_paths_manager.table_usd_path,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-
-    # Конфигурация миски (цели)
-    bowl = sim_utils.UsdFileCfg(
-        usd_path=Asset_paths_manager.bowl_usd_path,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,  # Миска неподвижна
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,  # Отключаем коллизии
-        ),
-    )
-
-class WheeledRobotEnv(DirectRLEnv):
-    cfg: WheeledRobotEnvCfg
-
-    def __init__(self, cfg: WheeledRobotEnvCfg, render_mode: str | None = None, **kwargs):
-        super().__init__(cfg, render_mode, **kwargs)
-        self._actions = torch.zeros((self.num_envs, 2), device=self.device)
-        self._actions[:, 1] = 0.0
-        self._left_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._right_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._desired_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self._episode_sums = {
-            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-            for key in ["moves"]
-        }
-        self._left_wheel_id = self._robot.find_joints("left_wheel")[0]
-        self._right_wheel_id = self._robot.find_joints("right_wheel")[0]
-
-        self.set_debug_vis(self.cfg.debug_vis)
-        self.Debug = True
-        self.event_update_counter = 0
-        self.episode_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.success_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.step_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.possible_goal_position = []
-        self.scene_manager = Scene_manager(self.num_envs, self.device, num_obstacles=3)
-        self.use_controller = True
-        self.imitation = False
-        if self.imitation:
-            self.use_controller = True
-        if self.use_controller:
-            self.path_manager = Path_manager(scene_manager=self.scene_manager, ratio=8.0, shift=[5, 4], device=self.device)
-            self.control_module = VectorizedPurePursuit(num_envs=self.num_envs, device=self.device)
-        self.memory_on = False
-        self.tracker = PathTracker(num_envs=self.num_envs, device=self.device)
-        if self.memory_on:
-            self.memory_manager = Memory_manager(
-                num_envs=self.num_envs,
-                embedding_size=512,  # Размер эмбеддинга ResNet18
-                action_size=2,      # Размер действия (линейная и угловая скорость)
-                history_length=25,  # n = 10, можно настроить
-                device=self.device
-            )
-        self.delete = 1
-        self.count = 0
-        self._debug_log_enabled = True
-        self._debug_envs_to_log = list(range(min(5, self.num_envs)))
-        self._inconsistencies = []
-        self._debug_step_counter = 0
-        self._debug_log_frequency = 10
-        self.turn_on_controller = False #it is not use or not use controller, it is flag for the first step
-        self.turn_on_controller_step = 0
-        self.my_episode_lenght = 256
-        self.turn_off_controller_step = 0
-        self.use_obstacles = True
-        self.turn_on_obstacles = False
-        self.turn_on_obstacles_always = False
-        if self.use_obstacles:
-            self.use_obstacles = True
-        self.previous_distance_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_angle_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_lin_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_ang_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.angular_speed = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        # Initialize ResNet18 for image embeddings
-        # self.resnet18 = models.resnet18(pretrained=True).to(self.device)
-        # self.resnet18.eval()  # Set to evaluation mode
-        # # Remove the final fully connected layer to get embeddings
-        # self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])
-        # # Image preprocessing for ResNet18
-        # transforms.ToTensor()
-        # self.transform = transforms.Compose([
-        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
-        # ])
-        self.success_rate = 0
-        self.sr_stack_capacity = 0
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-        self._step_update_counter = 0
-        self.mean_radius = 3.3
-        self.max_angle_error = torch.pi / 6
-        self.cur_angle_error = torch.pi / 12
-        self.warm = True
-        self.warm_len = 2048
-        self.without_imitation = self.warm_len / 2
-        self._obstacle_update_counter = 0
-        self.has_contact = torch.full((self.num_envs,), True, dtype=torch.bool, device=self.device)
-        self.sim = SimulationContext.instance()
-        self.obstacle_positions = None
-        self.key = None
-        self.success_ep_num = 0
-        # self.run = wandb.init(project="aloha_direct")
-        self.first_ep = True
-        self.first_ep_step = 0
-        self.second_ep = True
-        timestamp = datetime.datetime.now().strftime("%m_%d_%H_%M")
-        name = "dev"
-        self.episode_lengths = torch.zeros(self.num_envs, device=self.device)
-        self.episode_count = 0
-        self.total_episode_length = 0.0
-        # self.tensorboard_writer = SummaryWriter(log_dir=f"/home/xiso/IsaacLab/logs/tensorboard/navigation_rl_{name}_{timestamp}")
-        self.tensorboard_step = 0
-        self.cur_step = 0
-        self.velocities = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-        
-        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
-        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
-        self.clip_model.eval()  # Установить в режим оценки
-        self.second_try = 0
-        self.foult_ep_num = 0
-        # Инициализация стеков для хранения успехов (1 - успех, 0 - неуспех)
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.max_stack_size = 20  # Максимальный размер стека
-        self.sr_stack_full = False
-        self.start_mean_radius = 0
-        self.min_level_radius = 0
-        self.sr_treshhold = 85
-        self.LOG = False
-        if self.LOG:
-            from comet_ml import start
-            from comet_ml.integration.pytorch import log_model
-            self.experiment = start(
-                api_key="DRYfW6B6VtUQr9llvf3jup57R",
-                project_name="general",
-                workspace="xisonik"
-            )
-        self.print_config_info()
-
-    def print_config_info(self):
-        print("__________[ CONGIFG INFO ]__________")
-        print(f"|")
-        print(f"| Start mean radius is: {self.mean_radius}")
-        print(f"|")
-        print(f"| Start amx angle is: {self.max_angle_error}")
-        print(f"|")
-        print(f"| Use controller: {self.use_controller}")
-        print(f"|")
-        print(f"| Full imitation: {self.imitation}")
-        print(f"|")
-        print(f"| Use memory: {self.memory_on}")
-        print(f"|")
-        print(f"| Use obstacles: {self.use_obstacles}")
-        print(f"|")
-        print(f"| Start radius: {self.start_mean_radius}, min: {self.min_level_radius}")
-        print(f"|")
-        print(f"| Warm len: {self.warm_len}")
-        print(f"|")
-        print(f"| Turn on obstacles always: {self.turn_on_obstacles_always}")
-        print(f"|")
-        print(f"_______[ CONGIFG INFO CLOSE ]_______")
-
-    def _setup_scene(self):
-        from isaaclab.sensors import ContactSensor
-        import time
-        from pxr import Usd
-        
-        self._robot = Articulation(self.cfg.robot)
-        self.scene.articulations["robot"] = self._robot
-        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
-        self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
-        self.scene.clone_environments(copy_from_source=True)
-        self._tiled_camera = TiledCamera(self.cfg.tiled_camera)
-        self.scene.sensors["tiled_camera"] = self._tiled_camera
-        self.set_env()
-        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-        self.scene.sensors["contact_sensor"] = self._contact_sensor
-
-        # light_cfg = sim_utils.DomeLightCfg(intensity=300.0, color=(0.75, 0.75, 0.75))
-        # light_cfg.func("/World/Light", light_cfg)
-
-    def set_env(self):
-        from isaaclab.sim.spawners.from_files import spawn_from_usd
-        self.obstacle_positions = None
-        self.chair_prims = [[] for _ in range(self.cfg.scene.num_envs)]
-        spawn_from_usd(
-            prim_path="/World/envs/env_.*/Kitchen",
-            cfg=self.cfg.kitchen,
-            translation=(5.0, 4.0, 0.0),
-            orientation=(0.0, 0.0, 0.0, 1.0),
-        )
-        goal_pos = (-4.5, 0, 0.65)  # z=0.8 для поверхности стола
-        self.possible_goal_position = []
-        for i in range(3):
-            translation=(-4.5, -1 + i*1.0, 0.0)
-            self.possible_goal_position.append(list(translation))
-            spawn_from_usd(prim_path=f"/World/envs/env_.*/Table_{i}", cfg=self.cfg.table, translation=translation, orientation=(0.7071, 0.0, 0.0, 0.7071))  # Default за сценой
-        print("possible_goal_position ", self.possible_goal_position)
-        stage = omni.usd.get_context().get_stage()
-        for env_id in range(self.cfg.scene.num_envs):
-            for prim_path in [
-                f"/World/envs/env_{env_id}/Kitchen"
-            ]:
-                prim = stage.GetPrimAtPath(prim_path)
-                if not prim.IsValid():
-                    raise RuntimeError(f"Failed to create prim at {prim_path}")
-                # print(f"Created prim {prim_path}, Type: {prim.GetTypeName()}")
-        import random
-
-        self.chair_objects = [[] for _ in range(6)]
-        self.goal_objects = [[] for _ in range(6)]
-        grid_x = [-2.0, -1.0]
-        grid_y = [-1.0, 0.0, 1.0]
-        initial_positions = [(-2.0, -1.0, 0.0), (-2.0, 0.0, 0.0), (-2.0, 1.0, 0.0),
-                            (-1.0, -1.0, 0.0), (-1.0, 0.0, 0.0), (-1.0, 1.0, 0.0)]  # Позиции для ключа [7, 7]
-        # for env_id in range(self.cfg.scene.num_envs):
-        for i in range(3):
-            chair_cfg = RigidObjectCfg(
-                prim_path=f"/World/envs/env_.*/Chair_{i}",  # Уникальный путь для каждого стула в каждой среде
-                spawn=sim_utils.UsdFileCfg(
-                    usd_path=Asset_paths_manager.chair_usd_path,
-                    rigid_props=sim_utils.RigidBodyPropertiesCfg(
-                        rigid_body_enabled=True,
-                        kinematic_enabled=True,
-                    ),
-                    collision_props=sim_utils.CollisionPropertiesCfg(
-                        collision_enabled=True,
-                    ),
-                ),
-                init_state=RigidObjectCfg.InitialStateCfg(pos=(6.0 + initial_positions[i][0], initial_positions[i][1], 0.0), rot=(0.0, 0.0, 0.7071, 0.7071)),
-            )
-            chair_object = RigidObject(cfg=chair_cfg)
-            self.chair_objects[i] = chair_object  # Добавляем в список для конкретной среды
-
-            goal_cfg = RigidObjectCfg(
-                prim_path=f"/World/envs/env_.*/Goal_{i}",  # Уникальный путь для каждого стула в каждой среде
-                spawn=sim_utils.UsdFileCfg(
-                    usd_path=Asset_paths_manager.bowl_usd_path,
-                    rigid_props=sim_utils.RigidBodyPropertiesCfg(
-                        rigid_body_enabled=True,
-                        kinematic_enabled=True,
-                    ),
-                    collision_props=sim_utils.CollisionPropertiesCfg(
-                        collision_enabled=False,
-                    ),
-                ),
-                init_state=RigidObjectCfg.InitialStateCfg(pos=(9.0 + initial_positions[i][0], initial_positions[i][1], 0.6)),
-            )
-            goal_objects = RigidObject(cfg=goal_cfg)
-            self.goal_objects[i] = goal_objects  # Добавляем в список для конкретной среды
-
-    def _get_observations(self) -> dict:
-        self.tensorboard_step += 1
-        self.cur_step += 1
-        self.episode_lengths += 1
-        import os
-        from PIL import Image, ImageDraw, ImageFont
-        # Получение RGB изображений с камеры
-        camera_data = self._tiled_camera.data.output["rgb"].clone()  # Shape: (num_envs, 224, 224, 3)
-        
-        # Преобразование изображений для CLIP
-        # CLIP ожидает изображения в формате PIL или тензоры с правильной нормализацией
-        images = camera_data.cpu().numpy().astype(np.uint8)  # Конвертация в numpy uint8
-        # inputs = self.clip_processor(images=images, return_tensors="pt", padding=True).to(self.device)
-        images_list = [Image.fromarray(im) for im in images]  # если images shape (N,H,W,3) numpy, это даёт список 2D-arrays
-        inputs = self.clip_processor(images=images_list, return_tensors="pt", padding=True)
-        for k, v in inputs.items():
-            inputs[k] = v.to(self.device)
-        # Получение эмбеддингов изображений
-        with torch.no_grad():
-            image_embeddings = self.clip_model.get_image_features(**inputs)  # Shape: (num_envs, 512)
-            image_embeddings = image_embeddings / (image_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        
-        # Получение скоростей робота
-        root_lin_vel_w = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1).unsqueeze(-1)
-        root_ang_vel_w = self._robot.data.root_ang_vel_w[:, 2].unsqueeze(-1)
-        
-        # Обновление памяти, если используется
-        # if self.memory_on:
-        #     velocities = torch.cat([root_lin_vel_w, root_ang_vel_w], dim=-1)
-        #     self.memory_manager.update(image_embeddings, velocities)
-        #     memory_data = self.memory_manager.get_observations() 
-        #     obs = torch.cat([memory_data], dim=-1)
-        # else:
-        
-         # только в первом шаге
-        # if self.tensorboard_step % 50 == 0:
-        #     save_dir = "/home/xiso/Downloads/assets/tmp"
-        #     os.makedirs(save_dir, exist_ok=True)
-
-        #     for i in range(min(4, self.num_envs)):  # первые 4 среды
-        #         img_np = camera_data[i].cpu().numpy().astype(np.uint8)  # (H,W,3)
-        #         img_pil = Image.fromarray(img_np)
-        #         pos = self.to_local(self._robot.data.root_pos_w[:, :2])
-        #         # Добавим подпись с позициями
-        #         draw = ImageDraw.Draw(img_pil)
-        #         text = f"env {i}\nroot_pos_w: {pos[i, :2].cpu().numpy()}\n" \
-        #             f"goal_pos: {self._desired_pos_w[i, :2].cpu().numpy()}"
-        #         draw.text((5, 5), text, fill=(255, 255, 255))
-
-        #         img_pil.save(os.path.join(save_dir, f"env_{i}_step_{self.tensorboard_step}.png"))
-
-        #     print(f"[DEBUG] Saved first 4 env images with positions to {save_dir} {self.tensorboard_step}")
-        env_ids = self._robot._ALL_INDICES.clone()
-        scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        # print(image_embeddings.shape)
-        
-        # obs = torch.cat([image_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs = torch.cat([image_embeddings, scene_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-
-        self.previous_ang_vel = self.angular_speed
-        # log_embedding_stats(image_embeddings)
-        
-        observations = {"policy": obs}
-        return observations
-
-    # as they are not affected by the observation space change.
-
-    def _pre_physics_step(self, actions: torch.Tensor):
-        if self.cur_step % 256 == 0:
-            print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-        env_ids = self._robot._ALL_INDICES.clone()
-        self._actions = actions.clone().clamp(-1.0, 1.0)
-        # nan_mask = torch.isnan(self._actions)
-        # # Получаем индексы элементов с NaN
-        # nan_indices = torch.nonzero(nan_mask, as_tuple=False)  # будет тензор с позициями
-        # nan_mask = torch.isnan(self._actions).any(dim=1)   # [num_envs]
-        # if nan_mask.any():
-        #     bad_envs = torch.nonzero(nan_mask, as_tuple=False).squeeze(-1)
-        #     print(f"[ WARNING ] NaN actions detected in envs {bad_envs.tolist()} — resetting them.")
-            
-        #     # Обнулить действия, чтобы симулятор не упал
-        #     self._actions[bad_envs] = 0.0
-        #     actions[bad_envs] = 0.0
-
-        #     # Сбросить только испорченные среды
-        #     self._reset_idx(bad_envs)
-
-        #     return
-
-        nan_mask = torch.isnan(self._actions) | torch.isinf(self._actions)
-        nan_indices = torch.nonzero(nan_mask.any(dim=1), as_tuple=False).squeeze()  # env_ids где любой action NaN/inf
-        if nan_indices.numel() > 0:
-            print(f"[WARNING] NaN/Inf in actions for envs: {nan_indices.tolist()}. Attempting recovery...")
-            self._actions[nan_indices] = 0.0  # Заменить NaN на safe value (0), чтобы не крашить step
-            self.recover_corrupted_envs(nan_indices)  # Новый метод для recovery (см. ниже)
-        r = self.cfg.wheel_radius
-        L = self.cfg.wheel_distance
-        self._step_update_counter += 1
-        if self.turn_on_controller or self.imitation:
-            self.turn_on_controller_step += 1
-            # Получаем текущую ориентацию (yaw) из кватерниона
-            quat = self._robot.data.root_quat_w
-            siny_cosp = 2 * (quat[:, 0] * quat[:, 3] + quat[:, 1] * quat[:, 2])
-            cosy_cosp = 1 - 2 * (quat[:, 2] * quat[:, 2] + quat[:, 3] * quat[:, 3])
-            yaw = torch.atan2(siny_cosp, cosy_cosp)
-            linear_speed, angular_speed = self.control_module.compute_controls(
-                self.to_local(self._robot.data.root_pos_w[:, :2],env_ids),
-                yaw
-            )
-            # angular_speed = -angular_speed 
-            self._actions[:, 0] = (linear_speed / 0.6) - 1
-            self._actions[:, 1] = angular_speed / 2
-            actions.copy_(self._actions.clamp(-1.0, 1.0))
-        else:
-            self.turn_off_controller_step += 1
-            linear_speed = 0.6*(self._actions[:, 0] + 1.0) # [num_envs], всегда > 0
-            angular_speed = 2*self._actions[:, 1]  # [num_envs], оставляем как есть от RL
-        # linear_speed = torch.tensor([0], device=self.device)
-        self.angular_speed = angular_speed
-        self.velocities = torch.stack([linear_speed, angular_speed], dim=1)
-        # if self.tensorboard_step % 4 ==0:
-        # self.delete = -1 * self.delete 
-        # angular_speed = torch.tensor([0], device=self.device)
-        # print("vel is: ", linear_speed, angular_speed)
-        self._left_wheel_vel = (linear_speed - (angular_speed * L / 2)) / r
-        self._right_wheel_vel = (linear_speed + (angular_speed * L / 2)) / r
-        # self._left_wheel_vel = torch.clamp(self._left_wheel_vel, -10, 10)
-        # self._right_wheel_vel = torch.clamp(self._right_wheel_vel, -10, 10)
-
-    def _apply_action(self):
-        wheel_velocities = torch.stack([self._left_wheel_vel, self._right_wheel_vel], dim=1).unsqueeze(-1).to(dtype=torch.float32)
-        self._robot.set_joint_velocity_target(wheel_velocities, joint_ids=[self._left_wheel_id, self._right_wheel_id])
-
-    def _get_rewards(self) -> torch.Tensor:
-        lin_vel = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1)
-        
-        lin_vel_reward = torch.clamp(lin_vel*0.02, min=0, max=0.15)
-        ang_vel = self._robot.data.root_ang_vel_w[:, 2]
-        ang_vel_reward = torch.abs(self.angular_speed) * 0.1
-        a_penalty = 0.1 * torch.abs(self.angular_speed - self.previous_ang_vel) #+ torch.abs(lin_vel - self.previous_lin_vel))
-        # print("a_penalty ", -a_penalty, self.angular_speed, self.previous_ang_vel )
-        # self.previous_lin_vel = lin_vel
-
-        goal_reached, num_subs, r_error, a_error = self.goal_reached(get_num_subs=True)
-
-        moves = torch.clamp(5 * (self.previous_distance_error - r_error), min=0, max=1) + \
-                    torch.clamp(5 * (self.previous_angle_error - a_error), min=0, max=1)
-        env_ids = self._robot._ALL_INDICES.clone()
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        self.tracker.add_step(env_ids, self.to_local(root_pos_w, env_ids), self.velocities)
-        path_lengths = self.tracker.compute_path_lengths(env_ids)
-
-        moves_reward = moves * 0.1
-        
-        self.previous_distance_error = r_error
-        self.previous_angle_error = a_error
-
-        has_contact = self.get_contact()
-
-        time_out = self.is_time_out(self.my_episode_lenght-1)
-        time_out_penalty = -5 * time_out.float()
-
-        vel_penalty = -1 * (ang_vel_reward + lin_vel_reward)
-        mask = ~goal_reached
-        vel_penalty[mask] = 0
-        lin_vel_reward[goal_reached] = 0
-
-        paths = self.tracker.get_paths(env_ids)
-        # jerk_counts = self.tracker.compute_jerk(env_ids, threshold=0.2)
-
-        # print(jerk_counts)
-        
-        if self.turn_on_controller:
-            IL_reward = 0.5
-            punish = 0
-        else:
-            IL_reward = 0
-            punish = (
-                - 0.1 * (r_error * 0.2 + 1)
-                - ang_vel_reward / (1 + self.mean_radius)
-                + lin_vel_reward / (1 + self.mean_radius)
-            )
-        reward = (
-            IL_reward + punish #* r_error
-            + torch.clamp(goal_reached.float() * 7 * (1 + self.scene_manager.get_start_dist_error()) / (1 + path_lengths), min=0, max=15)
-            - torch.clamp(has_contact.float() * (7 + lin_vel_reward), min=0, max=10)
-        )
-
-        if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-            sr = self.update_success_rate(goal_reached)
-
-        # if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-        # print("path info")
-        # print(path_lengths)
-        # print(r_error)
-        # print("reward ", reward)
-        # print("- 0.1 - 0.05 * r_error ", - 0.1 - 0.05 * r_error)
-        # print("IL_reward * r_error ", IL_reward * r_error)
-        # print("goal_reached ", goal_reached)
-        # print("lin_vel_reward ", lin_vel_reward)
-        # print("torch.clamp(goal_reached ", torch.clamp(goal_reached.float() * 7 * (1 + self.mean_radius) / (1 + path_lengths), min=0, max=10))
-        # print("torch.clamp(has_contact ", torch.clamp(has_contact.float() * (3 + lin_vel_reward), min=0, max=6))
-        # print("ang_vel_reward ", -ang_vel_reward)
-        # print("___________")
-        check = {
-            "moves":moves,
-        }
-        for key, value in check.items():
-            self._episode_sums[key] += value
-
-        # if self.tensorboard_step % 100 == 0:
-        #     self.tensorboard_writer.add_scalar("Metrics/reward", torch.sum(reward), self.tensorboard_step)
-        return reward
-    
-    def quat_rotate(self, quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
-        """
-        Вращение вектора vec кватернионом quat.
-        quat: [N, 4] (w, x, y, z)
-        vec: [N, 3]
-        Возвращает: [N, 3] - вектор vec, повернутый кватернионом quat
-        """
-        w, x, y, z = quat.unbind(dim=1)
-        vx, vy, vz = vec.unbind(dim=1)
-
-        # Кватернионное умножение q * v
-        qw = -x*vx - y*vy - z*vz
-        qx = w*vx + y*vz - z*vy
-        qy = w*vy + z*vx - x*vz
-        qz = w*vz + x*vy - y*vx
-
-        # Обратный кватернион q*
-        rw = w
-        rx = -x
-        ry = -y
-        rz = -z
-
-        # Результат (q * v) * q*
-        rx_new = qw*rx + qx*rw + qy*rz - qz*ry
-        ry_new = qw*ry - qx*rz + qy*rw + qz*rx
-        rz_new = qw*rz + qx*ry - qy*rx + qz*rw
-
-        return torch.stack([rx_new, ry_new, rz_new], dim=1)
-
-
-    def goal_reached(self, angle_threshold: float = 17, radius_threshold: float = 1.2, get_num_subs=False):
-        """
-        Проверяет достижение цели с учётом расстояния и направления взгляда робота.
-        distance_to_goal: [N] расстояния до цели
-        angle_threshold: максимально допустимый угол в радианах между направлением взгляда и вектором на цель
-        Возвращает: [N] булев тензор, True если цель достигнута
-        """
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # Проверка по расстоянию (например, радиус достижения stored в self.radius)
-        close_enough = distance_to_goal <= radius_threshold
-
-        # Получаем ориентацию робота в виде кватерниона (w, x, y, z)
-        root_quat_w = self._robot.data.root_quat_w  # shape [N, 4]
-
-        # Локальный вектор взгляда робота (вперёд по оси X)
-        local_forward = torch.tensor([1.0, 0.0, 0.0], device=root_quat_w.device, dtype=root_quat_w.dtype)
-        local_forward = local_forward.unsqueeze(0).repeat(root_quat_w.shape[0], 1)  # [N, 3]
-
-        # Вектор взгляда в мировых координатах
-        forward_w = self.quat_rotate(root_quat_w, local_forward)  # [N, 3]
-
-        # Вектор от робота к цели
-        root_pos_w = self._robot.data.root_pos_w  # [N, 3]
-        to_goal = self._desired_pos_w - root_pos_w  # [N, 3]
-
-        # Нормализуем векторы
-        forward_w_norm = torch.nn.functional.normalize(forward_w[:, :2] , dim=1)
-        to_goal_norm = torch.nn.functional.normalize(to_goal[:, :2] , dim=1)
-
-        # Косинус угла между векторами взгляда и направления на цель
-        cos_angle = torch.sum(forward_w_norm * to_goal_norm, dim=1)
-        cos_angle = torch.clamp(cos_angle, -1.0, 1.0)  # для безопасности
-        # direction_to_goal = to_goal
-        # yaw_g = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-
-        # Вычисляем угол между векторами
-        angle = torch.acos(cos_angle)
-        angle_degrees = torch.abs(angle) * 180.0 / 3.141592653589793
-        # Проверяем, что угол меньше порога
-        facing_goal = angle_degrees < angle_threshold
-
-        # Итоговое условие: близко к цели и смотрит в её сторону
-        # print(distance_to_goal, angle_degrees)
-        
-        conditions = torch.stack([close_enough, facing_goal], dim=1)  # shape [N, M]
-        num_conditions_met = conditions.sum(dim=1)  # shape [N], количество True в каждой строк
-
-        # self.step_counter += torch.ones_like(self.step_counter) #torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        # enouth_steps = self.step_counter > 4
-        # returns = torch.logical_and(torch.logical_and(close_enough, facing_goal), enouth_steps)
-        # self.step_counter = torch.where(returns, torch.zeros_like(self.step_counter), self.step_counter)
-        returns = torch.logical_and(close_enough, facing_goal)
-        # if torch.any(returns):
-        #     print(close_enough, facing_goal)
-        # print("returns", returns)
-        if get_num_subs == False:
-            return returns
-        return returns, num_conditions_met, distance_to_goal+0.1-radius_threshold, angle_degrees
-
-    def get_contact(self):
-        force_matrix = self.scene["contact_sensor"].data.net_forces_w
-        force_matrix[..., 2] = 0
-        forces_magnitude = torch.norm(torch.norm(force_matrix, dim=2), dim=1)  # shape: [batch_size, num_contacts]
-        # вычисляем модуль силы для каждого контакта
-        if force_matrix is not None and force_matrix.numel() > 0:
-            contact_forces = torch.norm(force_matrix, dim=-1)
-            if contact_forces.dim() >= 3:
-                has_contact = torch.any(contact_forces > 0.1, dim=(1, 2))
-            else:
-                has_contact = torch.any(contact_forces > 0.1, dim=1) if contact_forces.dim() == 2 else torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-            # print("c ", has_contact)
-            num_contacts_per_env = torch.sum(contact_forces > 0.1, dim=1)
-            high_contact_envs = num_contacts_per_env >= 1
-        else:
-            print("force_matrix_w is None or empty")
-            high_contact_envs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-        # if torch.any(high_contact_envs):
-        #     print("high_contact_envs ", high_contact_envs)
-        return high_contact_envs
-
-    def update_SR_history(self):
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-
-    def update_success_rate(self, goal_reached):
-        if self.turn_on_controller:
-            return torch.tensor(self.success_rate, device=self.device)
-        
-        # Получаем завершенные эпизоды
-        died, time_out = self._get_dones(self.my_episode_lenght - 1, inner=True)
-        completed = died | time_out
-        
-        if torch.any(completed):
-            # Получаем релевантные среды среди завершенных
-            relevant_env_ids = self.scene_manager.get_relevant_env()
-            # Фильтруем завершенные среды, оставляя только релевантные
-            relevant_completed = self._robot._ALL_INDICES[completed] #relevant_env_ids[(relevant_env_ids.view(1, -1) == self._robot._ALL_INDICES[completed].view(-1, 1)).any(dim=0)]
-            success = goal_reached.clone()
-            # Обновляем стеки для релевантных завершенных сред
-            for env_id in self._robot._ALL_INDICES.clone()[completed]:
-                env_id = env_id.item()
-                if not success[env_id]:#here idia is colulate all fault and sucess only on relative envs
-                    self.success_stacks[env_id].append(0)
-                elif env_id in relevant_completed:
-                    self.success_stacks[env_id].append(1)
-                
-                if len(self.success_stacks[env_id]) > self.max_stack_size:
-                    self.success_stacks[env_id].pop(0)
-            # print("self.success_stacks ", self.success_stacks)
-        # Вычисляем процент успеха для всех сред с непустыми стеками
-        # Подсчитываем общий процент успеха по всем релевантным средам
-        total_successes = 0
-        total_elements = 0
-        # print(self.success_stacks)
-        for env_id in range(self.num_envs):
-            stack = self.success_stacks[env_id]
-            if len(stack) == 0:
-                continue
-            total_successes += sum(stack)
-            total_elements += len(stack)
-        # print("update ", self.success_stacks)
-        # Вычисляем процент успеха
-        # print("total_successes ", total_successes, total_elements)
-        self.sr_stack_capacity = total_elements
-        if total_elements > 0:
-            self.success_rate = (total_successes / total_elements) * 100.0
-        else:
-            self.success_rate = 0.0
-        # print(total_elements)
-        if total_elements >= self.num_envs * self.max_stack_size * 0.9:
-            self.sr_stack_full = True
-        # print(success_rates, self.success_rate)
-        return self.success_rate
-    
-    def update_sr_stack(self):
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.sr_stack_full = False
-
-    def _get_dones(self, my_episode_lenght = 256, inner=False) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.is_time_out(my_episode_lenght)
-        
-        has_contact = self.get_contact()
-        self.has_contact = has_contact
-        died = torch.logical_or(
-            torch.logical_or(self.goal_reached(), has_contact),
-            time_out,
-        )
-        if torch.any(died):
-            goal_reached = self.goal_reached()
-            self.episode_counter += died.long()
-            self.success_counter += goal_reached.long()
-            self.event_update_counter += torch.sum(died).item()
-        
-        if not inner:
-            self.episode_length_buf[died] = 0
-        # print("died ", time_out, self.episode_length_buf)
-        return died, time_out
-    
-    def is_time_out(self, max_episode_length=256):
-        time_out = self.episode_length_buf >= max_episode_length
-        return time_out
-
-    def _reset_idx(self, env_ids: torch.Tensor | None):
-        super()._reset_idx(env_ids) #maybe this shuld be the first
-        if self.first_ep or env_ids is None or len(env_ids) == self.num_envs:
-            env_ids = self._robot._ALL_INDICES.clone()
-        self._update_chairs(env_ids=env_ids, all_defoult=True)
-        self._update_chairs(env_ids=env_ids, sp=False)
-        if self.turn_on_controller_step > self.my_episode_lenght and self.turn_on_controller:
-            self.turn_on_controller_step = 0
-            self.turn_on_controller = False
-
-        cond_imitation = (
-            not self.warm and
-            self.sr_stack_full and
-            self.mean_radius != 0 and
-            self.use_controller and
-            not self.turn_on_controller and
-            not self.first_ep and
-            self.turn_off_controller_step > self.my_episode_lenght
-        )
-        if cond_imitation: 
-            self.turn_on_controller_step = 0
-            self.turn_off_controller_step = 0
-            prob = lambda x: torch.rand(1).item() <= x
-            self.turn_on_controller = prob(0.01 * max(10, min(40, 100 - self.success_rate)))
-            print(f"turn controller: {self.turn_on_controller} with SR {self.success_rate}")
-        elif self.cur_step < self.warm_len:
-                if self.cur_step < self.without_imitation:
-                    self.turn_on_controller = False
-                else:
-                    self.turn_on_controller = True
-            
-        
-        if (self.mean_radius >= 2.3 and self.use_obstacles) or self.turn_on_obstacles_always or self.warm:
-            if self.turn_on_obstacles_always and self.cur_step % 300:
-                print("[ WARNING ] ostacles allways turn on")
-
-            self.turn_on_obstacles = True
-            if not self.turn_on_obstacles_always:
-                self.min_level_radius = max(2.3, self.mean_radius - 0.3)
-        else:
-            self.turn_on_obstacles = False
-        self.curriculum_learning_module(env_ids)
-        env_ids = env_ids.to(dtype=torch.long)
-
-        final_distance_to_goal = torch.linalg.norm(
-            self._desired_pos_w[env_ids, :2] - self._robot.data.root_pos_w[env_ids, :2], dim=1
-        ).mean()
-        self._robot.reset(env_ids)
-        if len(env_ids) == self.num_envs:
-            self.episode_length_buf = torch.zeros_like(self.episode_length_buf) #, high=int(self.max_episode_length))
-        self._actions[env_ids] = 0.0
-        self._desired_pos_w[env_ids, :2] = self._terrain.env_origins[env_ids, :2]
-        min_radius_x = torch.tensor([1.2])
-        min_radius = torch.sqrt(min_radius_x)[0]
-        robot_pos, quaternion, goal_pos = self.scene_manager.reset(env_ids, self._terrain.env_origins, self.mean_radius, min_radius, self.cur_angle_error)
-        # print("i'm in path_manager")
-        if self.turn_on_controller or self.imitation:
-            if self.imitation:
-                print("[ WARNING ] imitation mode on")
-            if self.turn_on_controller_step == 0:
-                env_ids_for_control = self._robot._ALL_INDICES.clone()
-                robot_pos_for_control = self._robot.data.default_root_state[env_ids_for_control, :2].clone()
-                robot_pos_for_control[env_ids, :2] = robot_pos
-                goal_pos_for_control = self._desired_pos_w[env_ids_for_control, :2].clone()
-                goal_pos_for_control[env_ids, :2] = goal_pos
-            else:
-                env_ids_for_control = env_ids
-                robot_pos_for_control = robot_pos
-                goal_pos_for_control = goal_pos
-            paths = None
-            possible_try_steps = 3
-            for i in range(possible_try_steps):
-                paths = self.path_manager.get_paths(
-                    env_ids=env_ids_for_control,
-                    start_positions=self.to_local(robot_pos_for_control,env_ids_for_control),
-                    target_positions=self.to_local(goal_pos_for_control,env_ids_for_control),
-                    device=self.device
-                )
-                if paths is None:
-                    print(f"[ ERROR ] GET NONE PATH {i + 1} times")
-                    self._update_chairs(env_ids=env_ids, sp=False)
-                else:
-                    break
-            # print("out path_manager, paths: ", paths, self.turn_on_controller_step)
-            self.control_module.update_paths(env_ids_for_control, paths, self.to_local(goal_pos_for_control,env_ids_for_control))
-            # self.control_module.update_paths(self.to_local(robot_pos_for_control, env_ids_for_control), self.to_local(goal_pos_for_control, env_ids_for_control), paths, env_ids_for_control)
-        if self.memory_on:
-            self.memory_manager.reset()
-        # print("in reset robot pose ", robot_pos, goal_pos)
-        self._desired_pos_w[env_ids, :2] = goal_pos
-        self._desired_pos_w[env_ids, 2] = 0.7
-
-        joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        default_root_state[:, :2] = robot_pos
-        default_root_state[:, 2] = 0.1
-        default_root_state[:, 3:7] = quaternion
-        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-        # Логируем длину эпизодов для сброшенных сред
-        self.total_episode_length += torch.sum(self.episode_lengths[env_ids]).item()
-        self.episode_count += len(env_ids)
-        mean_episode_length = self.total_episode_length / self.episode_count if self.episode_count > 0 else 0.0
-        # self.tensorboard_writer.add_scalar("Metrics/Mean_episode_length", mean_episode_length, self.tensorboard_step)
-        # Сбрасываем счетчик длины для сброшенных сред
-        self.episode_lengths[env_ids] = 0
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # print("distance_to_goal ", distance_to_goal)
-        _, _, r_error, a_error = self.goal_reached(get_num_subs=True)
-        self.previous_distance_error[env_ids] = r_error[env_ids]
-        self.previous_angle_error[env_ids] = a_error[env_ids]
-        self._update_chairs(env_ids=env_ids, up=False)
-        self.first_ep = False
-        self.tracker.reset(env_ids)
-        env_ids_for_scene_embeddings = self._robot._ALL_INDICES.clone()
-        scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        if self.LOG:
-            self.experiment.log_metric("success_rate", self.success_rate, step=self.tensorboard_step)
-            self.experiment.log_metric("mean_radius", self.mean_radius, step=self.tensorboard_step)
-            self.experiment.log_metric("max_angle", self.max_angle_error, step=self.tensorboard_step)
-            # self.experiment.log_metric("use obstacles", self.turn_on_obstacles.float(), step=self.tensorboard_step)
-
-    def to_local(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] - env_origins[env_ids, :2]
-
-    def curriculum_learning_module(self, env_ids: torch.Tensor):
-        # print("self.success_rate ", self.success_rate)
-        if self.warm and self.cur_step >= self.warm_len:
-            self.warm = False
-            self.mean_radius = self.start_mean_radius
-            self.cur_angle_error = 0
-            self._step_update_counter = 0
-            print(f"end worm stage r: {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-        elif not self.warm and not self.turn_on_controller and self.sr_stack_full:
-            if self.success_rate >= self.sr_treshhold:
-                self.success_ep_num += 1
-                self.foult_ep_num = 0
-                if self.success_ep_num > self.num_envs:
-                    self.second_try = max(self.mean_radius, self.second_try)
-                    self.success_ep_num = 0
-                    old_mr = self.mean_radius
-                    old_a = self.cur_angle_error
-                    self.cur_angle_error += self.max_angle_error / 2
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    if self.cur_angle_error > self.max_angle_error:
-                        self.cur_angle_error = 0
-                        if self.mean_radius == 0:
-                            self.mean_radius += 0.3
-                        else:
-                            self.mean_radius += 1
-                        print(f"udate [ UP ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}")
-                    else:
-                        print(f"udate [ UP ] r: {round(self.mean_radius, 2)} a: from {round(old_a, 2)} to {round(self.cur_angle_error, 2)}")
-                    self._step_update_counter = 0
-                    self.update_sr_stack()
-            elif self.success_rate <= 10 or (self._step_update_counter >= 4000 and self.success_rate <= self.sr_treshhold):
-                self.foult_ep_num += 1
-                if self.foult_ep_num > 2000:
-                    self.success_ep_num = 0
-                    self.foult_ep_num = 0
-                    old_mr = self.mean_radius
-                    if self.cur_angle_error == 0:
-                        self.mean_radius += -0.1
-                        self.mean_radius = max(self.min_level_radius, self.mean_radius)
-                    self.cur_angle_error = 0
-                   
-                    self._step_update_counter = 0
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    print(f"udate [ DOWN ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-                    self.update_sr_stack()
-
-        self._obstacle_update_counter += 1
-        return None
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        pass
-
-    def _debug_vis_callback(self, event):
-        pass
-
-    def close(self):
-        # self.tensorboard_writer.close()
-        super().close()
-
-    def _update_goals(self, env_ids: torch.Tensor = None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        min_num_active=1
-        max_num_active=3
-
-        for i in range(self.scene_manager.num_obstacles):  # Для каждого стула
-            root_poses = torch.zeros((len(env_ids), 7), device=self.device)  # [num_envs, 7] (x, y, z, qw, qx, qy, qz)
-            for j, env_id in enumerate(env_ids):
-                # Получаем информацию о препятствии из obstacle_manager
-                node = self.scene_manager.graphs[env_id.item()].graph.nodes[i]
-                pos = node['position']
-                # Заполняем позицию и ориентацию
-                root_poses[j, 0] = pos[0]  # x
-                root_poses[j, 1] = pos[1]  # y
-                root_poses[j, 2] = pos[2]  # z
-                root_poses[j, 3] = 1.0  # qw
-                root_poses[j, 4:7] = 0.0  # qx, qy, qz
-
-            # Добавляем смещение среды
-            root_poses[:, :2] += self._terrain.env_origins[env_ids, :2]
-            self.goal_objects[i].write_root_pose_to_sim(root_poses, env_ids=env_ids)
-
-
-
-    def _update_chairs(self, env_ids: torch.Tensor = None, mess=False, all_defoult = False, up=True, sp=True):
-        """
-        Args:
-            env_ids: torch.Tensor, индексы сред для обновления. Если None, обновляются все среды.
-        """
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if up:
-            if self.first_ep or not self.turn_on_obstacles or all_defoult:
-                min_num_active = 0
-                max_num_active = 0
-            else:
-                min_num_active=0
-                max_num_active=3
-
-            self.scene_manager.generate_obstacle_positions(mess=False, env_ids=env_ids,terrain_origins=self._terrain.env_origins,
-                                                            min_num_active=min_num_active,max_num_active=max_num_active)
-        if sp:
-            # Обновляем позиции стульев для всех сред
-            for i in range(self.scene_manager.num_obstacles):  # Для каждого стула
-                root_poses = torch.zeros((len(env_ids), 7), device=self.device)  # [num_envs, 7] (x, y, z, qw, qx, qy, qz)
-                for j, env_id in enumerate(env_ids):
-                    # Получаем информацию о препятствии из obstacle_manager
-                    node = self.scene_manager.graphs[env_id.item()].graph.nodes[i]
-                    pos = node['position']
-                    # Заполняем позицию и ориентацию
-                    root_poses[j, 0] = pos[0]  # x
-                    root_poses[j, 1] = pos[1]  # y
-                    root_poses[j, 2] = pos[2]  # z
-                    root_poses[j, 3] = 1.0  # qw
-                    root_poses[j, 4:7] = 0.0  # qx, qy, qz
-
-                # Добавляем смещение среды
-                root_poses[:, :2] += self._terrain.env_origins[env_ids, :2]
-                self.chair_objects[i].write_root_pose_to_sim(root_poses, env_ids=env_ids)
-
-    def recover_corrupted_envs(self, env_ids: torch.Tensor):
-        if env_ids.numel() == 0:
-            return
-        env_ids = env_ids.to(dtype=torch.long).unique()  # Уникальные IDs
-        
-        # Шаг 1: Cleanup prims для corrupted envs (удалить и пересоздать USD-объекты)
-        stage = get_context().get_stage()
-        for env_id in env_ids:
-            env_path = f"/World/envs/env_{env_id.item()}"
-            print(f"[RECOVERY] Deleting corrupted prims at {env_path}...")
-            
-            # Удалить все sub-prims (Robot, Kitchen, Table, Bowl, Chairs)
-            sub_paths = [
-                f"{env_path}/Robot",
-                f"{env_path}/Kitchen",
-                f"{env_path}/Table",
-                f"{env_path}/Bowl",
-            ]
-            for i in range(self.scene_manager.num_obstacles):  # Chairs
-                sub_paths.append(f"{env_path}/Chair_{i}")
-            
-            for path in sub_paths:
-                if stage.GetPrimAtPath(path).IsValid():
-                    omni.kit.commands.execute("DeletePrims", paths=[path])
-        
-        # Шаг 2: Переспавнить объекты (вызов set_env для этих envs)
-        self.set_env()  # Ваш метод спавнит для всех, но поскольку cloned, он переспавнит по prim_path="/World/envs/env_.*/..."
-        # Если set_env не векторизован, сделайте его: передайте env_ids и спавните только для них
-        # Пример модификации set_env:
-        # def set_env(self, env_ids=None):
-        #     if env_ids is None: env_ids = self._robot._ALL_INDICES.clone()
-        #     # Затем в spawn_from_usd: prim_path=f"/World/envs/env_{{env_id}}/*" с loop по env_ids
-        
-        # Шаг 3: Вызов _reset_idx (стандартный reset)
-        self._reset_idx(env_ids)
-        
-        # Шаг 4: Force update physics (шаг симуляции, чтобы применить изменения)
-        self.sim.step()  # Или self.sim.render() если нужно
-        
-        # Шаг 5: Проверить, помогло ли (e.g., root_pos_w не NaN)
-        root_pos_w = self._robot.data.root_pos_w[env_ids, :2]
-        still_bad = torch.isnan(root_pos_w).any(dim=1) | torch.isinf(root_pos_w).any(dim=1)
-        if still_bad.any():
-            print(f"[WARNING] Targeted recovery failed for envs {env_ids[still_bad].tolist()}. Falling back to full sim reset...")
-            self.full_sim_reset()  # Fallback (см. ниже)
-        
-        # Лог (в tensorboard/comet)
-        if self.LOG:
-            self.experiment.log_metric("recovery_events", len(env_ids), step=self.tensorboard_step)
-    
-    def full_sim_reset(self):
-        print("[RECOVERY] Performing full simulation reset...")
-        self.sim.stop()  # Остановить симуляцию
-        self.sim.clear()  # Очистить буферы (если нужно; проверьте доки SimulationContext)
-        self.sim.play()   # Перезапустить
-        
-        # Переинициализировать сцену полностью
-        self._setup_scene()  # Ваш метод для спавна всего
-        self._reset_idx(None)  # Reset всех envs
-        
-        # Лог
-        if self.LOG:
-            self.experiment.log_metric("full_reset_events", 1, step=self.tensorboard_step)
-
-def log_embedding_stats(embedding):
-    mean_val = embedding.mean().item()
-    std_val = embedding.std().item()
-    min_val = embedding.min().item()
-    max_val = embedding.max().item()
-    print(f"[ EM ] mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env.py
deleted file mode 100644
index 075589a372..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env.py
+++ /dev/null
@@ -1,1226 +0,0 @@
-# env.py
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
- 
-import gymnasium as gym
-import torch
-import math
-import numpy as np
-import os
-import torchvision.models as models
-import torchvision.transforms as transforms
-from torch import nn
-import random
-
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation, ArticulationCfg, RigidObject, RigidObjectCfg
-from isaaclab.envs import DirectRLEnv, DirectRLEnvCfg
-from isaaclab.envs.ui import BaseEnvWindow
-from isaaclab.markers import VisualizationMarkers
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg, SimulationContext
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import subtract_frame_transforms
-from isaaclab.sensors import TiledCamera, TiledCameraCfg, ContactSensor, ContactSensorCfg
-from .scene_manager import SceneManager
-from .evaluation_manager import EvaluationManager
-from .control_manager import VectorizedPurePursuit
-from .path_manager import Path_manager
-from .memory_manager import Memory_manager, PathTracker
-from .asset_manager import AssetManager
-import omni.kit.commands
-import omni.usd
-import datetime
-# from torch.utils.tensorboard import SummaryWriter
-##
-# Pre-defined configs
-##
-from isaaclab_assets.robots.aloha import ALOHA_CFG
-from isaaclab.markers import CUBOID_MARKER_CFG
-from transformers import CLIPProcessor, CLIPModel
-from PIL import Image
-import omni.kit.commands  # Уже импортировано в вашем коде
-from omni.usd import get_context  # Для доступа к stage
-from pxr import Gf
-
-class WheeledRobotEnvWindow(BaseEnvWindow):
-    def __init__(self, env: 'WheeledRobotEnv', window_name: str = "IsaacLab"):
-        super().__init__(env, window_name)
-        with self.ui_window_elements["main_vstack"]:
-            with self.ui_window_elements["debug_frame"]:
-                with self.ui_window_elements["debug_vstack"]:
-                    self._create_debug_vis_ui_element("targets", self.env)
-
-@configclass
-class WheeledRobotEnvCfg(DirectRLEnvCfg):
-    episode_length_s = 512.0
-    decimation = 8
-    action_space = gym.spaces.Box(
-        low=np.array([-1.0, -1.0], dtype=np.float32),
-        high=np.array([1.0, 1.0], dtype=np.float32),
-        shape=(2,)
-    )
-    # Observation space is now the ResNet18 embedding size (512)
-    m = 1  # Например, 3 эмбеддинга и действия
-    # observation_space = gym.spaces.Box(
-    #     low=-float("inf"),
-    #     high=float("inf"),
-    #     shape=(m * (512 + 3),),  # m * (embedding_size + action_size) + 2 (скорости)
-    #     dtype="float32"
-    # )
-    # TODO automat compute num_total_objects
-    num_total_objects = 10
-
-    observation_space = gym.spaces.Dict({
-        "img": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(512 + 3,), dtype=np.float32),
-        "graph": gym.spaces.Dict({
-            "node_clip": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 1), dtype=np.float32),
-            "node_center": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 3), dtype=np.float32),
-            "node_extent": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 3), dtype=np.float32),
-            "rel_ids": gym.spaces.Box(low=0, high=25, shape=(num_total_objects, num_total_objects), dtype=np.int64),
-        })
-    })
-    state_space = 0
-    debug_vis = False
-
-    ui_window_class_type = WheeledRobotEnvWindow
-
-    sim: SimulationCfg = SimulationCfg(
-        dt=1/60,
-        render_interval=decimation,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="min",
-            restitution_combine_mode="min",
-            static_friction=0.2,
-            dynamic_friction=0.15,
-            restitution=0.0,
-        ),
-    )
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="plane",
-        collision_group=-1,
-        # physics_material=sim_utils.RigidBodyMaterialCfg(
-        #     friction_combine_mode="min",
-        #     restitution_combine_mode="min",
-        #     static_friction=0.8,
-        #     dynamic_friction=0.6,
-        #     restitution=0.0,
-        # ),
-        debug_vis=False,
-    )
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=64, env_spacing=18, replicate_physics=True)
-    robot: ArticulationCfg = ALOHA_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    wheel_radius = 0.068
-    wheel_distance = 0.34
-    tiled_camera: TiledCameraCfg = TiledCameraCfg(
-        prim_path="/World/envs/env_.*/Robot/box2_Link/Camera",
-        offset=TiledCameraCfg.OffsetCfg(pos=(-0.35, 0, 1.1), rot=(0.99619469809,0,0.08715574274,0), convention="world"),
-        # offset=TiledCameraCfg.OffsetCfg(pos=(0.0, 0, 0.9), rot=(1,0,0,0), convention="world"),
-        data_types=["rgb"],
-        spawn=sim_utils.PinholeCameraCfg(
-            focal_length=35.0, focus_distance=2.0, horizontal_aperture=36, clipping_range=(0.2, 10.0)
-        ),
-        width=224,
-        height=224,
-    )
-    current_dir = os.getcwd()
-    kitchen = sim_utils.UsdFileCfg(
-        usd_path=os.path.join(current_dir, "source/isaaclab_assets/data/aloha_assets", "scenes/scenes_sber_kitchen_for_BBQ/kitchen_new_simple.usd"),
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-    contact_sensor = ContactSensorCfg(
-        prim_path="/World/envs/env_.*/Robot/.*",
-        update_period=0.1,
-        history_length=1,
-        debug_vis=True,
-        filter_prim_paths_expr=["/World/envs/env_.*"],
-    )
-
-class WheeledRobotEnv(DirectRLEnv):
-    cfg: WheeledRobotEnvCfg
-
-    def __init__(self, cfg: WheeledRobotEnvCfg, render_mode: str | None = None, **kwargs):
-        self._super_init = True
-        self.current_dir = os.getcwd()
-        self.config_path=os.path.join(self.current_dir, "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json")
-        super().__init__(cfg, render_mode, **kwargs)
-        self._super_init = False
-        self.eval = False
-        self.eval_name = "baka"
-
-        self.eval_printed = False
-        self.scene_manager = SceneManager(self.num_envs, self.config_path, self.device)
-        self.eval_manager = EvaluationManager(self.num_envs)
-        self.eval_manager.set_task_lists(
-            robot_positions=[[0.0, -1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0],
-                             [1.0, -1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0],
-                             [2.0, -1.0, 0.0], [2.0, 0.0, 0.0], [2.0, 1.0, 0.0]],  # список стартовых позиций
-            angle_errors=[torch.pi, torch.pi*0.9, torch.pi*0.8]                         # список ошибок угла
-        )
-        self.use_controller = True
-        self.imitation = False
-        if self.imitation:
-            self.use_controller = True
-        if self.use_controller:
-            self.path_manager = Path_manager(scene_manager=self.scene_manager, ratio=8.0, shift=[5, 4], device=self.device)
-            self.control_module = VectorizedPurePursuit(num_envs=self.num_envs, device=self.device)
-        self.memory_on = False
-        self.tracker = PathTracker(num_envs=self.num_envs, device=self.device)
-        if self.memory_on:
-            self.memory_manager = Memory_manager(
-                num_envs=self.num_envs,
-                embedding_size=512,  # Размер эмбеддинга ResNet18
-                action_size=2,      # Размер действия (линейная и угловая скорость)
-                history_length=25,  # n = 10, можно настроить
-                device=self.device
-            )
-
-        self._actions = torch.zeros((self.num_envs, 2), device=self.device)
-        self._actions[:, 1] = 0.0
-        self._left_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._right_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._desired_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self._episode_sums = {
-            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-            for key in ["moves"]
-        }
-        self._left_wheel_id = self._robot.find_joints("left_wheel")[0]
-        self._right_wheel_id = self._robot.find_joints("right_wheel")[0]
-
-        self.set_debug_vis(self.cfg.debug_vis)
-        self.Debug = True
-        self.event_update_counter = 0
-        self.episode_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.success_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.step_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.possible_goal_position = []
-        
-        self.delete = 1
-        self.count = 0
-        self._debug_log_enabled = True
-        self._debug_envs_to_log = list(range(min(5, self.num_envs)))
-        self._inconsistencies = []
-        self._debug_step_counter = 0
-        self._debug_log_frequency = 10
-        self.turn_on_controller = False #it is not use or not use controller, it is flag for the first step
-        self.turn_on_controller_step = 0
-        self.my_episode_lenght = 256
-        self.turn_off_controller_step = 0
-        self.use_obstacles = True
-        self.turn_on_obstacles = False
-        self.turn_on_obstacles_always = False
-        if self.use_obstacles:
-            self.use_obstacles = True
-        self.previous_distance_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_angle_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_lin_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_ang_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.angular_speed = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        # Initialize ResNet18 for image embeddings
-        # self.resnet18 = models.resnet18(pretrained=True).to(self.device)
-        # self.resnet18.eval()  # Set to evaluation mode
-        # # Remove the final fully connected layer to get embeddings
-        # self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])
-        # # Image preprocessing for ResNet18
-        # transforms.ToTensor()
-        # self.transform = transforms.Compose([
-        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
-        # ])
-        self.success_rate = 0
-        self.sr_stack_capacity = 0
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-        self._step_update_counter = 0
-        self.mean_radius = 3.3
-        self.max_angle_error = torch.pi / 6
-        self.cur_angle_error = torch.pi / 12
-        self.warm = True
-        self.warm_len = 2500
-        self.without_imitation = self.warm_len / 2
-        self._obstacle_update_counter = 0
-        self.has_contact = torch.full((self.num_envs,), True, dtype=torch.bool, device=self.device)
-        self.sim = SimulationContext.instance()
-        self.obstacle_positions = None
-        self.key = None
-        self.success_ep_num = 0
-        # self.run = wandb.init(project="aloha_direct")
-        self.first_ep = [True, True]
-        self.first_ep_step = 0
-        self.second_ep = True
-        timestamp = datetime.datetime.now().strftime("%m_%d_%H_%M")
-        name = "dev"
-        self.episode_lengths = torch.zeros(self.num_envs, device=self.device)
-        self.episode_count = 0
-        self.total_episode_length = 0.0
-        # self.tensorboard_writer = SummaryWriter(log_dir=f"/home/xiso/IsaacLab/logs/tensorboard/navigation_rl_{name}_{timestamp}")
-        self.tensorboard_step = 0
-        self.cur_step = 0
-        self.velocities = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-        
-        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
-        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
-        self.clip_model.eval()  # Установить в режим оценки
-        self.second_try = 0
-        self.foult_ep_num = 0
-        # Инициализация стеков для хранения успехов (1 - успех, 0 - неуспех)
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.max_stack_size = 20  # Максимальный размер стека
-        self.sr_stack_full = False
-        self.start_mean_radius = 0
-        self.min_level_radius = 0
-        self.sr_treshhold = 85
-        self.LOG = False
-        self.text_embeddings = torch.zeros((self.num_envs, 512), device=self.device)
-        if self.LOG:
-            from comet_ml import start
-            from comet_ml.integration.pytorch import log_model
-            self.experiment = start(
-                api_key="DRYfW6B6VtUQr9llvf3jup57R",
-                project_name="general",
-                workspace="xisonik"
-            )
-        self.print_config_info()
-        self._setup_scene()
-        self.prim_paths = self.asset_manager.all_prim_paths
-        # сразу после создания scene_manager
-        self._material_cache = {}        # key -> material prim path (строка), key = "r_g_b"
-        self._applied_color_map = {}     # obj_index (int) -> color_key (str), чтобы не биндим повторно
-        self.scene_manager.init_graph_descriptor(self.clip_processor, self.clip_model)
-
-    def print_config_info(self):
-        print("__________[ CONGIFG INFO ]__________")
-        print(f"|")
-        print(f"| EVAL : {self.eval}")
-        print(f"|")
-        print(f"| Start mean radius is: {self.mean_radius}")
-        print(f"|")
-        print(f"| Start amx angle is: {self.max_angle_error}")
-        print(f"|")
-        print(f"| Use controller: {self.use_controller}")
-        print(f"|")
-        print(f"| Full imitation: {self.imitation}")
-        print(f"|")
-        print(f"| Use memory: {self.memory_on}")
-        print(f"|")
-        print(f"| Use obstacles: {self.use_obstacles}")
-        print(f"|")
-        print(f"| Start radius: {self.start_mean_radius}, min: {self.min_level_radius}")
-        print(f"|")
-        print(f"| Warm len: {self.warm_len}")
-        print(f"|")
-        print(f"| Turn on obstacles always: {self.turn_on_obstacles_always}")
-        print(f"|")
-        print(f"_______[ CONGIFG INFO CLOSE ]_______")
-
-    def _setup_scene(self):
-        from isaaclab.sensors import ContactSensor
-        import time
-        from pxr import Usd
-        from isaaclab.sim.spawners.from_files import spawn_from_usd
-        import random
-        if self._super_init:
-            self._robot = Articulation(self.cfg.robot)
-            self.scene.articulations["robot"] = self._robot
-            self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-            self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
-            self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
-            self.scene.clone_environments(copy_from_source=True)
-            self._tiled_camera = TiledCamera(self.cfg.tiled_camera)
-            self.scene.sensors["tiled_camera"] = self._tiled_camera
-            # Спавн кухни (статический элемент)
-            spawn_from_usd(
-                prim_path="/World/envs/env_.*/Kitchen",
-                cfg=self.cfg.kitchen,
-                translation=(5.0, 4.0, 0.0),
-                orientation=(0.0, 0.0, 0.0, 1.0),
-            )
-            self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-            self.scene.sensors["contact_sensor"] = self._contact_sensor
-            self.asset_manager = AssetManager(config_path=self.config_path)
-            self.scene_objects = self.asset_manager.spawn_assets_in_scene()
-            
-            # self.scene_manager.update_prims(prim_path)
-            
-
-        # light_cfg = sim_utils.DomeLightCfg(intensity=300.0, color=(0.75, 0.75, 0.75))
-        # light_cfg.func("/World/Light", light_cfg)
-
-    def _get_observations(self) -> dict:
-        self.tensorboard_step += 1
-        self.cur_step += 1
-        self.episode_lengths += 1
-        import os
-        from PIL import Image, ImageDraw, ImageFont
-        # Получение RGB изображений с камеры
-        camera_data = self._tiled_camera.data.output["rgb"].clone()  # Shape: (num_envs, 224, 224, 3)
-        
-        # Преобразование изображений для CLIP
-        # CLIP ожидает изображения в формате PIL или тензоры с правильной нормализацией
-        images = camera_data.cpu().numpy().astype(np.uint8)  # Конвертация в numpy uint8
-        # inputs = self.clip_processor(images=images, return_tensors="pt", padding=True).to(self.device)
-        images_list = [Image.fromarray(im) for im in images]  # если images shape (N,H,W,3) numpy, это даёт список 2D-arrays
-        inputs = self.clip_processor(images=images_list, return_tensors="pt", padding=True)
-        for k, v in inputs.items():
-            inputs[k] = v.to(self.device)
-        # Получение эмбеддингов изображений
-        with torch.no_grad():
-            image_embeddings = self.clip_model.get_image_features(**inputs)  # Shape: (num_envs, 512)
-            image_embeddings = image_embeddings / (image_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        
-        # Получение скоростей робота
-        root_lin_vel_w = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1).unsqueeze(-1)
-        root_ang_vel_w = self._robot.data.root_ang_vel_w[:, 2].unsqueeze(-1)
-        
-        scene_embeddings = self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        scene_embeddings_list = self.scene_manager.get_graph_obs(self._robot._ALL_INDICES.clone())
-        # print(scene_embeddings_list[0])
-        scene_embeddings_noize = self.scene_manager.add_noise_to_graph_obs(scene_embeddings_list)
-        # print(scene_embeddings_noize[0])
-        scene_embeddings_dict = self.scene_manager.tensorize_graph_obs(scene_embeddings_noize)
-        # obs = torch.cat([image_embeddings, scene_embeddings, text_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        # print(scene_embeddings_dict)
-        obs_img = torch.cat([image_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs = {
-            "img": obs_img,
-            "graph": scene_embeddings_dict
-        }
-        self.previous_ang_vel = self.angular_speed
-        # log_embedding_stats(image_embeddings)
-        
-        observations = {"policy": obs}       
-        return observations
-
-    # as they are not affected by the observation space change.
-
-    def _pre_physics_step(self, actions: torch.Tensor):
-        if self.cur_step % 256 == 0:
-            print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-        env_ids = self._robot._ALL_INDICES.clone()
-        self._actions = actions.clone().clamp(-1.0, 1.0)
-
-        nan_mask = torch.isnan(self._actions) | torch.isinf(self._actions)
-        nan_indices = torch.nonzero(nan_mask.any(dim=1), as_tuple=False).squeeze()  # env_ids где любой action NaN/inf
-        if nan_indices.numel() > 0:
-            print(f"[WARNING] NaN/Inf in actions for envs: {nan_indices.tolist()}. Attempting recovery...")
-        r = self.cfg.wheel_radius
-        L = self.cfg.wheel_distance
-        self._step_update_counter += 1
-        if self.turn_on_controller or self.imitation:
-            self.turn_on_controller_step += 1
-            # Получаем текущую ориентацию (yaw) из кватерниона
-            quat = self._robot.data.root_quat_w
-            siny_cosp = 2 * (quat[:, 0] * quat[:, 3] + quat[:, 1] * quat[:, 2])
-            cosy_cosp = 1 - 2 * (quat[:, 2] * quat[:, 2] + quat[:, 3] * quat[:, 3])
-            yaw = torch.atan2(siny_cosp, cosy_cosp)
-            linear_speed, angular_speed = self.control_module.compute_controls(
-                self.to_local(self._robot.data.root_pos_w[:, :2],env_ids),
-                yaw
-            )
-            # angular_speed = -angular_speed 
-            self._actions[:, 0] = (linear_speed / 0.6) - 1
-            self._actions[:, 1] = angular_speed / 2
-            actions.copy_(self._actions.clamp(-1.0, 1.0))
-        else:
-            self.turn_off_controller_step += 1
-            linear_speed = 0.6*(self._actions[:, 0] + 1.0) # [num_envs], всегда > 0
-            angular_speed = 2*self._actions[:, 1]  # [num_envs], оставляем как есть от RL
-        # linear_speed = torch.tensor([0], device=self.device)
-        self.angular_speed = angular_speed
-        self.velocities = torch.stack([linear_speed, angular_speed], dim=1)
-        # if self.tensorboard_step % 4 ==0:
-        # self.delete = -1 * self.delete 
-        # angular_speed = torch.tensor([0], device=self.device)
-        # print("vel is: ", linear_speed, angular_speed)
-        self._left_wheel_vel = (linear_speed - (angular_speed * L / 2)) / r
-        self._right_wheel_vel = (linear_speed + (angular_speed * L / 2)) / r
-        # self._left_wheel_vel = torch.clamp(self._left_wheel_vel, -10, 10)
-        # self._right_wheel_vel = torch.clamp(self._right_wheel_vel, -10, 10)
-
-    def _apply_action(self):
-        wheel_velocities = torch.stack([self._left_wheel_vel, self._right_wheel_vel], dim=1).unsqueeze(-1).to(dtype=torch.float32)
-        self._robot.set_joint_velocity_target(wheel_velocities, joint_ids=[self._left_wheel_id, self._right_wheel_id])
-
-    def _get_rewards(self) -> torch.Tensor:
-        # env_ids = self._robot._ALL_INDICES.clone()
-        # num_envs = len(env_ids)
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        # joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        # joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        # default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        # default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        # default_root_state[:, 2] = 0.1
-        # self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        # self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        # self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-        lin_vel = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1)
-        
-        lin_vel_reward = torch.clamp(lin_vel*0.02, min=0, max=0.15)
-        ang_vel = self._robot.data.root_ang_vel_w[:, 2]
-        ang_vel_reward = torch.abs(self.angular_speed) * 0.1
-        a_penalty = 0.1 * torch.abs(self.angular_speed - self.previous_ang_vel) #+ torch.abs(lin_vel - self.previous_lin_vel))
-        # print("a_penalty ", -a_penalty, self.angular_speed, self.previous_ang_vel )
-        # self.previous_lin_vel = lin_vel
-
-        goal_reached, num_subs, r_error, a_error = self.goal_reached(get_num_subs=True)
-
-        moves = torch.clamp(5 * (self.previous_distance_error - r_error), min=0, max=1) + \
-                    torch.clamp(5 * (self.previous_angle_error - a_error), min=0, max=1)
-        env_ids = self._robot._ALL_INDICES.clone()
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        self.tracker.add_step(env_ids, self.to_local(root_pos_w, env_ids), self.velocities)
-        path_lengths = self.tracker.compute_path_lengths(env_ids)
-
-        moves_reward = moves * 0.1
-        
-        self.previous_distance_error = r_error
-        self.previous_angle_error = a_error
-
-        has_contact = self.get_contact()
-
-        time_out = self.is_time_out(self.my_episode_lenght-1)
-        time_out_penalty = -5 * time_out.float()
-
-        vel_penalty = -1 * (ang_vel_reward + lin_vel_reward)
-        mask = ~goal_reached
-        vel_penalty[mask] = 0
-        lin_vel_reward[goal_reached] = 0
-
-        paths = self.tracker.get_paths(env_ids)
-        # jerk_counts = self.tracker.compute_jerk(env_ids, threshold=0.2)
-        # print(jerk_counts)
-        start_dists = self.eval_manager.get_start_dists(env_ids)
-        if self.turn_on_controller:
-            IL_reward = 0.5
-            punish = - 0.05
-        else:
-            IL_reward = 0
-            punish = (
-                - 0.1
-                - ang_vel_reward / (1 + 2 * self.mean_radius)
-                + lin_vel_reward / (1 + 2 * self.mean_radius)
-            )
-        reward = (
-            IL_reward + punish #* r_error
-            + torch.clamp(goal_reached.float() * 7, min=0, max=15) #* (1 + start_dists) / (1 + path_lengths)
-            - torch.clamp(has_contact.float() * (5 + lin_vel_reward), min=0, max=10)
-        )
-
-        if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-            sr = self.update_success_rate(goal_reached)
-
-        # if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-        # print("path info")
-        # print(path_lengths)
-        # print(r_error)
-        # print("reward ", reward)
-        # print("- 0.1 - 0.05 * r_error ", - 0.1 - 0.05 * r_error)
-        # print("IL_reward * r_error ", IL_reward * r_error)
-        # print("goal_reached ", goal_reached)
-        # print("lin_vel_reward ", lin_vel_reward)
-        # print("torch.clamp(goal_reached ", torch.clamp(goal_reached.float() * 7 * (1 + self.mean_radius) / (1 + path_lengths), min=0, max=10))
-        # print("torch.clamp(has_contact ", torch.clamp(has_contact.float() * (3 + lin_vel_reward), min=0, max=6))
-        # print("ang_vel_reward ", -ang_vel_reward)
-        # print("___________")
-        check = {
-            "moves":moves,
-        }
-        for key, value in check.items():
-            self._episode_sums[key] += value
-
-        # if self.tensorboard_step % 100 == 0:
-        #     self.tensorboard_writer.add_scalar("Metrics/reward", torch.sum(reward), self.tensorboard_step)
-        return reward
-    
-    def quat_rotate(self, quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
-        """
-        Вращение вектора vec кватернионом quat.
-        quat: [N, 4] (w, x, y, z)
-        vec: [N, 3]
-        Возвращает: [N, 3] - вектор vec, повернутый кватернионом quat
-        """
-        w, x, y, z = quat.unbind(dim=1)
-        vx, vy, vz = vec.unbind(dim=1)
-
-        # Кватернионное умножение q * v
-        qw = -x*vx - y*vy - z*vz
-        qx = w*vx + y*vz - z*vy
-        qy = w*vy + z*vx - x*vz
-        qz = w*vz + x*vy - y*vx
-
-        # Обратный кватернион q*
-        rw = w
-        rx = -x
-        ry = -y
-        rz = -z
-
-        # Результат (q * v) * q*
-        rx_new = qw*rx + qx*rw + qy*rz - qz*ry
-        ry_new = qw*ry - qx*rz + qy*rw + qz*rx
-        rz_new = qw*rz + qx*ry - qy*rx + qz*rw
-
-        return torch.stack([rx_new, ry_new, rz_new], dim=1)
-
-
-    def goal_reached(self, angle_threshold: float = 17, radius_threshold: float = 1.3, get_num_subs=False):
-        """
-        Проверяет достижение цели с учётом расстояния и направления взгляда робота.
-        distance_to_goal: [N] расстояния до цели
-        angle_threshold: максимально допустимый угол в радианах между направлением взгляда и вектором на цель
-        Возвращает: [N] булев тензор, True если цель достигнута
-        """
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root: ", self.to_local(root_pos_w))
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # Проверка по расстоянию (например, радиус достижения stored в self.radius)
-        close_enough = distance_to_goal <= radius_threshold
-
-        # Получаем ориентацию робота в виде кватерниона (w, x, y, z)
-        root_quat_w = self._robot.data.root_quat_w  # shape [N, 4]
-
-        # Локальный вектор взгляда робота (вперёд по оси X)
-        local_forward = torch.tensor([1.0, 0.0, 0.0], device=root_quat_w.device, dtype=root_quat_w.dtype)
-        local_forward = local_forward.unsqueeze(0).repeat(root_quat_w.shape[0], 1)  # [N, 3]
-
-        # Вектор взгляда в мировых координатах
-        forward_w = self.quat_rotate(root_quat_w, local_forward)  # [N, 3]
-
-        # Вектор от робота к цели
-        root_pos_w = self._robot.data.root_pos_w  # [N, 3]
-        to_goal = self._desired_pos_w - root_pos_w  # [N, 3]
-
-        # Нормализуем векторы
-        forward_w_norm = torch.nn.functional.normalize(forward_w[:, :2] , dim=1)
-        to_goal_norm = torch.nn.functional.normalize(to_goal[:, :2] , dim=1)
-
-        # Косинус угла между векторами взгляда и направления на цель
-        cos_angle = torch.sum(forward_w_norm * to_goal_norm, dim=1)
-        cos_angle = torch.clamp(cos_angle, -1.0, 1.0)  # для безопасности
-        # direction_to_goal = to_goal
-        # yaw_g = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-
-        # Вычисляем угол между векторами
-        angle = torch.acos(cos_angle)
-        angle_degrees = torch.abs(angle) * 180.0 / 3.141592653589793
-        # Проверяем, что угол меньше порога
-        facing_goal = angle_degrees < angle_threshold
-
-        # Итоговое условие: близко к цели и смотрит в её сторону
-        # print(distance_to_goal, angle_degrees)
-        
-        conditions = torch.stack([close_enough, facing_goal], dim=1)  # shape [N, M]
-        num_conditions_met = conditions.sum(dim=1)  # shape [N], количество True в каждой строк
-
-        # self.step_counter += torch.ones_like(self.step_counter) #torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        # enouth_steps = self.step_counter > 4
-        # returns = torch.logical_and(torch.logical_and(close_enough, facing_goal), enouth_steps)
-        # self.step_counter = torch.where(returns, torch.zeros_like(self.step_counter), self.step_counter)
-        returns = torch.logical_and(close_enough, facing_goal)
-        # if torch.any(returns):
-        #     print(close_enough, facing_goal)
-        # print("returns", returns)
-        if get_num_subs == False:
-            return returns
-        return returns, num_conditions_met, distance_to_goal+0.1-radius_threshold, angle_degrees
-
-    def get_contact(self):
-        force_matrix = self.scene["contact_sensor"].data.net_forces_w
-        force_matrix[..., 2] = 0
-        forces_magnitude = torch.norm(torch.norm(force_matrix, dim=2), dim=1)  # shape: [batch_size, num_contacts]
-        # вычисляем модуль силы для каждого контакта
-        if force_matrix is not None and force_matrix.numel() > 0:
-            contact_forces = torch.norm(force_matrix, dim=-1)
-            if contact_forces.dim() >= 3:
-                has_contact = torch.any(contact_forces > 0.1, dim=(1, 2))
-            else:
-                has_contact = torch.any(contact_forces > 0.1, dim=1) if contact_forces.dim() == 2 else torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-            # print("c ", has_contact)
-            num_contacts_per_env = torch.sum(contact_forces > 0.1, dim=1)
-            high_contact_envs = num_contacts_per_env >= 1
-        else:
-            print("force_matrix_w is None or empty")
-            high_contact_envs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-        # if torch.any(high_contact_envs):
-        #     print("high_contact_envs ", high_contact_envs)
-        return high_contact_envs
-
-    def update_SR_history(self):
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-
-    def update_success_rate(self, goal_reached):
-        if self.turn_on_controller:
-            return torch.tensor(self.success_rate, device=self.device)
-        
-        # Получаем завершенные эпизоды
-        died, time_out = self._get_dones(self.my_episode_lenght - 1, inner=True)
-        completed = died | time_out
-        
-        if torch.any(completed):
-            # Получаем релевантные среды среди завершенных
-            # Фильтруем завершенные среды, оставляя только релевантные
-            relevant_completed = self._robot._ALL_INDICES[completed] #relevant_env_ids[(relevant_env_ids.view(1, -1) == self._robot._ALL_INDICES[completed].view(-1, 1)).any(dim=0)]
-            success = goal_reached.clone()
-            # Обновляем стеки для релевантных завершенных сред
-            for env_id in self._robot._ALL_INDICES.clone()[completed]:
-                env_id = env_id.item()
-                if not success[env_id]:#here idia is colulate all fault and sucess only on relative envs
-                    self.success_stacks[env_id].append(0)
-                elif env_id in relevant_completed:
-                    self.success_stacks[env_id].append(1)
-                
-                if len(self.success_stacks[env_id]) > self.max_stack_size:
-                    self.success_stacks[env_id].pop(0)
-            # print("self.success_stacks ", self.success_stacks)
-        # Вычисляем процент успеха для всех сред с непустыми стеками
-        # Подсчитываем общий процент успеха по всем релевантным средам
-        total_successes = 0
-        total_elements = 0
-        # print(self.success_stacks)
-        for env_id in range(self.num_envs):
-            stack = self.success_stacks[env_id]
-            if len(stack) == 0:
-                continue
-            total_successes += sum(stack)
-            total_elements += len(stack)
-        # print("update ", self.success_stacks)
-        # Вычисляем процент успеха
-        # print("total_successes ", total_successes, total_elements)
-        self.sr_stack_capacity = total_elements
-        if total_elements > 0:
-            self.success_rate = (total_successes / total_elements) * 100.0
-        else:
-            self.success_rate = 0.0
-        # print(total_elements)
-        if total_elements >= self.num_envs * self.max_stack_size * 0.9:
-            self.sr_stack_full = True
-        # print(success_rates, self.success_rate)
-        return self.success_rate
-    
-    def update_sr_stack(self):
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.sr_stack_full = False
-
-    def _get_dones(self, my_episode_lenght = 256, inner=False) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.is_time_out(my_episode_lenght)
-        
-        has_contact = self.get_contact()
-        self.has_contact = has_contact
-        died = torch.logical_or(
-            torch.logical_or(self.goal_reached(), has_contact),
-            time_out,
-        )
-        env_ids = self._robot._ALL_INDICES[died]
-        if torch.any(died):
-            goal_reached = self.goal_reached()
-            if self.eval:
-                # === EVAL LOGGING ===
-                envs_finished = torch.where(died)[0]  # индексы завершившихся эпизодов
-                # успехи конкретно для них
-                successes = goal_reached[envs_finished].float()
-                traj_lens = self.tracker.compute_path_lengths(envs_finished)  # [K]
-                durations = self.tracker.lengths[envs_finished].float()       # [K]
-                start_dists = self.eval_manager.get_start_dists(envs_finished) # [K]
-                # логируем (ВАЖНО: log_results -> ДО next_episode)
-                self.eval_manager.log_results(envs_finished, successes, traj_lens, start_dists, durations)
-                self.eval_manager.next_episode(envs_finished)
-
-                if self.eval_manager.is_all_done() and not self.eval_printed:
-                    import pandas as pd
-                    df, global_stats, pos_stats = self.eval_manager.summarize()
-                    print("=== FINAL EVAL SUMMARY ===")
-                    print(global_stats)
-                    self.eval_printed = True
-
-                    # создаём директорию logs если её нет
-                    log_dir = os.path.join(self.current_dir, "logs/skrl/results")
-                    os.makedirs(log_dir, exist_ok=True)
-
-                    # 1. Сохраняем сырые результаты
-                    save_path = os.path.join(log_dir, f"eval_results_{self.eval_name}.csv")
-                    df.to_csv(save_path, index=False)
-
-                    # 2. Сохраняем агрегированную информацию
-                    summary_path = os.path.join(log_dir, f"eval_summary_{self.eval_name}.csv")
-
-                    # превращаем global_stats и pos_stats в один DataFrame
-                    summary_df = pos_stats.copy()
-                    summary_df["position_idx"] = summary_df.index
-                    summary_df.reset_index(drop=True, inplace=True)
-
-                    # добавляем глобальные метрики как отдельную строку
-                    global_row = global_stats.to_dict()
-                    global_row["position_idx"] = "ALL"
-                    summary_df = pd.concat([summary_df, pd.DataFrame([global_row])], ignore_index=True)
-
-                    summary_df.to_csv(summary_path, index=False)
-
-                    print(f"[EVAL] Results saved to {save_path}")
-                    print(f"[EVAL] Summary saved to {summary_path}")
-
-                
-        if not inner:
-            self.episode_length_buf[died] = 0
-        # print("died ", time_out, self.episode_length_buf)
-        return died, time_out
-    
-    def is_time_out(self, max_episode_length=256):
-        if self.first_ep[1]:
-            self.first_ep[1] = False
-            max_episode_length = 2
-        time_out = self.episode_length_buf >= max_episode_length
-        return time_out
-
-    def _reset_idx(self, env_ids: torch.Tensor | None):
-        super()._reset_idx(env_ids)
-        if self.first_ep[0] or env_ids is None or len(env_ids) == self.num_envs:
-            env_ids = self._robot._ALL_INDICES.clone()
-
-        num_envs = len(env_ids)
-        if self.eval:
-            positions = self.eval_manager.get_positions()
-            self.scene_manager.apply_fixed_positions(env_ids, positions)
-        else:
-            self.scene_manager.randomize_scene(
-                env_ids,
-                mess=False, # или False, в зависимости от режима
-                use_obstacles=self.turn_on_obstacles,
-                all_defoult=False
-            )
-        self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        goal_pos_local  = self.scene_manager.get_active_goal_state(env_ids)
-        # BLOCK TEXT_EMB
-        # colors = ["red" if x.item() > 0 else "green" for x in goal_pos_local[:, 0]]
-        # text_prompts = [f"move to bowl near {c} wall" for c in colors]
-
-        # text_inputs = self.clip_processor(
-        #     text=text_prompts, return_tensors="pt", padding=True
-        # ).to(self.device)
-        # with torch.no_grad():
-        #     text_embeddings = self.clip_model.get_text_features(**text_inputs)
-        #     text_embeddings = text_embeddings / (text_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        # self.text_embeddings[env_ids] = text_embeddings
-
-        # print("goal_pos_local ", goal_pos_local)
-        self._desired_pos_w[env_ids, :3] = goal_pos_local 
-        self._desired_pos_w[env_ids, :2] = self.to_global(goal_pos_local , env_ids)
-
-        self.curriculum_learning_module(env_ids) 
-
-        if self.turn_on_controller_step > self.my_episode_lenght and self.turn_on_controller:
-            self.turn_on_controller_step = 0
-            self.turn_on_controller = False
-        
-        if not self.eval:
-            cond_imitation = (
-                not self.warm and
-                # self.mean_radius >= 3.3 and
-                self.sr_stack_full and
-                self.mean_radius != 0 and
-                self.use_controller and
-                not self.turn_on_controller and
-                not self.first_ep[0] and
-                self.turn_off_controller_step > self.my_episode_lenght
-            )
-            if cond_imitation: 
-                self.turn_on_controller_step = 0
-                self.turn_off_controller_step = 0
-                prob = lambda x: torch.rand(1).item() <= x
-                self.turn_on_controller = prob(0.01 * max(10, min(40, 100 - self.success_rate)))
-                print(f"turn controller: {self.turn_on_controller} with SR {self.success_rate}")
-            elif self.cur_step < self.warm_len:
-                if self.cur_step < self.without_imitation:
-                    self.turn_on_controller = False
-                else:
-                    self.turn_on_controller = True
-                
-        
-        if (self.mean_radius >= 3.3 or self.mean_radius <= 0.3 and self.use_obstacles) or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-        # if self.use_obstacles or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-            if self.turn_on_obstacles_always and self.cur_step % 300:
-                print("[ WARNING ] ostacles allways turn on")
-
-            self.turn_on_obstacles = True
-            if not self.turn_on_obstacles_always and not self.warm and self.min_level_radius < 3.3 and self.mean_radius == 3.3:
-                print("level_up min_level_radius to: ", 3.3)
-                self.min_level_radius = 3.3
-        else:
-            self.turn_on_obstacles = False
-        env_ids = env_ids.to(dtype=torch.long)
-
-        final_distance_to_goal = torch.linalg.norm(
-            self._desired_pos_w[env_ids, :2] - self._robot.data.root_pos_w[env_ids, :2], dim=1
-        ).mean()
-        self._robot.reset(env_ids)
-        if len(env_ids) == self.num_envs:
-            self.episode_length_buf = torch.zeros_like(self.episode_length_buf) #, high=int(self.max_episode_length))
-        self._actions[env_ids] = 0.0
-        min_radius = 1.2
-        if self.eval:
-            robot_pos_local, robot_quats = self.eval_manager.get_current_tasks(env_ids)
-            # здесь angle_errors можно применить для ориентации робота
-            # вычислим стартовую евклидову дистанцию в локальных координатах
-            start_dists_local = torch.linalg.norm(goal_pos_local[:, :2] - robot_pos_local[:, :2], dim=1)
-            # сохраним стартовые дистанции в eval_manager
-            self.eval_manager.set_start_dists(env_ids, start_dists_local)
-        else:
-            robot_pos_local, robot_quats = self.scene_manager.place_robot_for_goal(
-                env_ids,
-                mean_dist=self.mean_radius,
-                min_dist=1.2,
-                max_dist=4.0,
-                angle_error=self.cur_angle_error,
-            )
-        robot_pos  = robot_pos_local
-        # print("robot_pos_local ", robot_pos_local)
-        # print("bounds ", self.scene_manager.room_bounds)
-        # print("i'm in path_manager")
-        if self.turn_on_controller or self.imitation:
-            if self.imitation:
-                print("[ WARNING ] imitation mode on")
-            if self.turn_on_controller_step == 0:
-                env_ids_for_control = self._robot._ALL_INDICES.clone()
-                robot_pos_for_control = self._robot.data.default_root_state[env_ids_for_control, :2].clone()
-                robot_pos_for_control[env_ids, :2] = robot_pos[:, :2]
-                goal_pos_for_control = self._desired_pos_w[env_ids_for_control, :2].clone()
-                goal_pos_for_control[env_ids, :2] = goal_pos_local[:, :2]
-            else:
-                env_ids_for_control = env_ids
-                robot_pos_for_control = robot_pos
-                goal_pos_for_control = goal_pos_local[:, :2]
-            paths = None
-            possible_try_steps = 3
-            obstacle_positions_list = self.scene_manager.get_active_obstacle_positions_for_path_planning(env_ids)
-
-            for i in range(possible_try_steps):
-                paths = self.path_manager.get_paths( # Используем старый get_paths
-                    env_ids=env_ids_for_control,
-                    # Передаем данные для генерации ключа
-                    active_obstacle_positions_list=obstacle_positions_list,
-                    start_positions=robot_pos_local,
-                    target_positions=goal_pos_local[:, :2]
-                )
-                if paths is None:
-                    print(f"[ ERROR ] GET NONE PATH {i + 1} times")
-                    self.scene_manager.randomize_scene(
-                        env_ids_for_control,
-                        mess=False, # или False, в зависимости от режима
-                        use_obstacles=self.turn_on_obstacles,
-                    )
-                    goal_pos_local = self.scene_manager.get_active_goal_state(env_ids_for_control)
-                    self._desired_pos_w[env_ids_for_control, :3] = goal_pos_local
-                    self._desired_pos_w[env_ids_for_control, :2] = self.to_global(goal_pos_local, env_ids_for_control)
-                else:
-                    break
-            # print("out path_manager, paths: ", paths)
-            # print(len(paths), len(env_ids_for_control), len(goal_pos_for_control))
-            self.control_module.update_paths(env_ids_for_control, paths, goal_pos_for_control)
-        if self.memory_on:
-            self.memory_manager.reset()
-        # print(f"in reset envs: {env_ids} goals:", goal_pos_local[:, :2])
-        
-        self._update_scene_objects(env_ids) #self._robot._ALL_INDICES.clone())
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        default_root_state[:, 2] = 0.1
-        default_root_state[:, 3:7] = robot_quats
-        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-        # Логируем длину эпизодов для сброшенных сред
-        self.total_episode_length += torch.sum(self.episode_lengths[env_ids]).item()
-        self.episode_count += len(env_ids)
-        mean_episode_length = self.total_episode_length / self.episode_count if self.episode_count > 0 else 0.0
-        # self.tensorboard_writer.add_scalar("Metrics/Mean_episode_length", mean_episode_length, self.tensorboard_step)
-        # Сбрасываем счетчик длины для сброшенных сред
-        self.episode_lengths[env_ids] = 0
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # print("distance_to_goal ", distance_to_goal)
-        _, _, r_error, a_error = self.goal_reached(get_num_subs=True)
-        self.previous_distance_error[env_ids] = r_error[env_ids]
-        self.previous_angle_error[env_ids] = a_error[env_ids]
-        self.first_ep[0] = False
-        self.tracker.reset(env_ids)
-        env_ids_for_scene_embeddings = self._robot._ALL_INDICES.clone()
-        # scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        # for i in env_ids_for_scene_embeddings:
-        #     self.scene_manager.print_graph_info(i)
-        if self.LOG and self.sr_stack_full:
-            self.experiment.log_metric("success_rate", self.success_rate, step=self.tensorboard_step)
-            self.experiment.log_metric("mean_radius", self.mean_radius, step=self.tensorboard_step)
-            self.experiment.log_metric("max_angle", self.max_angle_error, step=self.tensorboard_step)
-            # self.experiment.log_metric("use obstacles", self.turn_on_obstacles.float(), step=self.tensorboard_step)
-
-    def to_local(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] - env_origins[env_ids, :2]
-    
-    def to_global(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] + env_origins[env_ids, :2]
-
-    def curriculum_learning_module(self, env_ids: torch.Tensor):
-        # print("self.success_rate ", self.success_rate)
-        # if self.mean_radius > 3.3:
-        #     max_angle_error = torch.pi * 0.8
-        if self.warm and self.cur_step >= self.warm_len:
-            self.warm = False
-            self.mean_radius = self.start_mean_radius
-            self.cur_angle_error = 0
-            self._step_update_counter = 0
-            print(f"end worm stage r: {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-        elif not self.warm and not self.turn_on_controller and self.sr_stack_full:
-            if self.success_rate >= self.sr_treshhold:
-                self.success_ep_num += 1
-                self.foult_ep_num = 0
-                if self.success_ep_num > self.num_envs:
-                    self.second_try = max(self.mean_radius, self.second_try)
-                    self.success_ep_num = 0
-                    old_mr = self.mean_radius
-                    old_a = self.cur_angle_error
-                    self.cur_angle_error += self.max_angle_error / 2
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    if self.cur_angle_error > self.max_angle_error:
-                        self.cur_angle_error = 0
-                        if self.mean_radius == 0:
-                            self.mean_radius += 0.3
-                        else:
-                            self.mean_radius += 1
-                        print(f"udate [ UP ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}")
-                    else:
-                        print(f"udate [ UP ] r: {round(self.mean_radius, 2)} a: from {round(old_a, 2)} to {round(self.cur_angle_error, 2)}")
-                    self._step_update_counter = 0
-                    self.update_sr_stack()
-            elif self.success_rate <= 10 or (self._step_update_counter >= 4000 and self.success_rate <= self.sr_treshhold):
-                self.foult_ep_num += 1
-                if self.foult_ep_num > 2000:
-                    self.success_ep_num = 0
-                    self.foult_ep_num = 0
-                    old_mr = self.mean_radius
-                    if self.cur_angle_error == 0:
-                        self.mean_radius += -0.1
-                        self.mean_radius = max(self.min_level_radius, self.mean_radius)
-                    self.cur_angle_error = 0
-                   
-                    self._step_update_counter = 0
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    print(f"udate [ DOWN ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-                    self.update_sr_stack()
-
-        self._obstacle_update_counter += 1
-        return None
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        pass
-
-    def _debug_vis_callback(self, event):
-        pass
-
-    def close(self):
-        # self.tensorboard_writer.close()
-        super().close()
-
-    def _update_scene_objects(self, env_ids: torch.Tensor):
-        """Векторизованное обновление позиций всех объектов в симуляторе."""
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        # Получаем все локальные позиции из scene_manager'а
-        all_local_positions = self.scene_manager.positions
-        
-        # Конвертируем в глобальные координаты
-        env_origins_expanded = self._terrain.env_origins.unsqueeze(1).expand_as(all_local_positions)
-        all_global_positions = all_local_positions + env_origins_expanded
-        
-        # Создаем тензор для ориентации (по умолчанию Y-up: w=1)
-        all_quats = torch.zeros(self.num_envs, self.scene_manager.num_total_objects, 4, device=self.device)
-        all_quats[..., 0] = 1.0
-        
-        # Собираем полные состояния (поза + ориентация)
-        all_root_states = torch.cat([all_global_positions, all_quats], dim=-1)
-        
-        # Итерируемся по объектам, управляемым симулятором
-        for name, object_instances in self.scene_objects.items():
-            if name not in self.scene_manager.object_map:
-                continue
-            
-            # Получаем индексы для данного типа объектов
-            indices = self.scene_manager.object_map[name]['indices']
-            
-            # Собираем состояния только для этих объектов
-            object_root_states = all_root_states[:, indices, :]
-            
-            # Обновляем каждый экземпляр этого типа
-            for i, instance in enumerate(object_instances):
-                # Выбираем срез для i-го экземпляра по всем окружениям
-                instance_states = object_root_states[:, i, :]
-                # Применяем маску: неактивные объекты берём из default_positions
-                active_mask = self.scene_manager.active[:, indices[i]]
-                # Используем дефолтные позиции из SceneManager
-                inactive_pos = self.scene_manager.default_positions[0, indices[i]]  # (3,)
-                inactive_pos = inactive_pos.expand(self.num_envs, -1)  # (num_envs, 3)
-                # Конвертируем в глобальные координаты
-                inactive_pos_global = inactive_pos + env_origins_expanded[:, indices[i], :]
-                # Векторизованное обновление позиций
-                final_positions = torch.where(
-                    active_mask.unsqueeze(-1),
-                    instance_states[:, :3],
-                    inactive_pos_global
-                )
-                instance_states[:, :3] = final_positions
-                if name == "bowl":
-                    rot = torch.tensor([0.0, 0.0, 0.7071, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                if name == "cabinet":
-                    rot = torch.tensor([0.7071, 0.0, 0.0, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                # Записываем состояния в симулятор
-                instance.write_root_pose_to_sim(instance_states, env_ids=self._robot._ALL_INDICES.clone())
-                # changeable_indices = self.scene_manager.type_map.get("changeable_color", torch.tensor([], dtype=torch.long))
-                # if len(changeable_indices) > 0:
-                #     obj_global_idx = indices[i].item()  # глобальный индекс объекта
-                #     if len(changeable_indices) > 0:
-                #         obj_global_idx = indices[i].item()
-                #         is_changeable = bool(((changeable_indices == obj_global_idx).any()).item())
-                #         if is_changeable:
-                #             color_vec = self.scene_manager.colors[0, obj_global_idx].cpu().tolist()
-                #             prim_path_template = self.asset_manager.all_prim_paths[obj_global_idx]
-
-                #             # Преобразуем шаблон env_.* -> env_{idx}
-                #             for env_id in env_ids.tolist():
-                #                 prim_path = prim_path_template.replace("env_.*", f"env_{env_id}")
-                #                 if prim_path is None:
-                #                     continue
-                #                 self._set_object_color(prim_path, obj_global_idx, color_vec)
-
-    def _prepare_color_material(self, color_vec: list[float]):
-        """Создаёт OmniPBR-материал для цвета и сохраняет в кеш (если ещё нет)."""
-        from pxr import Gf
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        if color_key in self._material_cache:
-            return self._material_cache[color_key]
-
-        mtl_created = []
-        omni.kit.commands.execute(
-            "CreateAndBindMdlMaterialFromLibrary",
-            mdl_name="OmniPBR.mdl",
-            mtl_name=f"mat_{color_key}",
-            mtl_created_list=mtl_created
-        )
-        if not mtl_created:
-            return None
-
-        mtl_path = mtl_created[0]
-        stage = omni.usd.get_context().get_stage()
-        shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-        try:
-            shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-        except Exception:
-            try:
-                shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                pass
-
-        self._material_cache[color_key] = mtl_path
-        return mtl_path
-
-    def _set_object_color(self, prim_path: str, obj_idx: int, color_vec: list[float]):
-        """Назначает цвет объекту (только если он изменился)."""
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        prev_key = self._applied_color_map.get(obj_idx)
-        if prev_key == color_key:
-            return  # цвет уже установлен
-
-        mtl_path = self._prepare_color_material(color_vec)
-        if mtl_path is None:
-            return
-
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=mtl_path
-        )
-        self._applied_color_map[obj_idx] = color_key
-
-
-    def _apply_color_to_prim(self, prim_path: str, color_vec: list[float]):
-        """
-        Создаёт (или берёт из кеша) материал OmniPBR для указанного цвета и привязывает его к prim_path.
-        color_vec — список/кортеж из 3 чисел [r,g,b] (0..1).
-        """
-        if prim_path is None:
-            return
-
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        # если уже применён к этому объекту — пропускаем
-        # (в _update_scene_objects мы будем сравнивать с self._applied_color_map[obj_idx])
-        print("material cache: ", self._material_cache)
-        if color_key not in self._material_cache:
-            mtl_created = []
-            omni.kit.commands.execute(
-                "CreateAndBindMdlMaterialFromLibrary",
-                mdl_name="OmniPBR.mdl",
-                mtl_name=f"mat_{color_key}",
-                mtl_created_list=mtl_created
-            )
-            if len(mtl_created) == 0:
-                # если по какой-то причине не создалось — выходим
-                return
-            mtl_path = mtl_created[0]
-            # попытка поменять параметр цвета внутри шейдера
-            stage = omni.usd.get_context().get_stage()
-            shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-            try:
-                shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                # у разных материалов имя порта может отличаться
-                try:
-                    shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-                except Exception:
-                    pass
-            self._material_cache[color_key] = mtl_path
-
-        # Привязываем материал (bind) к приму
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=self._material_cache[color_key]
-        )
-
-
-def log_embedding_stats(embedding):
-    mean_val = embedding.mean().item()
-    std_val = embedding.std().item()
-    min_val = embedding.min().item()
-    max_val = embedding.max().item()
-    print(f"[ EM ] mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env_img.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env_img.py
deleted file mode 100644
index 7d6afe913b..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/aloha_env_img.py
+++ /dev/null
@@ -1,1217 +0,0 @@
-# env.py
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
- 
-import gymnasium as gym
-import torch
-import math
-import numpy as np
-import os
-import torchvision.models as models
-import torchvision.transforms as transforms
-from torch import nn
-import random
-
-import isaaclab.sim as sim_utils
-from isaaclab.assets import Articulation, ArticulationCfg, RigidObject, RigidObjectCfg
-from isaaclab.envs import DirectRLEnv, DirectRLEnvCfg
-from isaaclab.envs.ui import BaseEnvWindow
-from isaaclab.markers import VisualizationMarkers
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sim import SimulationCfg, SimulationContext
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.math import subtract_frame_transforms
-from isaaclab.sensors import TiledCamera, TiledCameraCfg, ContactSensor, ContactSensorCfg
-from .scene_manager import SceneManager
-from .evaluation_manager import EvaluationManager
-from .control_manager import VectorizedPurePursuit
-from .path_manager import Path_manager
-from .memory_manager import Memory_manager, PathTracker
-from .asset_manager import AssetManager
-import omni.kit.commands
-import omni.usd
-import datetime
-# from torch.utils.tensorboard import SummaryWriter
-##
-# Pre-defined configs
-##
-from isaaclab_assets.robots.aloha import ALOHA_CFG
-from isaaclab.markers import CUBOID_MARKER_CFG
-from transformers import CLIPProcessor, CLIPModel
-from PIL import Image
-import omni.kit.commands  # Уже импортировано в вашем коде
-from omni.usd import get_context  # Для доступа к stage
-from pxr import Gf
-
-class WheeledRobotEnvWindow(BaseEnvWindow):
-    def __init__(self, env: 'WheeledRobotEnv', window_name: str = "IsaacLab"):
-        super().__init__(env, window_name)
-        with self.ui_window_elements["main_vstack"]:
-            with self.ui_window_elements["debug_frame"]:
-                with self.ui_window_elements["debug_vstack"]:
-                    self._create_debug_vis_ui_element("targets", self.env)
-
-@configclass
-class WheeledRobotEnvCfg(DirectRLEnvCfg):
-    episode_length_s = 512.0
-    decimation = 8
-    action_space = gym.spaces.Box(
-        low=np.array([-1.0, -1.0], dtype=np.float32),
-        high=np.array([1.0, 1.0], dtype=np.float32),
-        shape=(2,)
-    )
-    # Observation space is now the ResNet18 embedding size (512)
-    m = 1  # Например, 3 эмбеддинга и действия
-    # observation_space = gym.spaces.Box(
-    #     low=-float("inf"),
-    #     high=float("inf"),
-    #     shape=(m * (512 + 3),),  # m * (embedding_size + action_size) + 2 (скорости)
-    #     dtype="float32"
-    # )
-    # TODO automat compute num_total_objects
-    num_total_objects = 10
-
-    observation_space = gym.spaces.Dict({
-        "img": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(512 + 3,), dtype=np.float32),
-        "graph": gym.spaces.Dict({
-            "node_features": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 14), dtype=np.float32),
-            "edge_features": gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(num_total_objects, 6), dtype=np.float32),
-        })
-    })
-    state_space = 0
-    debug_vis = False
-
-    ui_window_class_type = WheeledRobotEnvWindow
-
-    sim: SimulationCfg = SimulationCfg(
-        dt=1/60,
-        render_interval=decimation,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="min",
-            restitution_combine_mode="min",
-            static_friction=0.2,
-            dynamic_friction=0.15,
-            restitution=0.0,
-        ),
-    )
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="plane",
-        collision_group=-1,
-        # physics_material=sim_utils.RigidBodyMaterialCfg(
-        #     friction_combine_mode="min",
-        #     restitution_combine_mode="min",
-        #     static_friction=0.8,
-        #     dynamic_friction=0.6,
-        #     restitution=0.0,
-        # ),
-        debug_vis=False,
-    )
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=64, env_spacing=18, replicate_physics=True)
-    robot: ArticulationCfg = ALOHA_CFG.replace(prim_path="/World/envs/env_.*/Robot")
-    wheel_radius = 0.068
-    wheel_distance = 0.34
-    tiled_camera: TiledCameraCfg = TiledCameraCfg(
-        prim_path="/World/envs/env_.*/Robot/box2_Link/Camera",
-        offset=TiledCameraCfg.OffsetCfg(pos=(-0.35, 0, 1.1), rot=(0.99619469809,0,0.08715574274,0), convention="world"),
-        # offset=TiledCameraCfg.OffsetCfg(pos=(0.0, 0, 0.9), rot=(1,0,0,0), convention="world"),
-        data_types=["rgb"],
-        spawn=sim_utils.PinholeCameraCfg(
-            focal_length=35.0, focus_distance=2.0, horizontal_aperture=36, clipping_range=(0.2, 10.0)
-        ),
-        width=224,
-        height=224,
-    )
-    current_dir = os.getcwd()
-    kitchen = sim_utils.UsdFileCfg(
-        usd_path=os.path.join(current_dir, "source/isaaclab_assets/data/aloha_assets", "scenes/scenes_sber_kitchen_for_BBQ/kitchen_new_simple.usd"),
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=True,
-            kinematic_enabled=True,
-            rigid_body_enabled=True,
-        ),
-        collision_props=sim_utils.CollisionPropertiesCfg(
-            collision_enabled=True,
-        ),
-    )
-    contact_sensor = ContactSensorCfg(
-        prim_path="/World/envs/env_.*/Robot/.*",
-        update_period=0.1,
-        history_length=1,
-        debug_vis=True,
-        filter_prim_paths_expr=["/World/envs/env_.*"],
-    )
-
-class WheeledRobotEnv(DirectRLEnv):
-    cfg: WheeledRobotEnvCfg
-
-    def __init__(self, cfg: WheeledRobotEnvCfg, render_mode: str | None = None, **kwargs):
-        self._super_init = True
-        self.current_dir = os.getcwd()
-        self.config_path=os.path.join(self.current_dir, "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json")
-        super().__init__(cfg, render_mode, **kwargs)
-        self._super_init = False
-        self.eval = False
-        self.eval_name = "CI"
-
-        self.eval_printed = False
-        self.scene_manager = SceneManager(self.num_envs, self.config_path, self.device)
-        self.eval_manager = EvaluationManager(self.num_envs)
-        self.eval_manager.set_task_lists(
-            robot_positions=[[0.0, -1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0],
-                             [1.0, -1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0],
-                             [2.0, -1.0, 0.0], [2.0, 0.0, 0.0], [2.0, 1.0, 0.0]],  # список стартовых позиций
-            angle_errors=[torch.pi, torch.pi*0.9, torch.pi*0.8]                         # список ошибок угла
-        )
-        self.use_controller = True
-        self.imitation = False
-        if self.imitation:
-            self.use_controller = True
-        if self.use_controller:
-            self.path_manager = Path_manager(scene_manager=self.scene_manager, ratio=8.0, shift=[5, 4], device=self.device)
-            self.control_module = VectorizedPurePursuit(num_envs=self.num_envs, device=self.device)
-        self.memory_on = False
-        self.tracker = PathTracker(num_envs=self.num_envs, device=self.device)
-        if self.memory_on:
-            self.memory_manager = Memory_manager(
-                num_envs=self.num_envs,
-                embedding_size=512,  # Размер эмбеддинга ResNet18
-                action_size=2,      # Размер действия (линейная и угловая скорость)
-                history_length=25,  # n = 10, можно настроить
-                device=self.device
-            )
-
-        self._actions = torch.zeros((self.num_envs, 2), device=self.device)
-        self._actions[:, 1] = 0.0
-        self._left_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._right_wheel_vel = torch.zeros(self.num_envs, device=self.device)
-        self._desired_pos_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self._episode_sums = {
-            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-            for key in ["moves"]
-        }
-        self._left_wheel_id = self._robot.find_joints("left_wheel")[0]
-        self._right_wheel_id = self._robot.find_joints("right_wheel")[0]
-
-        self.set_debug_vis(self.cfg.debug_vis)
-        self.Debug = True
-        self.event_update_counter = 0
-        self.episode_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.success_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.step_counter = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        self.possible_goal_position = []
-        
-        self.delete = 1
-        self.count = 0
-        self._debug_log_enabled = True
-        self._debug_envs_to_log = list(range(min(5, self.num_envs)))
-        self._inconsistencies = []
-        self._debug_step_counter = 0
-        self._debug_log_frequency = 10
-        self.turn_on_controller = False #it is not use or not use controller, it is flag for the first step
-        self.turn_on_controller_step = 0
-        self.my_episode_lenght = 256
-        self.turn_off_controller_step = 0
-        self.use_obstacles = True
-        self.turn_on_obstacles = False
-        self.turn_on_obstacles_always = False
-        if self.use_obstacles:
-            self.use_obstacles = True
-        self.previous_distance_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_angle_error = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_lin_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.previous_ang_vel = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        self.angular_speed = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        # Initialize ResNet18 for image embeddings
-        # self.resnet18 = models.resnet18(pretrained=True).to(self.device)
-        # self.resnet18.eval()  # Set to evaluation mode
-        # # Remove the final fully connected layer to get embeddings
-        # self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])
-        # # Image preprocessing for ResNet18
-        # transforms.ToTensor()
-        # self.transform = transforms.Compose([
-        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
-        # ])
-        self.success_rate = 0
-        self.sr_stack_capacity = 0
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-        self._step_update_counter = 0
-        self.mean_radius = 4.3
-        self.max_angle_error = torch.pi / 6
-        self.cur_angle_error = torch.pi / 12
-        self.warm = True
-        self.warm_len = 2500
-        self.without_imitation = self.warm_len / 2
-        self._obstacle_update_counter = 0
-        self.has_contact = torch.full((self.num_envs,), True, dtype=torch.bool, device=self.device)
-        self.sim = SimulationContext.instance()
-        self.obstacle_positions = None
-        self.key = None
-        self.success_ep_num = 0
-        # self.run = wandb.init(project="aloha_direct")
-        self.first_ep = [True, True]
-        self.first_ep_step = 0
-        self.second_ep = True
-        timestamp = datetime.datetime.now().strftime("%m_%d_%H_%M")
-        name = "dev"
-        self.episode_lengths = torch.zeros(self.num_envs, device=self.device)
-        self.episode_count = 0
-        self.total_episode_length = 0.0
-        # self.tensorboard_writer = SummaryWriter(log_dir=f"/home/xiso/IsaacLab/logs/tensorboard/navigation_rl_{name}_{timestamp}")
-        self.tensorboard_step = 0
-        self.cur_step = 0
-        self.velocities = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-        
-        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
-        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
-        self.clip_model.eval()  # Установить в режим оценки
-        self.second_try = 0
-        self.foult_ep_num = 0
-        # Инициализация стеков для хранения успехов (1 - успех, 0 - неуспех)
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.max_stack_size = 20  # Максимальный размер стека
-        self.sr_stack_full = False
-        self.start_mean_radius = 0
-        self.min_level_radius = 0
-        self.sr_treshhold = 85
-        self.LOG = False
-        self.text_embeddings = torch.zeros((self.num_envs, 512), device=self.device)
-        if self.LOG:
-            from comet_ml import start
-            from comet_ml.integration.pytorch import log_model
-            self.experiment = start(
-                api_key="DRYfW6B6VtUQr9llvf3jup57R",
-                project_name="general",
-                workspace="xisonik"
-            )
-        self.print_config_info()
-        self._setup_scene()
-        self.prim_paths = self.asset_manager.all_prim_paths
-        # сразу после создания scene_manager
-        self._material_cache = {}        # key -> material prim path (строка), key = "r_g_b"
-        self._applied_color_map = {}     # obj_index (int) -> color_key (str), чтобы не биндим повторно
-
-
-    def print_config_info(self):
-        print("__________[ CONGIFG INFO ]__________")
-        print(f"|")
-        print(f"| Start mean radius is: {self.mean_radius}")
-        print(f"|")
-        print(f"| Start amx angle is: {self.max_angle_error}")
-        print(f"|")
-        print(f"| Use controller: {self.use_controller}")
-        print(f"|")
-        print(f"| Full imitation: {self.imitation}")
-        print(f"|")
-        print(f"| Use memory: {self.memory_on}")
-        print(f"|")
-        print(f"| Use obstacles: {self.use_obstacles}")
-        print(f"|")
-        print(f"| Start radius: {self.start_mean_radius}, min: {self.min_level_radius}")
-        print(f"|")
-        print(f"| Warm len: {self.warm_len}")
-        print(f"|")
-        print(f"| Turn on obstacles always: {self.turn_on_obstacles_always}")
-        print(f"|")
-        print(f"_______[ CONGIFG INFO CLOSE ]_______")
-
-    def _setup_scene(self):
-        from isaaclab.sensors import ContactSensor
-        import time
-        from pxr import Usd
-        from isaaclab.sim.spawners.from_files import spawn_from_usd
-        import random
-        if self._super_init:
-            self._robot = Articulation(self.cfg.robot)
-            self.scene.articulations["robot"] = self._robot
-            self.cfg.terrain.num_envs = self.scene.cfg.num_envs
-            self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
-            self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
-            self.scene.clone_environments(copy_from_source=True)
-            self._tiled_camera = TiledCamera(self.cfg.tiled_camera)
-            self.scene.sensors["tiled_camera"] = self._tiled_camera
-            # Спавн кухни (статический элемент)
-            spawn_from_usd(
-                prim_path="/World/envs/env_.*/Kitchen",
-                cfg=self.cfg.kitchen,
-                translation=(5.0, 4.0, 0.0),
-                orientation=(0.0, 0.0, 0.0, 1.0),
-            )
-            self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
-            self.scene.sensors["contact_sensor"] = self._contact_sensor
-            self.asset_manager = AssetManager(config_path=self.config_path)
-            self.scene_objects = self.asset_manager.spawn_assets_in_scene()
-            
-            # self.scene_manager.update_prims(prim_path)
-            
-
-        # light_cfg = sim_utils.DomeLightCfg(intensity=300.0, color=(0.75, 0.75, 0.75))
-        # light_cfg.func("/World/Light", light_cfg)
-
-    def _get_observations(self) -> dict:
-        self.tensorboard_step += 1
-        self.cur_step += 1
-        self.episode_lengths += 1
-        import os
-        from PIL import Image, ImageDraw, ImageFont
-        # Получение RGB изображений с камеры
-        camera_data = self._tiled_camera.data.output["rgb"].clone()  # Shape: (num_envs, 224, 224, 3)
-        
-        # Преобразование изображений для CLIP
-        # CLIP ожидает изображения в формате PIL или тензоры с правильной нормализацией
-        images = camera_data.cpu().numpy().astype(np.uint8)  # Конвертация в numpy uint8
-        # inputs = self.clip_processor(images=images, return_tensors="pt", padding=True).to(self.device)
-        images_list = [Image.fromarray(im) for im in images]  # если images shape (N,H,W,3) numpy, это даёт список 2D-arrays
-        inputs = self.clip_processor(images=images_list, return_tensors="pt", padding=True)
-        for k, v in inputs.items():
-            inputs[k] = v.to(self.device)
-        # Получение эмбеддингов изображений
-        with torch.no_grad():
-            image_embeddings = self.clip_model.get_image_features(**inputs)  # Shape: (num_envs, 512)
-            image_embeddings = image_embeddings / (image_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        
-        # Получение скоростей робота
-        root_lin_vel_w = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1).unsqueeze(-1)
-        root_ang_vel_w = self._robot.data.root_ang_vel_w[:, 2].unsqueeze(-1)
-        
-        scene_embeddings = self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        scene_embeddings_dict = self.scene_manager.get_graph_obs(self._robot._ALL_INDICES.clone())
-        # obs = torch.cat([image_embeddings, scene_embeddings, text_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs_img = torch.cat([image_embeddings, root_lin_vel_w*0.1, root_ang_vel_w*0.1, self.previous_ang_vel.unsqueeze(-1)*0.1], dim=-1)
-        obs = {
-            "img": obs_img,
-            "graph": scene_embeddings_dict
-        }
-        self.previous_ang_vel = self.angular_speed
-        # log_embedding_stats(image_embeddings)
-        
-        observations = {"policy": obs}       
-        return observations
-
-    # as they are not affected by the observation space change.
-
-    def _pre_physics_step(self, actions: torch.Tensor):
-        if self.cur_step % 256 == 0:
-            print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-        env_ids = self._robot._ALL_INDICES.clone()
-        self._actions = actions.clone().clamp(-1.0, 1.0)
-
-        nan_mask = torch.isnan(self._actions) | torch.isinf(self._actions)
-        nan_indices = torch.nonzero(nan_mask.any(dim=1), as_tuple=False).squeeze()  # env_ids где любой action NaN/inf
-        if nan_indices.numel() > 0:
-            print(f"[WARNING] NaN/Inf in actions for envs: {nan_indices.tolist()}. Attempting recovery...")
-        r = self.cfg.wheel_radius
-        L = self.cfg.wheel_distance
-        self._step_update_counter += 1
-        if self.turn_on_controller or self.imitation:
-            self.turn_on_controller_step += 1
-            # Получаем текущую ориентацию (yaw) из кватерниона
-            quat = self._robot.data.root_quat_w
-            siny_cosp = 2 * (quat[:, 0] * quat[:, 3] + quat[:, 1] * quat[:, 2])
-            cosy_cosp = 1 - 2 * (quat[:, 2] * quat[:, 2] + quat[:, 3] * quat[:, 3])
-            yaw = torch.atan2(siny_cosp, cosy_cosp)
-            linear_speed, angular_speed = self.control_module.compute_controls(
-                self.to_local(self._robot.data.root_pos_w[:, :2],env_ids),
-                yaw
-            )
-            # angular_speed = -angular_speed 
-            self._actions[:, 0] = (linear_speed / 0.6) - 1
-            self._actions[:, 1] = angular_speed / 2
-            actions.copy_(self._actions.clamp(-1.0, 1.0))
-        else:
-            self.turn_off_controller_step += 1
-            linear_speed = 0.6*(self._actions[:, 0] + 1.0) # [num_envs], всегда > 0
-            angular_speed = 2*self._actions[:, 1]  # [num_envs], оставляем как есть от RL
-        # linear_speed = torch.tensor([0], device=self.device)
-        self.angular_speed = angular_speed
-        self.velocities = torch.stack([linear_speed, angular_speed], dim=1)
-        # if self.tensorboard_step % 4 ==0:
-        # self.delete = -1 * self.delete 
-        # angular_speed = torch.tensor([0], device=self.device)
-        # print("vel is: ", linear_speed, angular_speed)
-        self._left_wheel_vel = (linear_speed - (angular_speed * L / 2)) / r
-        self._right_wheel_vel = (linear_speed + (angular_speed * L / 2)) / r
-        # self._left_wheel_vel = torch.clamp(self._left_wheel_vel, -10, 10)
-        # self._right_wheel_vel = torch.clamp(self._right_wheel_vel, -10, 10)
-
-    def _apply_action(self):
-        wheel_velocities = torch.stack([self._left_wheel_vel, self._right_wheel_vel], dim=1).unsqueeze(-1).to(dtype=torch.float32)
-        self._robot.set_joint_velocity_target(wheel_velocities, joint_ids=[self._left_wheel_id, self._right_wheel_id])
-
-    def _get_rewards(self) -> torch.Tensor:
-        # env_ids = self._robot._ALL_INDICES.clone()
-        # num_envs = len(env_ids)
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        # joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        # joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        # default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        # default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        # default_root_state[:, 2] = 0.1
-        # self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        # self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        # self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-        lin_vel = torch.norm(self._robot.data.root_lin_vel_w[:, :2], dim=1)
-        
-        lin_vel_reward = torch.clamp(lin_vel*0.02, min=0, max=0.15)
-        ang_vel = self._robot.data.root_ang_vel_w[:, 2]
-        ang_vel_reward = torch.abs(self.angular_speed) * 0.1
-        a_penalty = 0.1 * torch.abs(self.angular_speed - self.previous_ang_vel) #+ torch.abs(lin_vel - self.previous_lin_vel))
-        # print("a_penalty ", -a_penalty, self.angular_speed, self.previous_ang_vel )
-        # self.previous_lin_vel = lin_vel
-
-        goal_reached, num_subs, r_error, a_error = self.goal_reached(get_num_subs=True)
-
-        moves = torch.clamp(5 * (self.previous_distance_error - r_error), min=0, max=1) + \
-                    torch.clamp(5 * (self.previous_angle_error - a_error), min=0, max=1)
-        env_ids = self._robot._ALL_INDICES.clone()
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        self.tracker.add_step(env_ids, self.to_local(root_pos_w, env_ids), self.velocities)
-        path_lengths = self.tracker.compute_path_lengths(env_ids)
-
-        moves_reward = moves * 0.1
-        
-        self.previous_distance_error = r_error
-        self.previous_angle_error = a_error
-
-        has_contact = self.get_contact()
-
-        time_out = self.is_time_out(self.my_episode_lenght-1)
-        time_out_penalty = -5 * time_out.float()
-
-        vel_penalty = -1 * (ang_vel_reward + lin_vel_reward)
-        mask = ~goal_reached
-        vel_penalty[mask] = 0
-        lin_vel_reward[goal_reached] = 0
-
-        paths = self.tracker.get_paths(env_ids)
-        # jerk_counts = self.tracker.compute_jerk(env_ids, threshold=0.2)
-        # print(jerk_counts)
-        start_dists = self.eval_manager.get_start_dists(env_ids)
-        if self.turn_on_controller:
-            IL_reward = 0.5
-            punish = - 0.05
-        else:
-            IL_reward = 0
-            punish = (
-                - 0.1
-                - ang_vel_reward / (1 + 2 * self.mean_radius)
-                + lin_vel_reward / (1 + 2 * self.mean_radius)
-            )
-        reward = (
-            IL_reward + punish #* r_error
-            + torch.clamp(goal_reached.float() * 7, min=0, max=15) #* (1 + start_dists) / (1 + path_lengths)
-            - torch.clamp(has_contact.float() * (5 + lin_vel_reward), min=0, max=10)
-        )
-
-        if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-            sr = self.update_success_rate(goal_reached)
-
-        # if torch.any(has_contact) or torch.any(goal_reached) or torch.any(time_out):
-        # print("path info")
-        # print(path_lengths)
-        # print(r_error)
-        # print("reward ", reward)
-        # print("- 0.1 - 0.05 * r_error ", - 0.1 - 0.05 * r_error)
-        # print("IL_reward * r_error ", IL_reward * r_error)
-        # print("goal_reached ", goal_reached)
-        # print("lin_vel_reward ", lin_vel_reward)
-        # print("torch.clamp(goal_reached ", torch.clamp(goal_reached.float() * 7 * (1 + self.mean_radius) / (1 + path_lengths), min=0, max=10))
-        # print("torch.clamp(has_contact ", torch.clamp(has_contact.float() * (3 + lin_vel_reward), min=0, max=6))
-        # print("ang_vel_reward ", -ang_vel_reward)
-        # print("___________")
-        check = {
-            "moves":moves,
-        }
-        for key, value in check.items():
-            self._episode_sums[key] += value
-
-        # if self.tensorboard_step % 100 == 0:
-        #     self.tensorboard_writer.add_scalar("Metrics/reward", torch.sum(reward), self.tensorboard_step)
-        return reward
-    
-    def quat_rotate(self, quat: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:
-        """
-        Вращение вектора vec кватернионом quat.
-        quat: [N, 4] (w, x, y, z)
-        vec: [N, 3]
-        Возвращает: [N, 3] - вектор vec, повернутый кватернионом quat
-        """
-        w, x, y, z = quat.unbind(dim=1)
-        vx, vy, vz = vec.unbind(dim=1)
-
-        # Кватернионное умножение q * v
-        qw = -x*vx - y*vy - z*vz
-        qx = w*vx + y*vz - z*vy
-        qy = w*vy + z*vx - x*vz
-        qz = w*vz + x*vy - y*vx
-
-        # Обратный кватернион q*
-        rw = w
-        rx = -x
-        ry = -y
-        rz = -z
-
-        # Результат (q * v) * q*
-        rx_new = qw*rx + qx*rw + qy*rz - qz*ry
-        ry_new = qw*ry - qx*rz + qy*rw + qz*rx
-        rz_new = qw*rz + qx*ry - qy*rx + qz*rw
-
-        return torch.stack([rx_new, ry_new, rz_new], dim=1)
-
-
-    def goal_reached(self, angle_threshold: float = 17, radius_threshold: float = 1.3, get_num_subs=False):
-        """
-        Проверяет достижение цели с учётом расстояния и направления взгляда робота.
-        distance_to_goal: [N] расстояния до цели
-        angle_threshold: максимально допустимый угол в радианах между направлением взгляда и вектором на цель
-        Возвращает: [N] булев тензор, True если цель достигнута
-        """
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root: ", self.to_local(root_pos_w))
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # Проверка по расстоянию (например, радиус достижения stored в self.radius)
-        close_enough = distance_to_goal <= radius_threshold
-
-        # Получаем ориентацию робота в виде кватерниона (w, x, y, z)
-        root_quat_w = self._robot.data.root_quat_w  # shape [N, 4]
-
-        # Локальный вектор взгляда робота (вперёд по оси X)
-        local_forward = torch.tensor([1.0, 0.0, 0.0], device=root_quat_w.device, dtype=root_quat_w.dtype)
-        local_forward = local_forward.unsqueeze(0).repeat(root_quat_w.shape[0], 1)  # [N, 3]
-
-        # Вектор взгляда в мировых координатах
-        forward_w = self.quat_rotate(root_quat_w, local_forward)  # [N, 3]
-
-        # Вектор от робота к цели
-        root_pos_w = self._robot.data.root_pos_w  # [N, 3]
-        to_goal = self._desired_pos_w - root_pos_w  # [N, 3]
-
-        # Нормализуем векторы
-        forward_w_norm = torch.nn.functional.normalize(forward_w[:, :2] , dim=1)
-        to_goal_norm = torch.nn.functional.normalize(to_goal[:, :2] , dim=1)
-
-        # Косинус угла между векторами взгляда и направления на цель
-        cos_angle = torch.sum(forward_w_norm * to_goal_norm, dim=1)
-        cos_angle = torch.clamp(cos_angle, -1.0, 1.0)  # для безопасности
-        # direction_to_goal = to_goal
-        # yaw_g = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-
-        # Вычисляем угол между векторами
-        angle = torch.acos(cos_angle)
-        angle_degrees = torch.abs(angle) * 180.0 / 3.141592653589793
-        # Проверяем, что угол меньше порога
-        facing_goal = angle_degrees < angle_threshold
-
-        # Итоговое условие: близко к цели и смотрит в её сторону
-        # print(distance_to_goal, angle_degrees)
-        
-        conditions = torch.stack([close_enough, facing_goal], dim=1)  # shape [N, M]
-        num_conditions_met = conditions.sum(dim=1)  # shape [N], количество True в каждой строк
-
-        # self.step_counter += torch.ones_like(self.step_counter) #torch.zeros(self.num_envs, dtype=torch.long, device=self.device)
-        # enouth_steps = self.step_counter > 4
-        # returns = torch.logical_and(torch.logical_and(close_enough, facing_goal), enouth_steps)
-        # self.step_counter = torch.where(returns, torch.zeros_like(self.step_counter), self.step_counter)
-        returns = torch.logical_and(close_enough, facing_goal)
-        # if torch.any(returns):
-        #     print(close_enough, facing_goal)
-        # print("returns", returns)
-        if get_num_subs == False:
-            return returns
-        return returns, num_conditions_met, distance_to_goal+0.1-radius_threshold, angle_degrees
-
-    def get_contact(self):
-        force_matrix = self.scene["contact_sensor"].data.net_forces_w
-        force_matrix[..., 2] = 0
-        forces_magnitude = torch.norm(torch.norm(force_matrix, dim=2), dim=1)  # shape: [batch_size, num_contacts]
-        # вычисляем модуль силы для каждого контакта
-        if force_matrix is not None and force_matrix.numel() > 0:
-            contact_forces = torch.norm(force_matrix, dim=-1)
-            if contact_forces.dim() >= 3:
-                has_contact = torch.any(contact_forces > 0.1, dim=(1, 2))
-            else:
-                has_contact = torch.any(contact_forces > 0.1, dim=1) if contact_forces.dim() == 2 else torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-            # print("c ", has_contact)
-            num_contacts_per_env = torch.sum(contact_forces > 0.1, dim=1)
-            high_contact_envs = num_contacts_per_env >= 1
-        else:
-            print("force_matrix_w is None or empty")
-            high_contact_envs = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
-        # if torch.any(high_contact_envs):
-        #     print("high_contact_envs ", high_contact_envs)
-        return high_contact_envs
-
-    def update_SR_history(self):
-        self.episode_completion_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.success_history = torch.zeros((self.num_envs*4, self.num_envs), dtype=torch.bool, device=self.device)
-        self.history_index = 0
-        self.history_len = torch.zeros(self.num_envs, device=self.device)
-
-    def update_success_rate(self, goal_reached):
-        if self.turn_on_controller:
-            return torch.tensor(self.success_rate, device=self.device)
-        
-        # Получаем завершенные эпизоды
-        died, time_out = self._get_dones(self.my_episode_lenght - 1, inner=True)
-        completed = died | time_out
-        
-        if torch.any(completed):
-            # Получаем релевантные среды среди завершенных
-            # Фильтруем завершенные среды, оставляя только релевантные
-            relevant_completed = self._robot._ALL_INDICES[completed] #relevant_env_ids[(relevant_env_ids.view(1, -1) == self._robot._ALL_INDICES[completed].view(-1, 1)).any(dim=0)]
-            success = goal_reached.clone()
-            # Обновляем стеки для релевантных завершенных сред
-            for env_id in self._robot._ALL_INDICES.clone()[completed]:
-                env_id = env_id.item()
-                if not success[env_id]:#here idia is colulate all fault and sucess only on relative envs
-                    self.success_stacks[env_id].append(0)
-                elif env_id in relevant_completed:
-                    self.success_stacks[env_id].append(1)
-                
-                if len(self.success_stacks[env_id]) > self.max_stack_size:
-                    self.success_stacks[env_id].pop(0)
-            # print("self.success_stacks ", self.success_stacks)
-        # Вычисляем процент успеха для всех сред с непустыми стеками
-        # Подсчитываем общий процент успеха по всем релевантным средам
-        total_successes = 0
-        total_elements = 0
-        # print(self.success_stacks)
-        for env_id in range(self.num_envs):
-            stack = self.success_stacks[env_id]
-            if len(stack) == 0:
-                continue
-            total_successes += sum(stack)
-            total_elements += len(stack)
-        # print("update ", self.success_stacks)
-        # Вычисляем процент успеха
-        # print("total_successes ", total_successes, total_elements)
-        self.sr_stack_capacity = total_elements
-        if total_elements > 0:
-            self.success_rate = (total_successes / total_elements) * 100.0
-        else:
-            self.success_rate = 0.0
-        # print(total_elements)
-        if total_elements >= self.num_envs * self.max_stack_size * 0.9:
-            self.sr_stack_full = True
-        # print(success_rates, self.success_rate)
-        return self.success_rate
-    
-    def update_sr_stack(self):
-        self.success_stacks = [[] for _ in range(self.num_envs)]  # Список списков для каждой среды
-        self.sr_stack_full = False
-
-    def _get_dones(self, my_episode_lenght = 256, inner=False) -> tuple[torch.Tensor, torch.Tensor]:
-        time_out = self.is_time_out(my_episode_lenght)
-        
-        has_contact = self.get_contact()
-        self.has_contact = has_contact
-        died = torch.logical_or(
-            torch.logical_or(self.goal_reached(), has_contact),
-            time_out,
-        )
-        env_ids = self._robot._ALL_INDICES[died]
-        if torch.any(died):
-            goal_reached = self.goal_reached()
-            if self.eval:
-                # === EVAL LOGGING ===
-                envs_finished = torch.where(died)[0]  # индексы завершившихся эпизодов
-                # успехи конкретно для них
-                successes = goal_reached[envs_finished].float()
-                traj_lens = self.tracker.compute_path_lengths(envs_finished)  # [K]
-                durations = self.tracker.lengths[envs_finished].float()       # [K]
-                start_dists = self.eval_manager.get_start_dists(envs_finished) # [K]
-                # логируем (ВАЖНО: log_results -> ДО next_episode)
-                self.eval_manager.log_results(envs_finished, successes, traj_lens, start_dists, durations)
-                self.eval_manager.next_episode(envs_finished)
-
-                if self.eval_manager.is_all_done() and not self.eval_printed:
-                    import pandas as pd
-                    df, global_stats, pos_stats = self.eval_manager.summarize()
-                    print("=== FINAL EVAL SUMMARY ===")
-                    print(global_stats)
-                    self.eval_printed = True
-
-                    # создаём директорию logs если её нет
-                    log_dir = os.path.join(self.current_dir, "logs/skrl/results")
-                    os.makedirs(log_dir, exist_ok=True)
-
-                    # 1. Сохраняем сырые результаты
-                    save_path = os.path.join(log_dir, f"eval_results_{self.eval_name}.csv")
-                    df.to_csv(save_path, index=False)
-
-                    # 2. Сохраняем агрегированную информацию
-                    summary_path = os.path.join(log_dir, f"eval_summary_{self.eval_name}.csv")
-
-                    # превращаем global_stats и pos_stats в один DataFrame
-                    summary_df = pos_stats.copy()
-                    summary_df["position_idx"] = summary_df.index
-                    summary_df.reset_index(drop=True, inplace=True)
-
-                    # добавляем глобальные метрики как отдельную строку
-                    global_row = global_stats.to_dict()
-                    global_row["position_idx"] = "ALL"
-                    summary_df = pd.concat([summary_df, pd.DataFrame([global_row])], ignore_index=True)
-
-                    summary_df.to_csv(summary_path, index=False)
-
-                    print(f"[EVAL] Results saved to {save_path}")
-                    print(f"[EVAL] Summary saved to {summary_path}")
-
-                
-        if not inner:
-            self.episode_length_buf[died] = 0
-        # print("died ", time_out, self.episode_length_buf)
-        return died, time_out
-    
-    def is_time_out(self, max_episode_length=256):
-        if self.first_ep[1]:
-            self.first_ep[1] = False
-            max_episode_length = 2
-        time_out = self.episode_length_buf >= max_episode_length
-        return time_out
-
-    def _reset_idx(self, env_ids: torch.Tensor | None):
-        super()._reset_idx(env_ids)
-        if self.first_ep[0] or env_ids is None or len(env_ids) == self.num_envs:
-            env_ids = self._robot._ALL_INDICES.clone()
-
-        num_envs = len(env_ids)
-        if self.eval:
-            positions = self.eval_manager.get_positions()
-            self.scene_manager.apply_fixed_positions(env_ids, positions)
-        else:
-            self.scene_manager.randomize_scene(
-                env_ids,
-                mess=False, # или False, в зависимости от режима
-                use_obstacles=self.turn_on_obstacles,
-                all_defoult=False
-            )
-        self.scene_manager.get_graph_embedding(self._robot._ALL_INDICES.clone())
-        goal_pos_local  = self.scene_manager.get_active_goal_state(env_ids)
-        # BLOCK TEXT_EMB
-        # colors = ["red" if x.item() > 0 else "green" for x in goal_pos_local[:, 0]]
-        # text_prompts = [f"move to bowl near {c} wall" for c in colors]
-
-        # text_inputs = self.clip_processor(
-        #     text=text_prompts, return_tensors="pt", padding=True
-        # ).to(self.device)
-        # with torch.no_grad():
-        #     text_embeddings = self.clip_model.get_text_features(**text_inputs)
-        #     text_embeddings = text_embeddings / (text_embeddings.norm(dim=1, keepdim=True) + 1e-9)
-        # self.text_embeddings[env_ids] = text_embeddings
-
-        # print("goal_pos_local ", goal_pos_local)
-        self._desired_pos_w[env_ids, :3] = goal_pos_local 
-        self._desired_pos_w[env_ids, :2] = self.to_global(goal_pos_local , env_ids)
-
-        self.curriculum_learning_module(env_ids) 
-
-        if self.turn_on_controller_step > self.my_episode_lenght and self.turn_on_controller:
-            self.turn_on_controller_step = 0
-            self.turn_on_controller = False
-        
-        if not self.eval:
-            cond_imitation = (
-                not self.warm and
-                # self.mean_radius >= 3.3 and
-                self.sr_stack_full and
-                self.mean_radius != 0 and
-                self.use_controller and
-                not self.turn_on_controller and
-                not self.first_ep[0] and
-                self.turn_off_controller_step > self.my_episode_lenght
-            )
-            if cond_imitation: 
-                self.turn_on_controller_step = 0
-                self.turn_off_controller_step = 0
-                prob = lambda x: torch.rand(1).item() <= x
-                self.turn_on_controller = prob(0.01 * max(10, min(40, 100 - self.success_rate)))
-                print(f"turn controller: {self.turn_on_controller} with SR {self.success_rate}")
-            elif self.cur_step < self.warm_len:
-                if self.cur_step < self.without_imitation:
-                    self.turn_on_controller = False
-                else:
-                    self.turn_on_controller = True
-                
-        
-        if (self.mean_radius >= 3.3 and self.use_obstacles) or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-        # if self.use_obstacles or self.turn_on_obstacles_always or self.warm and not self.first_ep[0]:
-            if self.turn_on_obstacles_always and self.cur_step % 300:
-                print("[ WARNING ] ostacles allways turn on")
-
-            self.turn_on_obstacles = True
-            if not self.turn_on_obstacles_always and not self.warm and self.min_level_radius < 3.3:
-                print("level_up min_level_radius to: ", 3.3)
-                self.min_level_radius = 3.3
-        else:
-            self.turn_on_obstacles = False
-        env_ids = env_ids.to(dtype=torch.long)
-
-        final_distance_to_goal = torch.linalg.norm(
-            self._desired_pos_w[env_ids, :2] - self._robot.data.root_pos_w[env_ids, :2], dim=1
-        ).mean()
-        self._robot.reset(env_ids)
-        if len(env_ids) == self.num_envs:
-            self.episode_length_buf = torch.zeros_like(self.episode_length_buf) #, high=int(self.max_episode_length))
-        self._actions[env_ids] = 0.0
-        min_radius = 1.2
-        if self.eval:
-            robot_pos_local, robot_quats = self.eval_manager.get_current_tasks(env_ids)
-            # здесь angle_errors можно применить для ориентации робота
-            # вычислим стартовую евклидову дистанцию в локальных координатах
-            start_dists_local = torch.linalg.norm(goal_pos_local[:, :2] - robot_pos_local[:, :2], dim=1)
-            # сохраним стартовые дистанции в eval_manager
-            self.eval_manager.set_start_dists(env_ids, start_dists_local)
-        else:
-            robot_pos_local, robot_quats = self.scene_manager.place_robot_for_goal(
-                env_ids,
-                mean_dist=self.mean_radius,
-                min_dist=1.2,
-                max_dist=4.0,
-                angle_error=self.cur_angle_error,
-            )
-        robot_pos  = robot_pos_local
-        # print("robot_pos_local ", robot_pos_local)
-        # print("bounds ", self.scene_manager.room_bounds)
-        # print("i'm in path_manager")
-        if self.turn_on_controller or self.imitation:
-            if self.imitation:
-                print("[ WARNING ] imitation mode on")
-            if self.turn_on_controller_step == 0:
-                env_ids_for_control = self._robot._ALL_INDICES.clone()
-                robot_pos_for_control = self._robot.data.default_root_state[env_ids_for_control, :2].clone()
-                robot_pos_for_control[env_ids, :2] = robot_pos[:, :2]
-                goal_pos_for_control = self._desired_pos_w[env_ids_for_control, :2].clone()
-                goal_pos_for_control[env_ids, :2] = goal_pos_local[:, :2]
-            else:
-                env_ids_for_control = env_ids
-                robot_pos_for_control = robot_pos
-                goal_pos_for_control = goal_pos_local[:, :2]
-            paths = None
-            possible_try_steps = 3
-            obstacle_positions_list = self.scene_manager.get_active_obstacle_positions_for_path_planning(env_ids)
-
-            for i in range(possible_try_steps):
-                paths = self.path_manager.get_paths( # Используем старый get_paths
-                    env_ids=env_ids_for_control,
-                    # Передаем данные для генерации ключа
-                    active_obstacle_positions_list=obstacle_positions_list,
-                    start_positions=robot_pos_local,
-                    target_positions=goal_pos_local[:, :2]
-                )
-                if paths is None:
-                    print(f"[ ERROR ] GET NONE PATH {i + 1} times")
-                    self.scene_manager.randomize_scene(
-                        env_ids_for_control,
-                        mess=False, # или False, в зависимости от режима
-                        use_obstacles=self.turn_on_obstacles,
-                    )
-                    goal_pos_local = self.scene_manager.get_active_goal_state(env_ids_for_control)
-                    self._desired_pos_w[env_ids_for_control, :3] = goal_pos_local
-                    self._desired_pos_w[env_ids_for_control, :2] = self.to_global(goal_pos_local, env_ids_for_control)
-                else:
-                    break
-            # print("out path_manager, paths: ", paths)
-            # print(len(paths), len(env_ids_for_control), len(goal_pos_for_control))
-            self.control_module.update_paths(env_ids_for_control, paths, goal_pos_for_control)
-        if self.memory_on:
-            self.memory_manager.reset()
-        # print(f"in reset envs: {env_ids} goals:", goal_pos_local[:, :2])
-        
-        self._update_scene_objects(env_ids) #self._robot._ALL_INDICES.clone())
-        # value = torch.tensor([0, 0], dtype=torch.float32, device=self.device)
-        # robot_pos = value.unsqueeze(0).repeat(num_envs, 1)
-        joint_pos = self._robot.data.default_joint_pos[env_ids].clone()
-        joint_vel = self._robot.data.default_joint_vel[env_ids].clone()
-        default_root_state = self._robot.data.default_root_state[env_ids].clone()
-        default_root_state[:, :2] = self.to_global(robot_pos, env_ids)
-        default_root_state[:, 2] = 0.1
-        default_root_state[:, 3:7] = robot_quats
-        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-        # Логируем длину эпизодов для сброшенных сред
-        self.total_episode_length += torch.sum(self.episode_lengths[env_ids]).item()
-        self.episode_count += len(env_ids)
-        mean_episode_length = self.total_episode_length / self.episode_count if self.episode_count > 0 else 0.0
-        # self.tensorboard_writer.add_scalar("Metrics/Mean_episode_length", mean_episode_length, self.tensorboard_step)
-        # Сбрасываем счетчик длины для сброшенных сред
-        self.episode_lengths[env_ids] = 0
-        root_pos_w = self._robot.data.root_pos_w[:, :2]
-        # print("root_pos_w ", root_pos_w)
-        distance_to_goal = torch.linalg.norm(self._desired_pos_w[:, :2] - root_pos_w, dim=1)
-        # print("distance_to_goal ", distance_to_goal)
-        _, _, r_error, a_error = self.goal_reached(get_num_subs=True)
-        self.previous_distance_error[env_ids] = r_error[env_ids]
-        self.previous_angle_error[env_ids] = a_error[env_ids]
-        self.first_ep[0] = False
-        self.tracker.reset(env_ids)
-        env_ids_for_scene_embeddings = self._robot._ALL_INDICES.clone()
-        # scene_embeddings = self.scene_manager.get_scene_embedding(env_ids)
-        # for i in env_ids_for_scene_embeddings:
-        #     self.scene_manager.print_graph_info(i)
-        if self.LOG and self.sr_stack_full:
-            self.experiment.log_metric("success_rate", self.success_rate, step=self.tensorboard_step)
-            self.experiment.log_metric("mean_radius", self.mean_radius, step=self.tensorboard_step)
-            self.experiment.log_metric("max_angle", self.max_angle_error, step=self.tensorboard_step)
-            # self.experiment.log_metric("use obstacles", self.turn_on_obstacles.float(), step=self.tensorboard_step)
-
-    def to_local(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] - env_origins[env_ids, :2]
-    
-    def to_global(self, pos, env_ids=None, env_origins=None):
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        if env_origins is None:
-            env_origins = self._terrain.env_origins
-        return pos[:, :2] + env_origins[env_ids, :2]
-
-    def curriculum_learning_module(self, env_ids: torch.Tensor):
-        # print("self.success_rate ", self.success_rate)
-        # if self.mean_radius > 3.3:
-        #     max_angle_error = torch.pi * 0.8
-        if self.warm and self.cur_step >= self.warm_len:
-            self.warm = False
-            self.mean_radius = self.start_mean_radius
-            self.cur_angle_error = 0
-            self._step_update_counter = 0
-            print(f"end worm stage r: {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-        elif not self.warm and not self.turn_on_controller and self.sr_stack_full:
-            if self.success_rate >= self.sr_treshhold:
-                self.success_ep_num += 1
-                self.foult_ep_num = 0
-                if self.success_ep_num > self.num_envs:
-                    self.second_try = max(self.mean_radius, self.second_try)
-                    self.success_ep_num = 0
-                    old_mr = self.mean_radius
-                    old_a = self.cur_angle_error
-                    self.cur_angle_error += self.max_angle_error / 2
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    if self.cur_angle_error > self.max_angle_error:
-                        self.cur_angle_error = 0
-                        if self.mean_radius == 0:
-                            self.mean_radius += 0.3
-                        else:
-                            self.mean_radius += 1
-                        print(f"udate [ UP ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}")
-                    else:
-                        print(f"udate [ UP ] r: {round(self.mean_radius, 2)} a: from {round(old_a, 2)} to {round(self.cur_angle_error, 2)}")
-                    self._step_update_counter = 0
-                    self.update_sr_stack()
-            elif self.success_rate <= 10 or (self._step_update_counter >= 4000 and self.success_rate <= self.sr_treshhold):
-                self.foult_ep_num += 1
-                if self.foult_ep_num > 2000:
-                    self.success_ep_num = 0
-                    self.foult_ep_num = 0
-                    old_mr = self.mean_radius
-                    if self.cur_angle_error == 0:
-                        self.mean_radius += -0.1
-                        self.mean_radius = max(self.min_level_radius, self.mean_radius)
-                    self.cur_angle_error = 0
-                   
-                    self._step_update_counter = 0
-                    print("[ sr ]: ", round(self.success_rate, 2), self.sr_stack_capacity)
-                    print(f"udate [ DOWN ] r: from {round(old_mr, 2)} to {round(self.mean_radius, 2)}, a: {round(self.cur_angle_error, 2)}")
-                    self.update_sr_stack()
-
-        self._obstacle_update_counter += 1
-        return None
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        pass
-
-    def _debug_vis_callback(self, event):
-        pass
-
-    def close(self):
-        # self.tensorboard_writer.close()
-        super().close()
-
-    def _update_scene_objects(self, env_ids: torch.Tensor):
-        """Векторизованное обновление позиций всех объектов в симуляторе."""
-        if env_ids is None:
-            env_ids = self._robot._ALL_INDICES.clone()
-        # Получаем все локальные позиции из scene_manager'а
-        all_local_positions = self.scene_manager.positions
-        
-        # Конвертируем в глобальные координаты
-        env_origins_expanded = self._terrain.env_origins.unsqueeze(1).expand_as(all_local_positions)
-        all_global_positions = all_local_positions + env_origins_expanded
-        
-        # Создаем тензор для ориентации (по умолчанию Y-up: w=1)
-        all_quats = torch.zeros(self.num_envs, self.scene_manager.num_total_objects, 4, device=self.device)
-        all_quats[..., 0] = 1.0
-        
-        # Собираем полные состояния (поза + ориентация)
-        all_root_states = torch.cat([all_global_positions, all_quats], dim=-1)
-        
-        # Итерируемся по объектам, управляемым симулятором
-        for name, object_instances in self.scene_objects.items():
-            if name not in self.scene_manager.object_map:
-                continue
-            
-            # Получаем индексы для данного типа объектов
-            indices = self.scene_manager.object_map[name]['indices']
-            
-            # Собираем состояния только для этих объектов
-            object_root_states = all_root_states[:, indices, :]
-            
-            # Обновляем каждый экземпляр этого типа
-            for i, instance in enumerate(object_instances):
-                # Выбираем срез для i-го экземпляра по всем окружениям
-                instance_states = object_root_states[:, i, :]
-                # Применяем маску: неактивные объекты берём из default_positions
-                active_mask = self.scene_manager.active[:, indices[i]]
-                # Используем дефолтные позиции из SceneManager
-                inactive_pos = self.scene_manager.default_positions[0, indices[i]]  # (3,)
-                inactive_pos = inactive_pos.expand(self.num_envs, -1)  # (num_envs, 3)
-                # Конвертируем в глобальные координаты
-                inactive_pos_global = inactive_pos + env_origins_expanded[:, indices[i], :]
-                # Векторизованное обновление позиций
-                final_positions = torch.where(
-                    active_mask.unsqueeze(-1),
-                    instance_states[:, :3],
-                    inactive_pos_global
-                )
-                instance_states[:, :3] = final_positions
-                if name == "bowl":
-                    rot = torch.tensor([0.0, 0.0, 0.7071, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                if name == "cabinet":
-                    rot = torch.tensor([0.7071, 0.0, 0.0, 0.7071], device=self.device).expand(self.num_envs, -1)
-                    instance_states[:, 3:7] = rot
-                # Записываем состояния в симулятор
-                instance.write_root_pose_to_sim(instance_states, env_ids=self._robot._ALL_INDICES.clone())
-                # changeable_indices = self.scene_manager.type_map.get("changeable_color", torch.tensor([], dtype=torch.long))
-                # if len(changeable_indices) > 0:
-                #     obj_global_idx = indices[i].item()  # глобальный индекс объекта
-                #     if len(changeable_indices) > 0:
-                #         obj_global_idx = indices[i].item()
-                #         is_changeable = bool(((changeable_indices == obj_global_idx).any()).item())
-                #         if is_changeable:
-                #             color_vec = self.scene_manager.colors[0, obj_global_idx].cpu().tolist()
-                #             prim_path_template = self.asset_manager.all_prim_paths[obj_global_idx]
-
-                #             # Преобразуем шаблон env_.* -> env_{idx}
-                #             for env_id in env_ids.tolist():
-                #                 prim_path = prim_path_template.replace("env_.*", f"env_{env_id}")
-                #                 if prim_path is None:
-                #                     continue
-                #                 self._set_object_color(prim_path, obj_global_idx, color_vec)
-
-    def _prepare_color_material(self, color_vec: list[float]):
-        """Создаёт OmniPBR-материал для цвета и сохраняет в кеш (если ещё нет)."""
-        from pxr import Gf
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        if color_key in self._material_cache:
-            return self._material_cache[color_key]
-
-        mtl_created = []
-        omni.kit.commands.execute(
-            "CreateAndBindMdlMaterialFromLibrary",
-            mdl_name="OmniPBR.mdl",
-            mtl_name=f"mat_{color_key}",
-            mtl_created_list=mtl_created
-        )
-        if not mtl_created:
-            return None
-
-        mtl_path = mtl_created[0]
-        stage = omni.usd.get_context().get_stage()
-        shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-        try:
-            shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-        except Exception:
-            try:
-                shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                pass
-
-        self._material_cache[color_key] = mtl_path
-        return mtl_path
-
-    def _set_object_color(self, prim_path: str, obj_idx: int, color_vec: list[float]):
-        """Назначает цвет объекту (только если он изменился)."""
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        prev_key = self._applied_color_map.get(obj_idx)
-        if prev_key == color_key:
-            return  # цвет уже установлен
-
-        mtl_path = self._prepare_color_material(color_vec)
-        if mtl_path is None:
-            return
-
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=mtl_path
-        )
-        self._applied_color_map[obj_idx] = color_key
-
-
-    def _apply_color_to_prim(self, prim_path: str, color_vec: list[float]):
-        """
-        Создаёт (или берёт из кеша) материал OmniPBR для указанного цвета и привязывает его к prim_path.
-        color_vec — список/кортеж из 3 чисел [r,g,b] (0..1).
-        """
-        if prim_path is None:
-            return
-
-        color_key = f"{color_vec[0]:.3f}_{color_vec[1]:.3f}_{color_vec[2]:.3f}"
-        # если уже применён к этому объекту — пропускаем
-        # (в _update_scene_objects мы будем сравнивать с self._applied_color_map[obj_idx])
-        print("material cache: ", self._material_cache)
-        if color_key not in self._material_cache:
-            mtl_created = []
-            omni.kit.commands.execute(
-                "CreateAndBindMdlMaterialFromLibrary",
-                mdl_name="OmniPBR.mdl",
-                mtl_name=f"mat_{color_key}",
-                mtl_created_list=mtl_created
-            )
-            if len(mtl_created) == 0:
-                # если по какой-то причине не создалось — выходим
-                return
-            mtl_path = mtl_created[0]
-            # попытка поменять параметр цвета внутри шейдера
-            stage = omni.usd.get_context().get_stage()
-            shader = stage.GetPrimAtPath(mtl_path + "/Shader")
-            try:
-                shader.GetAttribute("inputs:diffuse_color_constant").Set(Gf.Vec3f(*color_vec))
-            except Exception:
-                # у разных материалов имя порта может отличаться
-                try:
-                    shader.GetAttribute("inputs:base_color").Set(Gf.Vec3f(*color_vec))
-                except Exception:
-                    pass
-            self._material_cache[color_key] = mtl_path
-
-        # Привязываем материал (bind) к приму
-        omni.kit.commands.execute(
-            "BindMaterial",
-            prim_path=prim_path,
-            material_path=self._material_cache[color_key]
-        )
-
-
-def log_embedding_stats(embedding):
-    mean_val = embedding.mean().item()
-    std_val = embedding.std().item()
-    min_val = embedding.min().item()
-    max_val = embedding.max().item()
-    print(f"[ EM ] mean={mean_val:.4f}, std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f}")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/asset_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/asset_manager.py
deleted file mode 100644
index cf97e17aa4..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/asset_manager.py
+++ /dev/null
@@ -1,78 +0,0 @@
-import json
-import random
-from isaaclab.assets import RigidObject, RigidObjectCfg
-import isaaclab.sim as sim_utils
-import os
-
-class AssetManager:
-    """Управляет загрузкой конфига и созданием ассетов в симуляции."""
-    def __init__(self, config_path: str):
-        with open(config_path, 'r') as f:
-            self.config = json.load(f)['objects']
-        
-        self.sim_objects = {}    # name -> [RigidObject, ...]
-        self.prim_paths = {}     # name -> [prim_path_str, ...]
-        self.all_prim_paths = [] # список prim_path для всех объектов в порядке индексов (глобальный mapping)
-
-    def spawn_assets_in_scene(self):
-        """Создает все объекты из конфига в симуляторе Isaac Lab."""
-        current_dir = os.getcwd()
-        j = 0
-        for obj_cfg in self.config:
-            j += 1
-            name = obj_cfg['name']
-            types = obj_cfg['type']
-            if "info" in types:
-                print("info with ", name)
-                continue  
-            count = obj_cfg['count']
-            usd_paths = obj_cfg['usd_paths']
-            
-            self.sim_objects[name] = []
-            self.prim_paths[name] = []
-            for i in range(count):
-                random_usd_path = random.choice(usd_paths)
-                usd_path = os.path.join(
-                    current_dir,
-                    "source/isaaclab_assets/data/aloha_assets",
-                    random_usd_path
-                )
-                rot = (1.0, 0.0, 0.0, 0.0) 
-                if name == "bowl":
-                    # Для миски используем Z-up ориентацию (кватернион [0, 0.7071, 0, 0.7071])
-                    rot = (0.0, 0.7071, 0.0, 0.7071)
-                if name == "bowl":
-                    # Для миски используем Z-up ориентацию (кватернион [0, 0.7071, 0, 0.7071])
-                    rot = (0.7071, 0.0, 0.0, 0.7071)
-
-                prim_path = f"/World/envs/env_.*/{name}_{i}"
-
-                instance_cfg = RigidObjectCfg(
-                    prim_path=prim_path,
-                    spawn=sim_utils.UsdFileCfg(
-                        usd_path=usd_path,
-                        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-                            rigid_body_enabled=True,
-                            kinematic_enabled=True,
-                        ),
-                        collision_props=sim_utils.CollisionPropertiesCfg(
-                            collision_enabled=True,
-                        ),
-                        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-                            articulation_enabled=False,  # Отключаем articulation root
-                        ),
-                    ),
-                    init_state=RigidObjectCfg.InitialStateCfg(
-                        pos=(5.0 + i, 6.0 + j, 0.0),  # Начальная позиция (кладбище)
-                        rot=rot
-                    ),
-                )
-                obj_instance = RigidObject(cfg=instance_cfg)
-
-                # сохраняем сам объект
-                self.sim_objects[name].append(obj_instance)
-                # сохраняем prim_path
-                self.prim_paths[name].append(prim_path)
-                self.all_prim_paths.append(prim_path)
-
-        return self.sim_objects
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/config_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/config_manager.py
deleted file mode 100644
index e69de29bb2..0000000000
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager copy.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager copy.py
deleted file mode 100644
index 3b64cec6a4..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager copy.py	
+++ /dev/null
@@ -1,345 +0,0 @@
-import torch
-import math
-
-class Control_module:
-    def __init__(self, num_envs: int, device: str = 'cuda:0'):
-        """
-        Инициализирует модуль управления для векторизованной среды.
-
-        Args:
-            num_envs (int): Количество сред.
-            ratio (float): Масштабный коэффициент для преобразования координат.
-            device (str): Устройство для тензоров ('cuda:0' или 'cpu').
-        """
-        #print(f"[DEBUG] Initializing Control_module with:")
-        print(f"  - num_envs: {num_envs}")
-        print(f"  - device: {device}")
-        
-        self.num_envs = num_envs
-        self.device = device
-        max_path_length = 72
-        self.max_path_length = max_path_length
-        self.paths = torch.full((self.num_envs, max_path_length, 2), -666.0, device=self.device, dtype=torch.float32)  # [num_envs, max_path_length, 2]
-        self.current_pos = torch.zeros((num_envs, 2), device=device)
-        self.target_positions = torch.zeros((num_envs, 2), device=device)
-        self.end = torch.zeros(num_envs, dtype=torch.bool, device=device)
-        self.start = torch.ones(num_envs, dtype=torch.bool, device=device)
-        self.first_ep = torch.ones(num_envs, dtype=torch.bool, device=device)
-        self.lookahead_distance = 0.35
-        self.linear_velocity = 0.6
-        self.max_angular_velocity = 3
-        
-        #print(f"[DEBUG] Control parameters:")
-        # print(f"  - lookahead_distance: {self.lookahead_distance}")
-        # print(f"  - linear_velocity: {self.linear_velocity}")
-        # print(f"  - max_angular_velocity: {self.max_angular_velocity}")
-        #print(f"[DEBUG] Control_module initialization complete")
-
-    def update(self, current_positions: torch.Tensor, target_positions: torch.Tensor, paths: torch.Tensor, env_ids: torch.Tensor = None):
-        """
-        Обновляет текущие позиции, цели и пути для следования.
-
-        Args:
-            current_positions (torch.Tensor): Текущие позиции роботов [num_envs, 2].
-            target_positions (torch.Tensor): Целевые позиции [num_envs, 2].
-            paths (torch.Tensor): Пути для каждой среды [num_envs, max_path_length, 2].
-        """
-        # print(f"\n[DEBUG] ===== UPDATE METHOD =====")
-        #print(f"[DEBUG] Input shapes:")
-        # print(f"  - current_positions: {current_positions.shape}")
-        # print(f"  - target_positions: {target_positions.shape}")
-        # print(f"  - paths: {paths.shape}")
-        
-        #print(f"[DEBUG] Current positions sample: {current_positions[:3]}")
-        #print(f"[DEBUG] Target positions sample: {target_positions[:3]}")
-        #print(f"[DEBUG] Paths sample (first env, first 3 points): {paths[0, :3]}")
-        # Reset states
-        prev_end = self.end.clone()
-        prev_start = self.start.clone()
-        if env_ids is None:
-            print("env_ids is none in control.update")
-            self.paths = paths.clone()
-            self.current_pos = current_positions.clone()
-            self.target_positions = target_positions.clone()
-            self.end[:] = False
-            self.start[:] = True
-            self.first_ep[:] = True
-        else:
-            self.current_pos[env_ids] = current_positions.clone()
-            self.target_positions[env_ids] = target_positions.clone()
-            self.paths[env_ids] = paths.clone()
-            self.end[env_ids] = False
-            self.start[env_ids] = True
-            self.first_ep[env_ids] = True
-       
-        #print(f"[DEBUG] State reset:")
-        # print(f"  - end: {prev_end.sum().item()} -> {self.end.sum().item()} (True count)")
-        # print(f"  - start: {prev_start.sum().item()} -> {self.start.sum().item()} (True count)")
-        # print(f"  - first_ep: {self.first_ep.sum().item()} (True count)")
-
-    def normalize_angle(self, angle: torch.Tensor) -> torch.Tensor:
-        """
-        Нормализует углы в диапазон [-pi, pi].
-
-        Args:
-            angle (torch.Tensor): Углы [num_envs].
-
-        Returns:
-            torch.Tensor: Нормализованные углы [num_envs].
-        """
-        original_angle = angle.clone()
-        iterations = 0
-        while torch.any(angle > math.pi):
-            angle[angle > math.pi] -= 2 * math.pi
-            iterations += 1
-            if iterations > 100:  # Safety check
-                print(f"[DEBUwndbG] Warning: normalize_angle stuck in positive loop")
-                break
-        while torch.any(angle < -math.pi):
-            angle[angle < -math.pi] += 2 * math.pi
-            iterations += 1
-            if iterations > 100:  # Safety check
-                print(f"[DEBUG] Warning: normalize_angle stuck in negative loop")
-                break
-        # if iterations > 0:
-        #     #print(f"[DEBUG] Normalized angles in {iterations} iterations")
-        #     print(f"  - Original range: [{original_angle.min():.3f}, {original_angle.max():.3f}]")
-        #     print(f"  - Normalized range: [{angle.min():.3f}, {angle.max():.3f}]")
-        
-        return angle
-
-    def get_lookahead_point(self, current_positions: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:
-        """
-        Вычисляет точки следования (lookahead points) для всех сред.
-
-        Args:
-            current_positions (torch.Tensor): Текущие позиции роботов [num_envs, 2].
-            mask (torch.Tensor, optional): Маска активных сред.
-
-        Returns:
-            torch.Tensor: Точки следования [num_envs, 2].
-        """
-        # print(f"\n[DEBUG] ===== GET_LOOKAHEAD_POINT =====")
-        
-        lookahead_points = torch.zeros((self.num_envs, 2), device=self.device)
-        if mask is None:
-            mask = torch.ones(self.num_envs, dtype=torch.bool, device=self.device)
-        active_envs = torch.where(mask)[0]
-        
-        for i in active_envs:
-            found_lookahead = False
-            for j in range(self.max_path_length-1, -1, -1):  # Поиск с конца, max_path_length=50
-                segment_start = self.paths[i, j]
-                segment_end = self.paths[i, min(j + 1, self.max_path_length-1)]
-                # Проверка близости последней точки к цели
-                if j == self.max_path_length-1:
-                    # print("self.target_positions ", self.target_positions)
-                    dist_to_target = torch.norm(segment_start - self.target_positions[i])
-                    # if dist_to_target > 0.1:
-                    #     print(f"[DEBUG] Warning: Env {i} last path point {segment_start} is far from target {self.target_positions[i]}")
-                # Логика поиска lookahead point
-                dist_to_start = torch.norm(segment_start - current_positions[i])
-                if dist_to_start <= self.lookahead_distance:
-                    # print(f"[ WHERE MY GOOD LP ] dist_to_start: {dist_to_start}\n lookahead_points[{i}]: segment_start")
-                    lookahead_points[i] = segment_end
-                    found_lookahead = True
-                    break
-            if not found_lookahead:
-                # print(f"[DEBUG] Env {i}: No valid lookahead point, setting end=True")
-                # self.end[i] = True
-                lookahead_points[i] = self.target_positions[i]
-        
-        #print(f"[DEBUG] Lookahead points sample: {lookahead_points[active_envs][:3]}")
-        return lookahead_points
-
-
-    def get_quadrant(self, nx: torch.Tensor, ny: torch.Tensor, vector: torch.Tensor) -> torch.Tensor:
-        """
-        Определяет квадрант для вектора относительно осей nx, ny.
-
-        Args:
-            nx (torch.Tensor): Ось X [num_envs, 2].
-            ny (torch.Tensor): Ось Y [num_envs, 2].
-            vector (torch.Tensor): Вектор к цели [num_envs, 2].
-
-        Returns:
-            torch.Tensor: Знак квадранта [num_envs].
-        """
-        LR = vector[:, 0] * nx[:, 1] - vector[:, 1] * nx[:, 0]
-        signs = torch.sign(LR)
-        
-        #print(f"[DEBUG] get_quadrant:")
-        # print(f"  - LR values: {LR[:5]}")
-        # print(f"  - Signs: {signs[:5]}")
-        
-        return signs
-
-    def pure_pursuit_controller(self, current_positions: torch.Tensor, current_orientations: torch.Tensor):
-        """
-        Реализует Pure Pursuit контроллер для всех сред.
-
-        Args:
-            current_positions (torch.Tensor): Текущие позиции роботов [num_envs, 2].
-            current_orientations (torch.Tensor): Текущие ориентации (yaw) [num_envs].
-
-        Returns:
-            tuple: (linear_velocity [num_envs], angular_velocity [num_envs])
-        """
-        # print(f"\n[DEBUG] ===== PURE_PURSUIT_CONTROLLER =====")
-        #print(f"[DEBUG] Input shapes:")
-        # print(f"  - current_positions: {current_positions}")
-        # print(f"  - current_orientations: {current_orientations}")
-        
-        #print(f"[DEBUG] Current orientations sample: {current_orientations[:5]}")
-        
-        linear_velocity = torch.full((self.num_envs,), self.linear_velocity, device=self.device)
-        angular_velocity = torch.zeros(self.num_envs, device=self.device)
-        # print("current_orientations", current_orientations)
-        current_heading = torch.where(
-            current_orientations < 0,
-            2*math.pi + current_orientations,
-            current_orientations
-        )
-        distance_to_target = torch.norm(self.target_positions - current_positions, dim=1)
-        distance_to_path_end = torch.norm(self.paths[:, -1] - current_positions, dim=1)
-        # print(f"  - distance_to_target {distance_to_target}", self.target_positions)
-        # print(f"  - distance_to_path_end {distance_to_path_end}", self.paths[:, -1])
-        close_to_target = (distance_to_target < 1.0) | (distance_to_path_end < 0.3)
-        # print(f"  - close_to_target: {close_to_target}")
-        self.end = close_to_target
-        
-        mask_start_or_end = self.start | self.end
-        mask_normal = ~mask_start_or_end
-        
-        #print(f"[DEBUG] Environment states:")
-        # print(f"  - start: {self.start.sum().item()}")
-        # print(f"  - end: {self.end.sum().item()}")
-        # print(f"  - first_ep: {self.first_ep.sum().item()}")
-        # print(f"  - mask_normal: {mask_normal}")
-        # print(f"  - mask_start_or_end: {mask_start_or_end}")
-        # print(f"  - self.start: {self.start}")
-        # print(f"  - self.end: {self.end}")
-
-        # Normal following behavior
-        if torch.any(mask_normal):
-            #print(f"[DEBUG] Processing {mask_normal.sum().item()} environments with normal following")
-            
-            lookahead_points = self.get_lookahead_point(current_positions, mask_normal)
-            # print(f" [ CONTROL MODULE DEBUG ] lookahead points: {lookahead_points}")
-            to_target = lookahead_points - current_positions
-            #print(f"[DEBUG] lookahead_points is {lookahead_points} from current_positions {current_positions} and to target is {to_target}")
-            target_angle = torch.atan2(to_target[:, 1], to_target[:, 0])
-            target_angle = torch.where(target_angle < 0, target_angle + 2 * math.pi, target_angle)
-            alpha = self.normalize_angle(target_angle - current_heading)
-            
-            # #print(f"[DEBUG] Normal following calculations:")
-            # print(f"  - target_angle sample: {target_angle[mask_normal][:3]}")
-            # print(f"  - alpha sample: {alpha[mask_normal][:3]}")
-            
-            curvature = 2 * torch.sin(alpha) / self.lookahead_distance
-            angular_velocity[mask_normal] = curvature[mask_normal] * self.linear_velocity
-            
-            # #print(f"[DEBUG] Curvature sample: {curvature[mask_normal][:3]}")
-            # #print(f"[DEBUG] Angular velocity before clamp: {angular_velocity[mask_normal][:3]}")
-            
-            angular_velocity[mask_normal] = torch.clamp(
-                angular_velocity[mask_normal],
-                -self.max_angular_velocity,
-                self.max_angular_velocity
-            )
-            
-            # #print(f"[DEBUG] Angular velocity after clamp: {angular_velocity[mask_normal][:3]}")
-            
-            # Velocity scaling
-            velocity_scale = (self.max_angular_velocity - angular_velocity[mask_normal].abs()) / self.max_angular_velocity
-            linear_velocity[mask_normal] *= velocity_scale
-            
-            # #print(f"[DEBUG] Velocity scale: {velocity_scale[:3]}")
-            # #print(f"[DEBUG] Final linear velocity: {linear_velocity[mask_normal][:3]}")
-
-        # Start or end behavior
-        if torch.any(mask_start_or_end):
-            #print(f"[DEBUG]  {mask_start_or_end.sum().item()} environments with start/end behavior")
-            
-            if torch.any(self.first_ep):
-                #print(f"[DEBUG] First episode: stopping {self.first_ep.sum().item()} environments")
-                linear_velocity[self.first_ep] = 0.0
-                angular_velocity[self.first_ep] = 0.0
-                self.first_ep[:] = False
-            else:
-                #print(f"[DEBUG] Processing orientation adjustment for {mask_start_or_end.sum().item()} environments")
-                
-                nx = torch.tensor([[1.0, 0.0]], device=self.device).repeat(self.num_envs, 1)
-                ny = torch.tensor([[0.0, 1.0]], device=self.device).repeat(self.num_envs, 1)
-                # Находим первую валидную точку пути (не -666)
-                # valid_mask = ~torch.any(self.paths == -666.0, dim=2)  # [num_envs, 50]
-                # print("________________________check")
-                # print("paths: ", self.paths)
-                valid_mask = ~torch.any(torch.abs(self.paths) > 30, dim=2)
-                valid_indices = torch.argmax(valid_mask.to(torch.int32), dim=1)  # Индекс первой валидной точки
-                invalid_paths = ~torch.any(valid_mask, dim=1)  # Среды с полностью недействительными путями
-                # print("paths: ", invalid_paths)
-                # print("paths: ",  self.paths[invalid_paths])
-                # Выводим информацию о невалидных путях
-                # if torch.any(invalid_paths):
-                #     invalid_env_ids = invalid_paths.nonzero(as_tuple=False).squeeze(-1)
-                #     print(f"[DEBUG] Invalid paths detected in environments: {invalid_env_ids.tolist()}")
-                #     for env_id in invalid_env_ids:
-                #         print(f"[DEBUG] Path for env {env_id.item()}: {self.paths[env_id].tolist()}")
-                self.end |= invalid_paths
-                if torch.any(invalid_paths):
-                    print(f"[ CM DEBUG ] Environments with invalid paths: {invalid_paths.sum().item()}")
-                
-                # Собираем первые валидные точки
-                first_valid_points = torch.zeros((self.num_envs, 2), device=self.device, dtype=torch.float32)
-                for i in range(self.num_envs):
-                    if invalid_paths[i]:
-                        first_valid_points[i] = self.target_positions[i]  # Для пустых путей используем цель
-                    else:
-                        add = 0
-                        if len(self.paths[i]) > 1 and valid_indices[i] < self.max_path_length - 1:
-                            add = 1
-                        first_valid_points[i] = self.paths[i, valid_indices[i]+add]
-                # print(f"[ CM DEBUG ] valid_indices: {valid_indices}")
-                 # Вычисляем угол к целевой точке
-                to_goal_vec = torch.where(
-                    self.end[:, None],
-                    self.target_positions - current_positions,
-                    first_valid_points - current_positions
-                )
-                # print(f"[ CM DEBUG ] first_valid_points: {first_valid_points}, cur_position: {current_positions}", to_goal_vec)
-                target_angle = torch.atan2(to_goal_vec[:, 1], to_goal_vec[:, 0])
-                target_angle = torch.where(target_angle < 0, target_angle + 2 * math.pi, target_angle)
-                # current_heading = 2 * math.pi - current_heading
-                # current_heading = math.pi - current_heading
-                current_heading = torch.where(
-                    current_heading < 0,
-                    2 * math.pi + current_heading,
-                    current_heading
-                )
-                alpha = self.normalize_angle(target_angle - current_heading)
-                # print("current_heading ", current_heading)
-                # print("target_angle ", target_angle)
-                # print("alpha ", alpha)
-                # Устанавливаем угловую скорость для выравнивания
-                angular_velocity[mask_start_or_end] = torch.sign(alpha[mask_start_or_end]) * 1.2
-                linear_velocity[mask_start_or_end] = 0.0
-                
-                # Проверяем, достаточно ли выровнена ориентация
-                angle_error = torch.abs(alpha)
-                orientation_ok = angle_error < math.pi / 80
-                self.start &= ~orientation_ok
-                
-                #print(f"[DEBUG] Orientation adjustment:")
-                # print(f"  - angle_error sample: {angle_error[mask_start_or_end][:3]}")
-                # print(f"  - threshold: {math.pi / 80:.4f}")
-                # print(f"  - orientation_ok: {orientation_ok.sum().item()}")
-                # print(f"  - start count: {prev_start_count} -> {new_start_count}")
-
-        #print(f"[DEBUG] Final control outputs:")
-        # print(f"  - linear_velocity range: [{linear_velocity.min():.3f}, {linear_velocity.max():.3f}]")
-        # print(f"  - angular_velocity range: [{angular_velocity.min():.3f}, {angular_velocity.max():.3f}]")
-        # print(f"  - linear_velocity sample: {linear_velocity[:5]}")
-        # print(f"  - angular_velocity sample: {angular_velocity[:5]}")
-
-        return linear_velocity, angular_velocity
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager.py
deleted file mode 100644
index 7f9fb6de4e..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/control_manager.py
+++ /dev/null
@@ -1,221 +0,0 @@
-import torch
-import math
-
-class VectorizedPurePursuit:
-    def __init__(self, num_envs, device='cuda', max_path_length=15, lookahead_distance=0.35,
-                 base_linear_velocity=1.0, max_angular_velocity=1.8, arrival_threshold=0.2):
-        self.num_envs = num_envs
-        self.device = torch.device(device)
-        self.max_path_length = max_path_length
-        self.lookahead_distance = lookahead_distance
-        self.base_linear_velocity = float(base_linear_velocity)
-        self.max_angular_velocity = float(max_angular_velocity)
-        self.arrival_threshold = float(arrival_threshold)
-
-        # paths: (num_envs, max_path_length, 2) padded with NaN
-        self.paths = torch.full((num_envs, max_path_length, 2),
-                                float('nan'), dtype=torch.float32, device=self.device)
-        self.path_lengths = torch.zeros(num_envs, dtype=torch.int64, device=self.device)
-        self.finished = torch.ones(num_envs, dtype=torch.bool, device=self.device)
-        self.target_positions = torch.full((num_envs, 2), float('nan'),
-                                           dtype=torch.float32, device=self.device)
-
-        # new: прогресс по арк-длине (не падает назад)
-        self.progress_arclen = torch.zeros(num_envs, dtype=torch.float32, device=self.device)
-
-    def update_paths(self, env_indices, new_paths, target_positions):
-        if not isinstance(env_indices, torch.Tensor):
-            env_indices = torch.tensor(env_indices, dtype=torch.int64, device=self.device)
-
-        # set target positions (assume target_positions aligns with env_indices)
-        self.target_positions[env_indices] = torch.tensor(target_positions, dtype=torch.float32, device=self.device)
-
-        for i, env_id in enumerate(env_indices):
-            path = new_paths[i]
-            # accept numpy or torch
-            if not isinstance(path, torch.Tensor):
-                path = torch.tensor(path, dtype=torch.float32, device=self.device)
-            length = int(path.shape[0])
-            if length > self.max_path_length:
-                raise ValueError(f"Path length {length} exceeds max_path_length {self.max_path_length}")
-            self.paths[env_id, :length] = path
-            # pad remainder with NaN
-            if length < self.max_path_length:
-                self.paths[env_id, length:] = float('nan')
-            self.path_lengths[env_id] = length
-
-            # reset progress counter for this env
-            self.progress_arclen[env_id] = 0.0
-
-        # mark these envs as not finished (start following)
-        self.finished[env_indices] = False
-
-    def compute_controls(self, positions, orientations):
-        linear_vels = torch.zeros(self.num_envs, dtype=torch.float32, device=self.device)
-        angular_vels = torch.zeros(self.num_envs, dtype=torch.float32, device=self.device)
-
-        # active: есть путь >=2 и не finished
-        active = (self.path_lengths >= 2) & (~self.finished)
-        if not active.any():
-            # Нет активных движений — обрабатываем только выравнивание для пришедших (finished)
-            # Но только для тех, у кого есть валидная target_positions
-            # print("chachacha")
-            valid_target_mask = self.finished & torch.isfinite(self.target_positions).all(dim=1)
-            jf_indices = torch.where(valid_target_mask)[0]
-            if jf_indices.numel() == 0:
-                return linear_vels, angular_vels
-
-            pos_jf = positions[jf_indices]
-            ori_jf = orientations[jf_indices]
-            target_pos_jf = self.target_positions[jf_indices]
-
-            to_targets_jf = target_pos_jf - pos_jf
-            target_angles_jf = torch.atan2(to_targets_jf[:, 1], to_targets_jf[:, 0])
-            alphas_jf = target_angles_jf - ori_jf
-            alphas_jf = (alphas_jf + math.pi) % (2 * math.pi) - math.pi
-            signs = torch.sign(alphas_jf)
-            signs[signs == 0] = 1
-            
-            ang_vels_jf = torch.clamp(signs * 2.0, -self.max_angular_velocity, self.max_angular_velocity)
-            # P-controller for orientation
-            # kp = 2.0
-            # ang_vels_jf = torch.clamp(kp * alphas_jf, -self.max_angular_velocity, self.max_angular_velocity)
-
-            linear_vels[jf_indices] = 0.0
-            angular_vels[jf_indices] = ang_vels_jf
-
-            return linear_vels, angular_vels
-
-        # subset active
-        active_indices = torch.where(active)[0]
-        num_active = active_indices.shape[0]
-        pos = positions[active_indices]  # (num_active, 2)
-        ori = orientations[active_indices]  # (num_active,)
-        paths_active = self.paths[active_indices]  # (num_active, max_path_length, 2)
-        path_lens = self.path_lengths[active_indices]  # (num_active,)
-
-        max_segments = self.max_path_length - 1
-        segment_starts = paths_active[:, :-1, :]  # (num_active, max_segments, 2)
-        segment_ends = paths_active[:, 1:, :]     # (num_active, max_segments, 2)
-        segment_vecs = segment_ends - segment_starts
-        segment_lengths = torch.norm(segment_vecs, dim=-1)  # (num_active, max_segments)
-
-        # mask valid segments by path length and by non-zero length
-        seg_index = torch.arange(max_segments, device=self.device).unsqueeze(0).expand(num_active, max_segments)
-        segment_mask = seg_index < (path_lens - 1).unsqueeze(1)
-        # also ignore extremely short segments (to avoid div by ~0)
-        segment_mask = segment_mask & (segment_lengths > 1e-6)
-
-        # compute projections safely
-        pos_exp = pos.unsqueeze(1)  # (num_active, 1, 2)
-        to_starts = pos_exp - segment_starts
-        denom = (segment_lengths ** 2) + 1e-8
-        projs = torch.sum(to_starts * segment_vecs, dim=-1) / denom
-        projs_clamped = torch.clamp(projs, min=0.0, max=1.0)
-        closest_points = segment_starts + segment_vecs * projs_clamped.unsqueeze(-1)
-        dists = torch.norm(pos_exp - closest_points, dim=-1)
-
-        # invalidate non-valid segments
-        dists[~segment_mask] = float('inf')
-
-        # choose closest segment per env
-        min_dists, min_segments = torch.min(dists, dim=1)  # (num_active,)
-        min_projs = projs_clamped[torch.arange(num_active, device=self.device), min_segments]
-        min_seg_lengths = segment_lengths[torch.arange(num_active, device=self.device), min_segments]
-
-        # cumulative arclengths along path (start-of-segment positions)
-        padded_segment_lengths = segment_lengths.clone()
-        padded_segment_lengths[~segment_mask] = 0.0
-        cum_lengths = torch.cat([torch.zeros(num_active, 1, device=self.device), padded_segment_lengths], dim=1)  # (num_active, max_segments+1)
-        cum_lengths = torch.cumsum(cum_lengths, dim=1)
-
-        cum_at_min_seg = cum_lengths[torch.arange(num_active, device=self.device), min_segments]
-        closest_arclen = cum_at_min_seg + min_projs * min_seg_lengths  # arclength to the closest point on path
-
-        # **Главная правка**: не позволяем прогрессу уменьшаться (чтобы не идти назад)
-        prev_progress = self.progress_arclen[active_indices]
-        new_progress = torch.maximum(prev_progress, closest_arclen)
-        self.progress_arclen[active_indices] = new_progress
-        curr_arclen = new_progress
-
-        # цель по арк-длине
-        target_arclen = curr_arclen + self.lookahead_distance
-
-        # полная длина пути:
-        total_lengths = cum_lengths[torch.arange(num_active, device=self.device), (path_lens - 1).clamp(max=max_segments)]
-
-        is_beyond = target_arclen >= total_lengths
-
-        lookahead_points = torch.zeros(num_active, 2, dtype=torch.float32, device=self.device)
-        last_point_indices = (path_lens - 1).clamp(max=self.max_path_length - 1)
-        last_points = paths_active[torch.arange(num_active, device=self.device), last_point_indices]
-        lookahead_points[is_beyond] = last_points[is_beyond]
-
-        not_beyond = ~is_beyond
-        if not_beyond.any():
-            nb_idx = torch.where(not_beyond)[0]
-            target_arclen_nb = target_arclen[not_beyond]
-            cum_lengths_nb = cum_lengths[not_beyond]  # (num_nb, max_segments+1)
-            # find segment that contains target_arclen
-            segs_nb = torch.searchsorted(cum_lengths_nb, target_arclen_nb.unsqueeze(1), right=False).squeeze(1) - 1
-            segs_nb = torch.clamp(segs_nb, min=0, max=max_segments - 1)
-            cum_at_seg_nb = cum_lengths_nb[torch.arange(segs_nb.shape[0], device=self.device), segs_nb]
-            seg_lengths_nb = padded_segment_lengths[not_beyond, segs_nb]
-            fracs_nb = (target_arclen_nb - cum_at_seg_nb) / (seg_lengths_nb + 1e-8)
-            fracs_nb = torch.clamp(fracs_nb, min=0.0, max=1.0)
-            starts_nb = segment_starts[not_beyond, segs_nb]
-            vecs_nb = segment_vecs[not_beyond, segs_nb]
-            lookahead_points[not_beyond] = starts_nb + vecs_nb * fracs_nb.unsqueeze(-1)
-
-        # управление
-        to_targets = lookahead_points - pos
-        target_angles = torch.atan2(to_targets[:, 1], to_targets[:, 0])
-        alphas = target_angles - ori
-        alphas = (alphas + math.pi) % (2 * math.pi) - math.pi
-        curvatures = 2 * torch.sin(alphas) / (self.lookahead_distance + 1e-8)
-        ang_vels_active = curvatures * self.base_linear_velocity
-        ang_vels_active = torch.clamp(ang_vels_active, -self.max_angular_velocity, self.max_angular_velocity)
-        lin_vels_active = self.base_linear_velocity * (1 - torch.abs(ang_vels_active) / (self.max_angular_velocity + 1e-8))
-        lin_vels_active = torch.clamp(lin_vels_active, min=0.0)
-
-        # пометим те активные среды, которые достигли конца пути
-        dists_to_end = torch.norm(pos - last_points, dim=1)
-        finished_active = dists_to_end < self.arrival_threshold
-        if finished_active.any():
-            # объявляем их finished; дальше они будут обрабатываться в блоке finished
-            self.finished[active_indices[finished_active]] = True
-            lin_vels_active[finished_active] = 0.0
-            ang_vels_active[finished_active] = 0.0
-
-        # записываем результаты в глобальные векторы
-        linear_vels[active_indices] = lin_vels_active
-        angular_vels[active_indices] = ang_vels_active
-
-        # блок выравнивания ориентации для всех finished (и имеющих валидные target_positions)
-        valid_target_mask = self.finished & torch.isfinite(self.target_positions).all(dim=1)
-        jf_indices = torch.where(valid_target_mask)[0]
-        if jf_indices.numel() > 0:
-            pos_jf = positions[jf_indices]
-            ori_jf = orientations[jf_indices]
-            target_pos_jf = self.target_positions[jf_indices]
-            to_targets_jf = target_pos_jf - pos_jf
-            target_angles_jf = torch.atan2(to_targets_jf[:, 1], to_targets_jf[:, 0])
-            alphas_jf = target_angles_jf - ori_jf
-            alphas_jf = (alphas_jf + math.pi) % (2 * math.pi) - math.pi
-            kp = 2.0
-            # ang_vels_jf = torch.clamp(kp * alphas_jf, -self.max_angular_velocity, self.max_angular_velocity)
-            signs = torch.sign(alphas_jf)
-            signs[signs == 0] = 1
-            
-            ang_vels_jf = torch.clamp(signs * 2.0, -self.max_angular_velocity, self.max_angular_velocity)
-            linear_vels[jf_indices] = 0.0
-            angular_vels[jf_indices] = ang_vels_jf
-
-            # если уже выровнялись — обнулим угловую скорость
-            aligned_mask = torch.abs(alphas_jf) < 0.1
-            if aligned_mask.any():
-                aligned_idxs = jf_indices[aligned_mask]
-                angular_vels[aligned_idxs] = 0.0
-                # (оставляем self.finished=True — внешняя логика может считать эту среду "done")
-
-        return linear_vels, angular_vels
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/custom_runner.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/custom_runner.py
deleted file mode 100644
index e99f6bfe21..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/custom_runner.py
+++ /dev/null
@@ -1,53 +0,0 @@
-from rl_games.common import env_configurations, experiment
-from rl_games.torch_runner import Runner
-from torch.utils.tensorboard import SummaryWriter
-import numpy as np
-import os
-
-class TensorBoardRunner(Runner):
-    def __init__(self):
-        super().__init__()
-        self.writer = None
-
-    def reset(self):
-        super().reset()
-        # Инициализация TensorBoard, если включено
-        if self.params['config'].get('enable_tensorboard', False):
-            log_dir = self.params['config']['log_dir']
-            os.makedirs(log_dir, exist_ok=True)
-            self.writer = SummaryWriter(log_dir=log_dir)
-
-    def run(self, args):
-        super().run(args)
-        # Закрываем writer после завершения обучения
-        if self.writer is not None:
-            self.writer.close()
-
-    def log_metrics(self, metrics, step):
-        # Логирование метрик в TensorBoard
-        if self.writer is not None:
-            for key, value in metrics.items():
-                if isinstance(value, (int, float, np.floating, np.integer)):
-                    self.writer.add_scalar(key, value, step)
-                elif isinstance(value, (list, np.ndarray)):
-                    # Логируем среднее значение для массивов
-                    self.writer.add_scalar(key, np.mean(value), step)
-
-    def play_one_epoch(self):
-        # Переопределяем метод для добавления логирования
-        metrics = super().play_one_epoch()
-        if self.writer is not None:
-            # Логируем стандартные метрики rl_games
-            self.log_metrics({
-                'train/loss': metrics.get('loss', 0.0),
-                'train/reward': np.mean(metrics.get('rewards', 0.0)),
-                'train/episode_reward': np.mean(metrics.get('episode_rewards', 0.0))
-            }, self.global_step)
-            # Логируем пользовательские метрики из среды
-            if 'infos' in metrics:
-                for info in metrics['infos']:
-                    if 'custom_metrics' in info:
-                        self.log_metrics({
-                            f'custom/{key}': value for key, value in info['custom_metrics'].items()
-                        }, self.global_step)
-        return metrics
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/eval_scenes.json b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/eval_scenes.json
deleted file mode 100644
index c5b5314de2..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/eval_scenes.json
+++ /dev/null
@@ -1,9 +0,0 @@
-[
-    {"table": [[-4.5, 0.0, 0.0]], "chair": [[-2.2, 0.0, 0.0]], "cabinet": [[-0.5, -3.2, 0.0]], "bowl": [[-4.5, 0.0, 0.75]]},
-    {"table": [[-4.5, 0.0, 0.0]], "chair": [[-2.2, 1.1, 0.0]], "cabinet": [[-0.5, -3.2, 0.0]], "bowl": [[-4.5, 0.0, 0.75]]},
-    {"table": [[-4.5, -1.1, 0.0],[-4.5, 0.0, 0.0], [-4.5, 1.1, 0.0]], "chair": [[-2.2, 0.0, 0.0],[-2.2, 1.1, 0.0]], "cabinet": [[-0.5, -3.2, 0.0]], "bowl": [[-4.5, 0.0, 0.75]]},
-    {"table": [[-4.5, -1.1, 0.0],[-4.5, 0.0, 0.0],[-4.5, 1.1, 0.0]], "chair": [[-2.2, -1.1, 0.0],[-2.2, 1.1, 0.0]], "cabinet": [[-0.5, 3.2, 0.0]], "bowl": [[-4.5, -1.1, 0.75]]},
-    {"table": [[-4.5, -1.1, 0.0],[-4.5, 1.1, 0.0]], "chair": [[-2.2, -1.1, 0.0],[-2.2, 0.0, 0.0]], "cabinet": [[-0.5, -3.2, 0.0], [-3.5, -3.2, 0.0]], "bowl": [[-4.5, -1.1, 0.75]]},
-    {"table": [[-4.5, 0.0, 0.0],[-4.5, 1.1, 0.0]], "chair": [[-2.2, -1.1, 0.0],[-2.2, 0.0, 0.0],[-2.2, 1.1, 0.0]], "cabinet": [[-0.5, 3.2, 0.0], [-3.5, 3.2, 0.0]], "bowl": [[-4.5, 0.0, 0.75]]},
-    {"table": [[-4.5, -1.1, 0.0],[-4.5, 0.0, 0.0]], "chair": [[-2.2, -1.1, 0.0],[-2.2, 0.0, 0.0],[-2.2, 1.1, 0.0]], "cabinet": [[-0.5, -3.2, 0.0], [-0.5, 3.2, 0.0]], "bowl": [[-4.5, 0.0, 0.75]]}
-]
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/evaluation_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/evaluation_manager.py
deleted file mode 100644
index 46e28c5734..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/evaluation_manager.py
+++ /dev/null
@@ -1,152 +0,0 @@
-# evaluation_manager.py
-import torch
-import pandas as pd
-import os
-import json
-
-class EvaluationManager:
-    def __init__(self, num_envs, device="cuda"):
-        self.num_envs = num_envs
-        self.device = device
-
-        # позиции/углы (списки задач)
-        self.robot_positions = []   # list of [x,y,(z)]
-        self.angle_errors = []      # list (or list of lists) depending on формат
-
-        # указатели задач по env
-        self.position_idx = torch.zeros(num_envs, dtype=torch.long, device=device)
-        self.angle_idx = torch.zeros(num_envs, dtype=torch.long, device=device)
-
-        # сохранённые стартовые дистанции (на ресете)
-        self.start_dists = torch.full((num_envs,), float("nan"), device=device, dtype=torch.float32)
-
-        # результаты по env: список словарей
-        self.results = {env: [] for env in range(num_envs)}
-
-        # finished flags
-        self.finished_envs = torch.zeros(num_envs, dtype=torch.bool, device=device)
-        self.all_done = False
-
-    def set_task_lists(self, robot_positions, angle_errors):
-        """Одинаковые списки для всех env (по желанию можно сделать per-env)."""
-        self.robot_positions = robot_positions
-        self.angle_errors = angle_errors
-        self.position_idx[:] = 0
-        self.angle_idx[:] = 0
-        self.finished_envs[:] = False
-        self.all_done = False
-        self.start_dists[:] = float("nan")
-        # очистим результаты
-        self.results = {env: [] for env in range(self.num_envs)}
-
-    def set_start_dists(self, env_ids: torch.Tensor, start_dists: torch.Tensor):
-        env_ids = env_ids.to(self.device)
-        start_dists = start_dists.to(self.device).to(dtype=torch.float32)
-        self.start_dists[env_ids] = start_dists
-
-    def get_start_dists(self, env_ids: torch.Tensor):
-        env_ids = env_ids.to(self.device)
-        return self.start_dists[env_ids]
-    
-    def get_positions(self):
-        current_dir = os.getcwd()
-        filepath = os.path.join(current_dir, "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/eval_scenes.json")
-        with open(filepath, 'r', encoding='utf-8') as f:
-            positions = json.load(f)
-        return positions
-
-    def get_current_tasks(self, env_ids: torch.Tensor):
-        """
-        Вернёт пачку (robot_pos_batch, angle_err_batch) для env_ids.
-        Ожидается, что robot_positions и angle_errors — списки, одинаковые для всех env.
-        """
-        num_envs = len(env_ids)
-        robot_pos_batch = []
-        angle_batch = []
-        for env in env_ids.tolist():
-            pidx = int(self.position_idx[env].item())
-            aidx = int(self.angle_idx[env].item())
-            # clamp
-            pidx = min(pidx, max(0, len(self.robot_positions)-1))
-            aidx = min(aidx, max(0, len(self.angle_errors)-1))
-            robot_pos_batch.append(self.robot_positions[pidx])
-            angle_batch.append(self.angle_errors[aidx])
-        # возвращаем тензоры на нужном устройстве
-        final_yaw = torch.tensor(angle_batch, device=self.device)
-        robot_quats = torch.zeros(num_envs, 4, device=self.device)
-        robot_quats[:, 0] = torch.cos(final_yaw / 2.0)
-        robot_quats[:, 3] = torch.sin(final_yaw / 2.0)
-        return torch.tensor(robot_pos_batch, device=self.device), robot_quats
-
-    def log_results(self, env_ids: torch.Tensor, successes, traj_lens, start_dists, durations):
-        """
-        Логируем пачку результатов. `start_dists`/`durations`/`traj_lens` - батчи длины len(env_ids).
-        Важно: вызывать ДО next_episode.
-        """
-        env_ids = env_ids.to(self.device)
-        # allow tensor or list for other inputs
-        successes = torch.as_tensor(successes, device=self.device).to(dtype=torch.float32)
-        traj_lens = torch.as_tensor(traj_lens, device=self.device).to(dtype=torch.float32)
-        durations = torch.as_tensor(durations, device=self.device).to(dtype=torch.float32)
-        start_dists = torch.as_tensor(start_dists, device=self.device).to(dtype=torch.float32)
-
-        for i, env in enumerate(env_ids.tolist()):
-            env = int(env)
-            pos_i = int(self.position_idx[env].item())
-            ang_i = int(self.angle_idx[env].item())
-            s = float(successes[i].item())
-            tl = float(traj_lens[i].item())
-            sd = float(start_dists[i].item()) if not torch.isnan(start_dists[i]) else float(self.start_dists[env].item())
-            dur = float(durations[i].item())
-
-            spl = 0.0
-            if s > 0.5 and tl > 1e-6 and sd > 1e-6:
-                spl = (sd / tl)
-            # сохраняем запись
-            self.results[env].append({
-                "position_idx": pos_i,
-                "angle_idx": ang_i,
-                "success": s,
-                "traj_len": tl,
-                "start_dist": sd,
-                "duration": dur,
-                "spl": spl,
-            })
-
-    def next_episode(self, env_ids: torch.Tensor):
-        """Продвигаем указатели задач для пачки env (после логирования)."""
-        env_ids = env_ids.to(self.device)
-        for env in env_ids.tolist():
-            env = int(env)
-            if self.finished_envs[env]:
-                continue
-            # продвигаем angle
-            if (self.angle_idx[env] + 1) < len(self.angle_errors):
-                self.angle_idx[env] += 1
-            else:
-                self.angle_idx[env] = 0
-                # продвигаем позицию
-                if (self.position_idx[env] + 1) < len(self.robot_positions):
-                    self.position_idx[env] += 1
-                else:
-                    self.finished_envs[env] = True
-        if torch.all(self.finished_envs):
-            self.all_done = True
-
-    def is_all_done(self):
-        return bool(self.all_done)
-
-    def summarize(self):
-        all_rows = []
-        for env, recs in self.results.items():
-            for r in recs:
-                row = {"env": env}
-                row.update(r)
-                all_rows.append(row)
-        df = pd.DataFrame(all_rows)
-        if df.empty:
-            return df, pd.Series(), pd.DataFrame()
-        df = df.round(2)
-        global_stats = df.mean(numeric_only=True)
-        pos_stats = df.groupby("position_idx").mean(numeric_only=True)
-        return df, global_stats, pos_stats
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py
deleted file mode 100644
index fd32d61710..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py
+++ /dev/null
@@ -1,149 +0,0 @@
-import torch
-import networkx as nx
-import math
-from tabulate import tabulate
-
-class ObstacleGraph:
-    def __init__(self, objects_config: list, device='cuda:0'):
-        self.device = device
-        self.graph = nx.Graph()
-        self.objects_config = objects_config
-        self.num_nodes = sum(obj['count'] for obj in self.objects_config)
-        self._initialize_nodes()
-
-    def _initialize_nodes(self):
-        # Атрибуты узлов
-        self.positions = torch.zeros(self.num_nodes, 3, device=self.device)
-        self.radii = torch.zeros(self.num_nodes, device=self.device)
-        self.sizes = torch.zeros(self.num_nodes, 3, device=self.device)
-        self.active = torch.zeros(self.num_nodes, dtype=torch.bool, device=self.device)
-        
-        # Иерархия поверхностей
-        self.on_surface_idx = torch.full((self.num_nodes,), -1, dtype=torch.long, device=self.device)  # -1 означает пол
-        self.surface_level = torch.zeros(self.num_nodes, dtype=torch.long, device=self.device)  # 0 означает пол
-        
-        # Параметры сетки
-        max_per_row = 4  # Максимум объектов в строке
-        spacing = 1.0    # Расстояние между объектами (1 метр)
-        start_x = 3.0    # Начало сетки по X (за пределами x_max=2.0)
-        start_y = 4.0    # Начало сетки по Y (за пределами y_max=3.0)
-        
-        # Рассчитываем количество строк
-        num_rows = (self.num_nodes + max_per_row - 1) // max_per_row
-        
-        # Инициализация позиций в сетке
-        node_idx = 0
-        for row in range(num_rows):
-            for col in range(min(max_per_row, self.num_nodes - row * max_per_row)):
-                self.positions[node_idx, 0] = start_x + col * spacing
-                self.positions[node_idx, 1] = start_y + row * spacing
-                self.positions[node_idx, 2] = 0.0  # На полу
-                node_idx += 1
-        
-        node_idx = 0
-        for obj_cfg in self.objects_config:
-            count = obj_cfg['count']
-            size_tensor = torch.tensor(obj_cfg['size'], device=self.device)
-            # Автоматический расчет радиуса
-            radius = math.sqrt((size_tensor[0] / 2)**2 + (size_tensor[1] / 2)**2)
-
-            for i in range(count):
-                self.sizes[node_idx] = size_tensor
-                self.radii[node_idx] = radius
-                
-                # Инициализация узла в графе NetworkX
-                self.graph.add_node(
-                    node_idx,
-                    name=obj_cfg['name'],
-                    types=set(obj_cfg['type']),
-                    size=tuple(size_tensor.tolist()),
-                    radius=radius,
-                    active=False,
-                    on_surface_idx=-1,
-                    surface_level=0
-                )
-                node_idx += 1
-        
-        self.default_positions = self.positions.clone()
-        self.update_graph_state()
-
-    def get_nodes_by_type(self, type_str: str, only_active: bool = False) -> list[int]:
-        """Возвращает список индексов узлов с указанным типом."""
-        nodes = []
-        for i, data in self.graph.nodes(data=True):
-            if type_str in data['types']:
-                if not only_active or data['active']:
-                    nodes.append(i)
-        return nodes
-
-    def get_objects_on_surface(self, surface_node_idx: int) -> list[int]:
-        """Возвращает объекты, находящиеся на указанной поверхности."""
-        return [
-            i for i, idx in enumerate(self.on_surface_idx) if idx == surface_node_idx
-        ]
-
-    def update_graph_state(self):
-        """Синхронизирует данные из тензоров в атрибуты узлов графа NetworkX."""
-        for i in range(self.num_nodes):
-            node_data = self.graph.nodes[i]
-            node_data['position'] = tuple(self.positions[i].tolist())
-            node_data['active'] = self.active[i].item()
-            node_data['on_surface_idx'] = self.on_surface_idx[i].item()
-            node_data['surface_level'] = self.surface_level[i].item()
-
-    def print_graph_info(self, add_info=None):
-        """Prints detailed and formatted information about the ObstacleGraph."""
-        print("\n=== ObstacleGraph Information ===")
-        
-        # Graph Summary
-        active_nodes = self.active.sum().item()
-        print("\nGraph Summary:")
-        if add_info is not None:
-            print(add_info)
-        print(f"  Total Nodes: {self.num_nodes}")
-        print(f"  Active Nodes: {active_nodes}")
-        print(f"  Device: {self.device}")
-        
-        # Object Type Counts
-        type_counts = {}
-        for obj_cfg in self.objects_config:
-            type_counts[obj_cfg['name']] = obj_cfg['count']
-        print("\nObject Types and Counts:")
-        for name, count in type_counts.items():
-            print(f"  {name.capitalize()}: {count}")
-        
-        # Node Details Table
-        table_data = []
-        for i, data in self.graph.nodes(data=True):
-            pos = data['position']
-            types = ", ".join(data['types'])
-            size = data['size']
-            row = [
-                i,
-                data['name'],
-                types,
-                f"({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})",
-                f"{data['radius']:.2f}",
-                f"({size[0]:.2f}, {size[1]:.2f}, {size[2]:.2f})",
-                str(data['active']),
-                data['on_surface_idx'],
-                data['surface_level']
-            ]
-            table_data.append(row)
-        
-        headers = ["ID", "Name", "Types", "Position", "Radius", "Size", "Active", "On Surface", "Surface Level"]
-        print("\nNode Details:")
-        print(tabulate(table_data, headers=headers, tablefmt="grid"))
-        
-        # Surface Hierarchy
-        print("\nSurface Hierarchy:")
-        print(self.graph.nodes)
-        for i in range(self.num_nodes):
-            if self.on_surface_idx[i] != -1:
-                obj_name = self.graph.nodes[i]['name']
-                print(self.graph.nodes[i]['name'], self.graph.nodes[i], i)
-                print(self.on_surface_idx)
-                surface_name = self.graph.nodes[self.on_surface_idx[i].item()]['name']
-                print(f"  Node {i} ({obj_name}) is on surface Node {self.on_surface_idx[i]} ({surface_name})")
-            elif self.graph.nodes[i]['active']:
-                print(f"  Node {i} ({self.graph.nodes[i]['name']}) is on floor")
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/memory_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/memory_manager.py
deleted file mode 100644
index 43e0b47ab9..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/memory_manager.py
+++ /dev/null
@@ -1,188 +0,0 @@
-import torch
-
-class Memory_manager:
-    def __init__(self, num_envs: int, embedding_size: int, action_size: int, history_length: int, device: torch.device):
-        """
-        Инициализация менеджера памяти для хранения эмбеддингов и действий как стека.
-
-        Args:
-            num_envs (int): Количество сред.
-            embedding_size (int): Размер эмбеддинга (например, 512 для ResNet18).
-            action_size (int): Размер действия (например, 2 для линейной и угловой скорости).
-            history_length (int): Длина стека (n).
-            device (torch.device): Устройство для хранения тензоров (CPU/GPU).
-        """
-        self.num_envs = num_envs
-        self.embedding_size = embedding_size
-        self.action_size = action_size
-        self.history_length = history_length
-        self.device = device
-                
-        # Создаем индексы для выборки: [0, k, 2k, ..., (m-1)*k]
-        m = 4
-        k = 4
-        indices = torch.arange(0, m * k, k, device="cpu", dtype=torch.long)  # [m]
-        # Ограничиваем индексы длиной стека
-        self.indices = torch.clamp(indices, 0, self.history_length - 1)
-
-        # Инициализация стека для эмбеддингов: [num_envs, history_length, embedding_size]
-        self.embedding_history = torch.zeros((num_envs, history_length, embedding_size), device="cpu")
-        self.action_history = torch.zeros((num_envs, history_length, action_size), device="cpu")
-        # Флаг, указывающий, что стек еще не заполнен (для первой инициализации)
-        self.is_initialized = False
-
-    def update(self, embeddings: torch.Tensor, actions: torch.Tensor):
-        """
-        Обновление стека: новый эмбеддинг и действие добавляются в начало, старые сдвигаются вправо.
-
-        Args:
-            embeddings (torch.Tensor): Эмбеддинги текущего шага, форма [num_envs, embedding_size].
-            actions (torch.Tensor): Действия текущего шага, форма [num_envs, action_size].
-        """
-        # Если стек не инициализирован, заполняем его первым эмбеддингом и действием
-        if not self.is_initialized:
-            self.embedding_history = embeddings.cpu().unsqueeze(1).expand(-1, self.history_length, -1)
-            self.is_initialized = True
-        # Сдвигаем историю вправо (удаляем самый старый элемент)
-        # Создаем новую историю, объединяя новый эмбеддинг/действие с частью старой истории
-        self.embedding_history = torch.cat([embeddings.cpu().unsqueeze(1), self.embedding_history[:, :-1]], dim=1)
-        self.action_history = torch.cat([actions.cpu().unsqueeze(1), self.action_history[:, :-1]], dim=1)
-
-    def get_observations(self, m=4, k=4) -> torch.Tensor:
-        """
-        Получение m эмбеддингов и m действий с частотой k, начиная с последнего (индекс 0).
-
-        Args:
-            m (int): Количество возвращаемых эмбеддингов и действий.
-            k (int): Шаг выборки (частота).
-
-        Returns:
-            torch.Tensor: Тензор формы [num_envs, m * (embedding_size + action_size)] с m эмбеддингами и действиями.
-        """
-        # Выбираем эмбеддинги и действия для всех сред одновременно
-        selected_embeddings = self.embedding_history[:, self.indices].to(self.device)  # [num_envs, m, embedding_size]
-        selected_actions = self.action_history[:, self.indices].to(self.device)  # [num_envs, m, action_size]
-
-        # Объединяем эмбеддинги и действия по последней размерности
-        combined = torch.cat([selected_embeddings, selected_actions], dim=-1)  # [num_envs, m, embedding_size + action_size]
-
-        # Разворачиваем в плоский вектор
-        output = combined.view(self.num_envs, -1).to(self.device)  # [num_envs, m * (embedding_size + action_size)]
-
-        return output
-
-    def reset(self):
-        """
-        Сброс стека для указанных сред или всех сред.
-        """
-        self.is_initialized = False
-
-class PathTracker:
-    def __init__(self, num_envs: int, T_max: int = 256, device: str = "cuda", pos_dim: int = 2):
-        """
-        Батчевый менеджер траекторий и управляющих воздействий.
-
-        Args:
-            num_envs (int): количество сред
-            T_max (int): максимальная длина траектории
-            device (str): устройство
-            pos_dim (int): размерность позиции (обычно 2 или 3)
-        """
-        self.num_envs = num_envs
-        self.T_max = T_max
-        self.device = device
-        self.pos_dim = pos_dim
-
-        # [num_envs, T_max, pos_dim]
-        self.positions = torch.zeros((num_envs, T_max, pos_dim), device=device, dtype=torch.float32)
-        # [num_envs, T_max, 2] (lin, ang)
-        self.velocities = torch.zeros((num_envs, T_max, 2), device=device, dtype=torch.float32)
-        # Счётчик длины траектории для каждой среды
-        self.lengths = torch.zeros(num_envs, device=device, dtype=torch.long)
-
-    @torch.no_grad()
-    def add_step(self, env_ids: torch.Tensor, positions: torch.Tensor, velocities: torch.Tensor):
-        """
-        Добавить позиции и управляющие воздействия в батчевом режиме.
-        Args:
-            env_ids (torch.Tensor): [K]
-            positions (torch.Tensor): [K, pos_dim]
-            velocities (torch.Tensor): [K, 2]
-        """
-        env_ids = env_ids.to(self.device)
-        idxs = self.lengths[env_ids]  # текущие индексы вставки
-        for i, env_id in enumerate(env_ids):
-            if idxs[i] < self.T_max:
-                self.positions[env_id, idxs[i]] = positions[i].to(self.device)
-                self.velocities[env_id, idxs[i]] = velocities[i].to(self.device)
-                self.lengths[env_id] += 1  # увеличиваем счётчик
-
-    def reset(self, env_ids: torch.Tensor):
-        """
-        Очистить траектории и управляющие воздействия для указанных сред.
-        """
-        env_ids = env_ids.to(self.device)
-        self.positions[env_ids] = 0.0
-        self.velocities[env_ids] = 0.0
-        self.lengths[env_ids] = 0
-
-    @torch.no_grad()
-    def compute_path_lengths(self, env_ids: torch.Tensor) -> torch.Tensor:
-        """
-        Подсчитать длину пути для агентов (евклидова сумма).
-        Returns: [K]
-        """
-        env_ids = env_ids.to(self.device)
-        pos = self.positions[env_ids]  # [K, T_max, pos_dim]
-        L = self.lengths[env_ids]      # [K]
-
-        # Считаем диффы вдоль оси T
-        diffs = pos[:, 1:] - pos[:, :-1]       # [K, T_max-1, pos_dim]
-        dist = torch.norm(diffs, dim=-1)       # [K, T_max-1]
-
-        # Маска по длине
-        mask = torch.arange(self.T_max-1, device=self.device).unsqueeze(0) < (L.unsqueeze(1)-1)
-        dist = dist * mask
-
-        return dist.sum(dim=1)
-
-    def get_paths(self, env_ids: torch.Tensor):
-        """
-        Вернуть пути агентов (с обрезкой до длины).
-        """
-        env_ids = env_ids.to(self.device)
-        out = {}
-        for i, env_id in enumerate(env_ids):
-            L = self.lengths[env_id].item()
-            out[env_id.item()] = self.positions[env_id, :L]
-        return out
-
-    def get_velocities(self, env_ids: torch.Tensor):
-        """
-        Вернуть последовательности управляющих воздействий.
-        """
-        env_ids = env_ids.to(self.device)
-        out = {}
-        for i, env_id in enumerate(env_ids):
-            L = self.lengths[env_id].item()
-            out[env_id.item()] = self.velocities[env_id, :L]
-        return out
-
-    @torch.no_grad()
-    def compute_jerk(self, env_ids: torch.Tensor, threshold: float = 0.1) -> torch.Tensor:
-        """
-        Подсчитать количество резких скачков скоростей.
-        Args:
-            threshold (float): порог
-        Returns: [K] количество скачков
-        """
-        env_ids = env_ids.to(self.device)
-        vels = self.velocities[env_ids]  # [K, T_max, 2]
-        L = self.lengths[env_ids]
-
-        diffs = torch.norm(vels[:, 1:] - vels[:, :-1], dim=-1)  # [K, T_max-1]
-
-        mask = torch.arange(self.T_max-1, device=self.device).unsqueeze(0) < (L.unsqueeze(1)-1)
-        diffs = diffs * mask
-
-        return (diffs > threshold).sum(dim=1).float()
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_generator.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_generator.py
deleted file mode 100644
index f63bf64333..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_generator.py
+++ /dev/null
@@ -1,535 +0,0 @@
-import torch
-import networkx as nx
-import json
-import os
-from pathlib import Path
-import matplotlib.pyplot as plt
-from itertools import combinations
-import importlib.util
-import random
-
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_for_pg.py"
-SceneManager = import_class_from_path(module_path, "SceneManager")
-
-class PathGenerator:
-    def __init__(self, num_obstacles=6, config_path="source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json", device='cuda:0', ratio=8, room_len_x=10, room_len_y=8, test_mode=False):
-        print(f"[DEBUG] Initializing PathGenerator with:")
-        print(f"  - num_obstacles: {num_obstacles}")
-        print(f"  - device: {device}")
-        print(f"  - ratio: {ratio}")
-        print(f"  - room_len_x: {room_len_x}")
-        print(f"  - room_len_y: {room_len_y}")
-        print(f"  - test_mode: {test_mode}")
-        
-        self.num_obstacles = num_obstacles
-        self.device = device
-        self.ratio = ratio
-        self.room_len_x = room_len_x
-        self.room_len_y = room_len_y
-        self.test_mode = test_mode
-        self.max_start_nodes = 1 if test_mode else float('inf')  # Ограничение на число стартовых узлов в тестовом режиме
-        self.shift = [5, 4]  # Смещение для локальных координат
-        
-        print(f"[DEBUG] Creating Scene_manager with num_envs=1, device={device}, num_obstacles={num_obstacles}")
-        self.scene_manager = SceneManager(num_envs=1, config_path=config_path, device=device)  # Новый вызов
-        print(f"[DEBUG] Scene_manager created successfully")
-        
-        self.log_dir = "logs/aloha_data_graphs"#str(Path().resolve()) + "/logs/"
-        print(f"[DEBUG] Log directory: {self.log_dir}")
-        os.makedirs(self.log_dir, exist_ok=True)
-        
-        self.paths_file = os.path.join("data", "all_paths.json")
-        self.graphs_dir = os.path.join(self.log_dir, "graphs")
-        print(f"[DEBUG] Paths file: {self.paths_file}")
-        print(f"[DEBUG] Graphs directory: {self.graphs_dir}")
-        
-        os.makedirs(self.graphs_dir, exist_ok=True)
-        self.all_paths = {}
-        print(f"[DEBUG] PathGenerator initialization complete")
-
-    def _create_grid_with_diagonals(self, width, height):
-        print(f"[DEBUG] Creating grid with diagonals: width={width}, height={height}")
-        graph = nx.grid_2d_graph(width, height)
-        initial_nodes = len(graph.nodes)
-        initial_edges = len(graph.edges)
-        print(f"[DEBUG] Initial grid: {initial_nodes} nodes, {initial_edges} edges")
-        
-        for u, v in graph.edges():
-            graph[u][v]['weight'] = 1.0  # Устанавливаем вес 1.0 для всех рёбер
-        
-        diagonal_count = 0
-        for x in range(width):
-            for y in range(height):
-                if x + 1 < width and y + 1 < height:
-                    graph.add_edge((x, y), (x + 1, y + 1), weight=1)  # Диагональ
-                    diagonal_count += 1
-                if x + 1 < width and y - 1 >= 0:
-                    graph.add_edge((x, y), (x + 1, y - 1), weight=1)
-                    diagonal_count += 1
-        
-        print(f"[DEBUG] Added {diagonal_count} diagonal edges")
-        print(f"[DEBUG] Final grid: {len(graph.nodes)} nodes, {len(graph.edges)} edges")
-        return graph
-
-    def find_boundary_nodes(self, graph):
-        print(f"[DEBUG] Finding boundary nodes in graph with {len(graph.nodes)} nodes")
-        if not graph.nodes:
-            print(f"[DEBUG] Graph has no nodes, returning empty set")
-            return set()
-        
-        degrees = dict(graph.degree())
-        max_degree = max(degrees.values()) if degrees else 0
-        boundary_nodes = {node for node in graph.nodes() if graph.degree(node) < max_degree}
-        
-        print(f"[DEBUG] Max degree: {max_degree}")
-        print(f"[DEBUG] Found {len(boundary_nodes)} boundary nodes")
-        print(f"[DEBUG] Boundary nodes sample: {list(boundary_nodes)[:10]}...")
-        return boundary_nodes
-
-    def find_expanded_boundary(self, graph, boundary_nodes, excluded_nodes=set()):
-        print(f"[DEBUG] Finding expanded boundary from {len(boundary_nodes)} boundary nodes")
-        print(f"[DEBUG] Excluded nodes: {len(excluded_nodes)}")
-        
-        expanded_boundary = set()
-        for node in boundary_nodes:
-            neighbors = list(graph.neighbors(node))
-            valid_neighbors = [n for n in neighbors if n not in excluded_nodes]
-            expanded_boundary.update(valid_neighbors)
-        
-        result = expanded_boundary - boundary_nodes
-        print(f"[DEBUG] Expanded boundary: {len(result)} nodes")
-        print(f"[DEBUG] Expanded boundary sample: {list(result)[:10]}...")
-        return result
-
-    def assign_edge_weights(self, graph, boundary_nodes, expanded_boundary):
-        print(f"[DEBUG] Assigning edge weights")
-        print(f"[DEBUG] Boundary nodes: {len(boundary_nodes)}")
-        print(f"[DEBUG] Expanded boundary: {len(expanded_boundary)}")
-        
-        weight_changes = {'boundary': 0, 'expanded': 0, 'normal': 0}
-
-        for u, v in graph.edges():
-            old_weight = graph[u][v]['weight']
-            if u in boundary_nodes or v in boundary_nodes:
-                graph[u][v]['weight'] = max(old_weight, 3)
-                weight_changes['boundary'] += 1
-            elif u in expanded_boundary or v in expanded_boundary:
-                graph[u][v]['weight'] = max(old_weight, 2)
-                weight_changes['expanded'] += 1
-            else:
-                graph[u][v]['weight'] = old_weight
-                weight_changes['normal'] += 1
-        
-        print(f"[DEBUG] Weight assignment complete:")
-        print(f"  - Boundary edges: {weight_changes['boundary']}")
-        print(f"  - Expanded boundary edges: {weight_changes['expanded']}")
-        print(f"  - Normal edges: {weight_changes['normal']}")
-
-    def _check_intersection(self, point, obstacle_positions, obstacle_radii, add_r=0.0):
-        # print("obstacle_positions ", obstacle_positions)
-        if len(obstacle_positions) == 0:
-            return torch.tensor([False], device=self.device)
-        # point_tensor = self.grid_to_real(point)
-        # # for pos in obstacle_positions
-        # print("point_tensor is ", point_tensor, point)
-        result = False
-        for pos in obstacle_positions:
-            # print("pos ", pos, point)
-            distances = torch.norm(pos - point)
-            # print(distances)
-            if distances < (0.35 + self.scene_manager.robot_radius + add_r):
-                result = True
-        # result = torch.any(distances < (obstacle_radii + self.scene_manager.robot_radius + add_r), dim=-1)
-        return result
-
-    def get_scene_grid(self, obstacle_positions, obstacle_radii):
-        ratio_x = self.ratio * self.room_len_x
-        ratio_y = self.ratio * self.room_len_y
-        G = self._create_grid_with_diagonals(ratio_x, ratio_y)
-        obstacle_positions = torch.tensor(obstacle_positions, device=self.device, dtype=torch.float32)
-        obstacle_radii = torch.tensor(obstacle_radii, device=self.device, dtype=torch.float32)
-        room_bounds = self.scene_manager.room_bounds
-        add_r = 1 / self.ratio
-
-        # print(f"[DEBUG] Initial graph: {len(G.nodes)} nodes, {len(G.edges)} edges")
-        
-        for node in list(G.nodes):
-            x, y = node
-            scaled_point = self.grid_to_real(node)
-            # print(scaled_point)
-            # print(1)
-            if (self._check_intersection(scaled_point, obstacle_positions, obstacle_radii, add_r) or
-                scaled_point[0] < room_bounds['x_min'] + 0.2 or
-                scaled_point[0] > room_bounds['x_max'] - self.scene_manager.robot_radius or
-                scaled_point[1] < room_bounds['y_min'] + self.scene_manager.robot_radius or
-                scaled_point[1] > room_bounds['y_max'] - self.scene_manager.robot_radius):
-                # print("remove node ", node, )
-                G.remove_node(node)
-
-        # print(f"[DEBUG] Graph after removing nodes: {len(G.nodes)} nodes, {len(G.edges)} edges")
-        
-        boundary_nodes = self.find_boundary_nodes(G)
-        expanded_boundary = [boundary_nodes]
-        levels = 2
-        for i in range(levels):
-            expanded_boundary.append(self.find_expanded_boundary(G, expanded_boundary[-1]))
-        expanded_boundary.reverse()
-
-        print(f"[DEBUG] Assigning edge weights for {len(G.edges)} edges")
-        self.assign_edge_weights(G, boundary_nodes, expanded_boundary[-1])
-        for u, v in G.edges():
-            for i in range(1, len(expanded_boundary)):
-                set_nodes = expanded_boundary[i]
-                prev_set_nodes = expanded_boundary[i - 1]
-                if (u in prev_set_nodes and v in set_nodes) or (u in set_nodes and v in set_nodes):
-                    G[u][v]['weight'] = 1 + i
-
-        print(f"[DEBUG] Graph has {len(G.nodes)} nodes and {len(G.edges)} edges after weight assignment")
-        return G
-
-    def find_nearest_reachable_node(self, graph, target):
-        print(f"[DEBUG] Finding nearest reachable node to target: {target}")
-        
-        if target in graph and len(list(graph.neighbors(target))) > 0:
-            print(f"[DEBUG] Target {target} is already reachable")
-            return target
-        
-        min_distance = float('inf')
-        nearest_node = None
-        target_x, target_y = target
-        candidates_checked = 0
-        
-        for node in graph.nodes:
-            if len(list(graph.neighbors(node))) > 0:
-                candidates_checked += 1
-                x, y = node
-                distance = abs(target_x - x) + abs(target_y - y)
-                if distance < min_distance:
-                    min_distance = distance
-                    nearest_node = node
-        
-        print(f"[DEBUG] Checked {candidates_checked} candidate nodes")
-        print(f"[DEBUG] Nearest reachable node: {nearest_node} (distance: {min_distance})")
-        return nearest_node
-
-    def remove_straight_segments(self, path):
-        print(f"[DEBUG] Removing straight segments from path of length {len(path)}")
-        
-        if len(path) < 3:
-            print(f"[DEBUG] Path too short, returning unchanged")
-            return path
-        
-        filtered_path = [path[0]]
-        removed_count = 0
-        
-        for i in range(1, len(path) - 1):
-            x1, y1 = path[i - 1]
-            x2, y2 = path[i]
-            x3, y3 = path[i + 1]
-            
-            if (x3 - x1) * (y2 - y1) != (y3 - y1) * (x2 - x1):
-                filtered_path.append(path[i])
-            else:
-                removed_count += 1
-        
-        filtered_path.append(path[-1])
-        print(f"[DEBUG] Removed {removed_count} straight segments, new length: {len(filtered_path)}")
-        return filtered_path
-
-    def save_graph_image(self, graph, path, config_key, n_save=1000):
-        print(f"[DEBUG] Saving graph image for config: {config_key}")
-        if len(path) > 4:
-            static_path = self.remove_straight_segments(path)
-        else:
-            static_path = path
-        pos = {node: node for node in graph.nodes}
-        
-        node_colors = []
-        color_counts = {'green': 0, 'red': 0, 'lightblue': 0, 'yellow': 0, 'cyan': 0}
-        start_node = path[0] if path else None
-        end_node = path[-1] if path else None
-        
-        for node in graph.nodes:
-            if node == end_node:
-                node_colors.append('yellow')  # Цель — жёлтый
-                color_counts['yellow'] += 1
-            elif node == start_node:
-                node_colors.append('cyan')   # Старт — голубой
-                color_counts['cyan'] += 1
-            elif node in static_path:
-                node_colors.append('green')
-                color_counts['green'] += 1
-            elif node in path:
-                node_colors.append('red')
-                color_counts['red'] += 1
-            else:
-                node_colors.append('lightblue')
-                color_counts['lightblue'] += 1
-        
-        print(f"[DEBUG] Node colors: {color_counts}")
-        
-        plt.figure(figsize=(32, 32))
-        nx.draw(graph, pos, with_labels=False, node_color=node_colors, node_size=200)
-        
-        edge_labels = {(u, v): f"{d.get('weight', 100.0):.1f}" for u, v, d in graph.edges(data=True)}
-        nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels, font_size=12, font_color='red')
-        
-        # Добавляем точку (0, 0)
-        plt.scatter([0], [0], color='black', s=500, marker='*', label='Grid Origin (0, 0)')
-        
-        # Добавляем оси координат
-        arrow_length = min(self.ratio * self.room_len_x, self.ratio * self.room_len_y) * 0.1  # 10% от размера сетки
-        plt.arrow(0, 0, arrow_length, 0, head_width=1, head_length=2, fc='blue', ec='blue', label='X-axis')
-        plt.arrow(0, 0, 0, arrow_length, head_width=1, head_length=2, fc='red', ec='red', label='Y-axis')
-        
-        # Добавляем подписи осей
-        plt.text(arrow_length + 2, 0, 'X', fontsize=14, color='blue', verticalalignment='center')
-        plt.text(0, arrow_length + 2, 'Y', fontsize=14, color='red', horizontalalignment='center')
-        
-        plt.title(f"Grid Graph for Configuration {config_key}")
-        if path:
-            start_node = path[0]
-            end_node = path[-1]
-            graph_img_file = os.path.join(self.graphs_dir, f"grid_graph_{config_key}_start_{start_node[0]}_{start_node[1]}_end_{end_node[0]}_{end_node[1]}.png")
-        else:
-            graph_img_file = os.path.join(self.graphs_dir, f"grid_graph_{config_key}_no_path.png")
-        
-        print(f"[DEBUG] Saving graph image to: {graph_img_file}")
-        plt.legend(fontsize=12)
-        plt.savefig(graph_img_file, dpi=200)
-        plt.close()
-        print(f"[DEBUG] Graph image saved successfully")
-
-    def generate_paths(self, n_save=1500, targets=[[-4.5, 0]]):
-        """
-        Генерирует и сохраняет пути для всех возможных комбинаций координат препятствий.
-        Логика:
-        - Извлекаем grid_coords для movable_obstacle из конфига (self.scene_manager.config).
-        - Для каждого k (от 1 до len(grid_coords)): Генерируем combinations(grid_coords, k) — подмножества позиций.
-        - Для каждой комбо позиций: 
-        - Создаем config_str как ','.join(sorted([f"{x:.1f}_{y:.1f}_{z:.1f}" for x,y,z in combo])).
-        - Устанавливаем эти позиции для активных movable_obstacle nodes с помощью set_obstacle_positions (первые k nodes активны, остальным — default).
-        - Для каждой цели в targets: Устанавливаем позицию цели с set_goal_position (учитывая surface_only: на surface_provider с margin=0 для фикса).
-        - Строим граф, удаляем intersecting nodes, вычисляем пути как раньше.
-        - Сохраняем в all_paths.json после n_save конфигураций или в конце.
-        """
-        print(f"[DEBUG] Starting path generation for {n_save} saves, targets={targets}")
-        
-        # Шаг 1: Извлекаем grid_coords для movable_obstacle из конфига
-        movable_grid_coords = []
-        for obj_cfg in self.scene_manager.config:
-            if "movable_obstacle" in obj_cfg['type']:
-                placement = obj_cfg.get('placement', [])
-                if placement and placement[0]['strategy'] == 'grid':
-                    movable_grid_coords = placement[0]['grid_coordinates']  # [[x,y,z], ...]
-                    num_movable = obj_cfg['count']  # 3 для chairs
-                    break
-        if not movable_grid_coords:
-            raise ValueError("[ERROR] No movable_obstacle with grid strategy found in config")
-        
-        print(f"[DEBUG] Movable obstacle grid_coords: {movable_grid_coords}, count={num_movable}")
-        
-        # Шаг 2: Извлекаем возможные цели (targets уже даны, но проверяем possible_goal)
-        goal_surfaces = []
-        for obj_cfg in self.scene_manager.config:
-            if "possible_goal" in obj_cfg['type']:
-                placement = obj_cfg.get('placement', [])
-                if "surface_only" in obj_cfg['type'] and placement and placement[0]['strategy'] == 'on_surface':
-                    # Находим surface_provider grid_coords
-                    surface_types = placement[0]['surface_types']
-                    for surf_cfg in self.scene_manager.config:
-                        if any(t in surf_cfg['type'] for t in surface_types):
-                            surf_placement = surf_cfg.get('placement', [])
-                            if surf_placement and surf_placement[0]['strategy'] == 'grid':
-                                goal_surfaces.extend(surf_placement[0]['grid_coordinates'])
-        if goal_surfaces:
-            print(f"[DEBUG] Goal possible positions from surfaces: {goal_surfaces}")
-            targets = goal_surfaces
-            # Но используем targets как фиксированные, можно расширить targets += goal_surfaces если нужно
-        
-        # Шаг 3: Генерация комбинаций подмножеств координат (subsets)
-        from itertools import combinations
-        grid = self._create_grid_with_diagonals(self.room_len_x * self.ratio, self.room_len_y * self.ratio)
-        print(f"[DEBUG] Base grid created with {len(grid.nodes)} nodes")
-        
-        num_processed = 0
-        for k in range(0, len(movable_grid_coords) + 1):  # k=1 to 3
-            for comb in combinations(movable_grid_coords, k):  # comb = tuple of [x,y,z] lists
-                # Создаем config_str: sorted string of positions
-                sorted_comb = sorted(comb, key=lambda p: (p[0], p[1], p[2]))  # Sort by x,y,z
-                config_str = ','.join([f"{p[0]:.1f}_{p[1]:.1f}_{p[2]:.1f}" for p in sorted_comb])
-                print(f"[DEBUG] Processing config {config_str} with {k} obstacles")
-                
-                # Шаг 4: Устанавливаем позиции препятствий детерминировано
-                env_ids = torch.tensor([0], device=self.device)
-                self.scene_manager.set_obstacle_positions(env_ids, list(comb))  # Новая функция, см. пункт 2
-                
-                # Извлекаем positions/radii для активных movable_obstacle
-                graph = self.scene_manager.graphs[0]
-                obstacle_indices = graph.get_nodes_by_type("movable_obstacle", only_active=True)
-                obstacle_positions = graph.positions[obstacle_indices, :2]  # [num_active, 2]
-                obstacle_radii = graph.radii[obstacle_indices]  # [num_active]
-                
-                # Шаг 5: Строим граф сцены (удаляем intersections)
-                config_graph = self.get_scene_grid(obstacle_positions, obstacle_radii)
-                print(f"[DEBUG] Config graph for {config_str}: {len(config_graph.nodes)} nodes")
-                
-                # Шаг 6: Для каждой цели в targets
-                for target in targets:
-                    # Устанавливаем позицию цели (с учетом surface_only)
-                    self.scene_manager.set_goal_position(env_ids, target)  # Новая функция, см. пункт 2
-                    
-                    # Конвертируем target в grid (теперь target — реальная, после set)
-                    goal_pos = self.scene_manager.goal_positions[0, :3].cpu().tolist()  # После set
-                    grid_targets = self.get_grid_targets([goal_pos], obstacle_positions, obstacle_radii, config_graph)
-                    
-                    for grid_target in grid_targets:
-                        print(f"[DEBUG] Computing paths for target {grid_target}")
-                        
-                        # Вычисляем shortest paths (как раньше)
-                        try:
-                            paths_from_target = nx.single_source_dijkstra_path(config_graph, grid_target)
-                        except nx.NodeNotFound:
-                            print(f"[WARNING] Target {grid_target} not in graph, skipping")
-                            continue
-                        
-                        # Обрабатываем пути (reverse, simplify)
-                        start_nodes = list(paths_from_target.keys())
-                        if self.test_mode:
-                            start_nodes = random.sample(start_nodes, min(self.max_start_nodes, len(start_nodes)))
-                        
-                        for start in start_nodes:
-                            path = paths_from_target[start][::-1]  # Reverse
-                            simplified_path = _simplify_path(path)
-                            num_processed += 1
-                            # if num_processed % n_save == 0:
-                            #     self.save_graph_image(config_graph, simplified_path, config_str)
-                            if len(simplified_path) > 1:
-                                self.all_paths.setdefault(config_str, {}).setdefault(str(grid_target), {})[str(start)] = simplified_path
-                        # self.save_graph_image(config_graph, path, config_key=15, n_save=500000000)
-                        print(f"[DEBUG] Processed {len(start_nodes)} start nodes for target {grid_target}")
-                
-                
-            
-        self._save_paths()
-        print(f"\n[DEBUG] ===== PATH GENERATION COMPLETE =====")
-        print(f"[DEBUG] Total configurations processed: {len(self.all_paths)}")
-
-    def _save_paths(self):
-        import re
-        json_paths = {}
-        print("all paths: ", self.all_paths)
-        for config_str, targets_inner in self.all_paths.items():
-            print("config_str: ", config_str)
-            json_paths[config_str] = {}
-            for target, nodes in targets_inner.items():
-                print("target: ", target)
-                target_str = re.sub(r"[()]", "", target)  # str(grid_target) уже tuple, но на str
-                json_paths[config_str][target_str] = {}
-                print(config_str, target_str)
-                for node, path in nodes.items():
-                    print("node", node)
-                    print("path", path)
-                    node_str = re.sub(r"[()]", "", node)
-                    json_paths[config_str][target_str][node_str] = [list(p) for p in path]
-        with open(self.paths_file, 'w') as f:
-            json.dump(json_paths, f, indent=4)
-        print(f"[DEBUG] Saved {len(self.all_paths)} configurations to {self.paths_file}")
-        print(f"[DEBUG] JSON file size: {os.path.getsize(self.paths_file)} bytes")
-
-    def get_grid_targets(self, targets, obstacle_positions, obstacle_radii, config_graph):
-        print(f"[DEBUG] Converting targets to grid: {targets}")
-        grid_targets = []
-        for target in targets:
-            grid_node = self.real_to_grid(target)
-            
-            scaled_point = self.grid_to_real(grid_node)
-            print(3)
-            is_valid = (
-                not self._check_intersection(scaled_point, obstacle_positions, obstacle_radii) and
-                grid_node in config_graph and
-                len(list(config_graph.neighbors(grid_node))) > 0
-            )
-            
-            if is_valid:
-                grid_targets.append(grid_node)
-            else:
-                nearest_node = self.find_nearest_reachable_node(config_graph, grid_node)
-                if nearest_node is not None:
-                    grid_targets.append(nearest_node)
-                else:
-                    print(f"Warning: No reachable node found for target {target}. Using original grid node.")
-                    grid_targets.append(grid_node)
-        
-        print(f"[DEBUG] Grid targets: {grid_targets}")
-        return grid_targets
-
-    def grid_to_real(self, grid_point):
-        """Преобразует сеточные координаты (x, y) в реальные с поворотом на 90 градусов по часовой стрелке."""
-        x, y = grid_point
-        real_x = x / self.ratio - self.shift[0]  # x_real = y / ratio + shift_x
-        real_y = y / self.ratio - self.shift[1]  # y_real = -x / ratio + shift_y
-        return torch.tensor([real_x, real_y], device=self.device, dtype=torch.float32)
-
-    def real_to_grid(self, real_point):
-        """Преобразует реальные координаты (x, y) в сеточные с поворотом на 90 градусов по часовой стрелке."""
-        x, y, _ = real_point
-        grid_x = int((x + self.shift[0]) * self.ratio)  # x_grid = (-y + shift_y) * ratio
-        grid_y = int((y + self.shift[1]) * self.ratio)   # y_grid = (x - shift_x) * ratio
-        return (grid_x, grid_y)
-
-import math
-
-def _simplify_path(points, tol=1e-5):
-    """Удаляет точки на прямых сегментах и 'ложные углы'"""
-    if len(points) <= 2:
-        return points
-
-    simplified = [points[0]]
-    for i in range(1, len(points) - 1):
-        prev = simplified[-1]  # Последняя оставленная точка
-        curr = points[i]
-        nxt = points[i + 1]
-
-        # Векторы
-        v1 = (curr[0] - prev[0], curr[1] - prev[1])
-        v2 = (nxt[0] - curr[0], nxt[1] - curr[1])
-
-        # -------------------------
-        # 1. Удаляем точки на прямых
-        len1 = math.hypot(*v1)
-        len2 = math.hypot(*v2)
-        if len1 >= tol and len2 >= tol:
-            v1n = (v1[0] / len1, v1[1] / len1)
-            v2n = (v2[0] / len2, v2[1] / len2)
-            if abs(v1n[0] - v2n[0]) < tol and abs(v1n[1] - v2n[1]) < tol:
-                continue
-
-        # -------------------------
-        # 2. Удаляем 'ложные углы'
-        dx1, dy1 = int(round(v1[0])), int(round(v1[1]))
-        dx2, dy2 = int(round(v2[0])), int(round(v2[1]))
-        if ((abs(dx1) == 1 and abs(dy1) == 1) and (abs(dx2) + abs(dy2) == 1)) or \
-           ((abs(dx2) == 1 and abs(dy2) == 1) and (abs(dx1) + abs(dy1) == 1)):
-            continue
-
-        simplified.append(curr)
-
-    simplified.append(points[-1])
-    return simplified
-
-
-if __name__ == "__main__":
-    print("[DEBUG] Starting PathGenerator script...")
-    generator = PathGenerator(num_obstacles=3, device='cuda:0', test_mode=False)
-    generator.generate_paths(n_save=500000, targets=[[-4.5, 0],[-4.5, -1],[-4.5, 1]])    #[[-4.2, -1],[-4.2, 0],[-4.2, 1]])
-    print("[DEBUG] PathGenerator script complete!")
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_manager.py
deleted file mode 100644
index 95f9b0b78d..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/path_manager.py
+++ /dev/null
@@ -1,171 +0,0 @@
-import json
-import torch
-import os
-from pathlib import Path
-
-class Path_manager:
-    def __init__(self, scene_manager, ratio: float = 4.0, shift: list = [5, 4], device: str = 'cpu'):
-        """
-        Инициализирует менеджер путей для загрузки путей из all_paths.json и преобразования координат.
-
-        Args:
-            log_dir (str): Путь к директории с all_paths.json.
-            ratio (float): Масштабный коэффициент для преобразования координат (по умолчанию 4.0).
-            shift (list): Смещение координат [shift_x, shift_y] (по умолчанию [5, 4]).
-            device (str): Устройство для хранения путей (по умолчанию 'cpu').
-        """
-        self.device = device
-        self.ratio = ratio
-        self.shift = torch.tensor(shift, device=device, dtype=torch.float32)
-        self.all_paths = {}
-        self.paths_file = os.path.join("data", "all_paths.json")
-        self._load_paths()
-        self.scene_manager = scene_manager
-
-    def _load_paths(self):
-        """
-        Загружает пути из all_paths.json.
-        Формат: {config_key: {target_node: {start_node: path}}}
-        """
-        if os.path.exists(self.paths_file):
-            try:
-                with open(self.paths_file, 'r') as f:
-                    loaded_paths = json.load(f)
-                for config_key, targets in loaded_paths.items():
-                    self.all_paths[config_key] = {}
-                    for target_str, nodes in targets.items():
-                        target = tuple(map(int, target_str.split(',')))
-                        self.all_paths[config_key][target] = {}
-                        for node_str, path in nodes.items():
-                            node = tuple(map(int, node_str.split(',')))
-                            self.all_paths[config_key][target][node] = [tuple(p) for p in path]
-                print(f"Loaded {len(self.all_paths)} configurations from {self.paths_file}")
-            except Exception as e:
-                print(f"Error loading paths file: {e}")
-                self.all_paths = {}
-        else:
-            print(f"No paths file found at {self.paths_file}")
-            self.all_paths = {}
-
-    def real_to_grid(self, real_point: torch.Tensor) -> torch.Tensor:
-        """
-        Преобразует реальные координаты (x, y) в сеточные.
-
-        Args:
-            real_point (torch.Tensor): Реальные координаты [num_envs, 2].
-
-        Returns:
-            torch.Tensor: Сеточные координаты [num_envs, 2], целочисленные.
-        """
-        grid_x = torch.round((real_point[:, 0] + self.shift[0]) * self.ratio).to(torch.int32)
-        grid_y = torch.round((real_point[:, 1] + self.shift[1]) * self.ratio).to(torch.int32)
-        return torch.stack([grid_x, grid_y], dim=-1)
-
-    def grid_to_real(self, grid_point: torch.Tensor) -> torch.Tensor:
-        """
-        Преобразует сеточные координаты (x, y) в реальные.
-
-        Args:
-            grid_point (torch.Tensor): Сеточные координаты [..., 2].
-
-        Returns:
-            torch.Tensor: Реальные координаты [..., 2].
-        """
-        # print(grid_point)
-        # print(grid_point[:, 0])
-        # print(grid_point[..., 0])
-        real_x = grid_point[..., 0] / self.ratio - self.shift[0]
-        real_y = grid_point[..., 1] / self.ratio - self.shift[1]
-        return torch.stack([real_x, real_y], dim=-1)
-    
-    def get_paths(self, env_ids: torch.Tensor, start_positions: torch.Tensor, target_positions: torch.Tensor, active_obstacle_positions_list: list, device: str = 'cuda:0'):
-        """
-        Возвращает пути для заданных конфигураций, стартовых и целевых позиций в реальных координатах.
-        Теперь принимает готовый список позиций препятствий.
-
-        Args:
-            env_ids (torch.Tensor): Индексы сред [num_envs].
-            start_positions (torch.Tensor): Стартовые позиции в реальных координатах [num_envs, 2].
-            target_positions (torch.Tensor): Целевые позиции в реальных координатах [num_envs, 2].
-            active_obstacle_positions_list (list): Список списков с позициями активных препятствий 
-                                                     для каждой среды. Формат: [[(x,y,z), ...], [...]]
-            device (str): Устройство для возвращаемых тензоров.
-
-        Returns:
-            torch.Tensor: Пути в реальных координатах [num_envs, max_path_length, 2].
-        """
-        # Создаем ключи конфигураций из предоставленного списка
-        configs = []
-        for active_pos in active_obstacle_positions_list:
-            # `active_pos` уже отсортирован и округлен в scene_manager
-            config = ','.join([f"{p[0]:.1f}_{p[1]:.1f}_{p[2]:.1f}" for p in active_pos]) if active_pos else ''
-            configs.append(config)
-        # print("configs: ", configs)
-        # --- Остальная часть функции остается без изменений ---
-
-        # Преобразуем реальные координаты в сеточные
-        start_nodes = self.real_to_grid(start_positions)
-        target_nodes = self.real_to_grid(target_positions)
-        
-        max_path_length = 15
-        self.max_path_length = max_path_length
-        paths = []
-        for i, (config, start, target) in enumerate(zip(configs, start_nodes.tolist(), target_nodes.tolist())):
-            start = tuple(start)
-            target = tuple(target)
-            path = self.all_paths.get(config, {}).get(target, {}).get(start, [])
-            
-            # Если путь не найден, ищем ближайшие узлы
-            if not path:
-                target_dict = self.all_paths.get(config, {})
-                if target_dict:
-                    nearest_target = self.find_nearest_node(target, set(target_dict.keys()))
-                    if nearest_target:
-                        start_dict = target_dict.get(nearest_target, {})
-                        if start_dict:
-                            nearest_start = self.find_nearest_node(start, set(start_dict.keys()))
-                            if nearest_start:
-                                path = target_dict[nearest_target].get(nearest_start, [])
-            paths.append(path)
-
-        # Создаем тензор путей в сеточных координатах
-        path_tensor = torch.full((len(env_ids), max_path_length, 2), -7777.0, device=device, dtype=torch.float32)
-        for i, (env_id, path) in enumerate(zip(env_ids, paths)):
-            if path:
-                path_length = len(path)
-                if path_length > max_path_length:
-                    print(f"[DEBUG] Warning: Path for env {env_id} truncated from {path_length} to {max_path_length}")
-                    path = path[-max_path_length:]  # Берем последние max_path_length точек
-                path_tensor[i, -len(path):] = torch.tensor(path, device=device, dtype=torch.float32)
-            else:
-                # print(f"No path found for env {env_id}, config {configs[i]}, start {start_nodes[i].tolist()}, target {target_nodes[i].tolist()}")
-                # Заполняем последнюю точку стартовой позицией
-                path_tensor[i, -1] = start_nodes[i].to(device=device, dtype=torch.float32)
-
-        # Преобразуем пути в реальные координаты
-        path_tensor = self.grid_to_real(path_tensor.to(device=device))
-        return path_tensor
-
-    def find_nearest_node(self, target: tuple, nodes: set) -> tuple:
-        """
-        Находит ближайший узел из набора узлов к целевой точке (манхэттенское расстояние).
-
-        Args:
-            target (tuple): Целевая точка в сеточных координатах (x, y).
-            nodes (set): Набор узлов в сеточных координатах.
-
-        Returns:
-            tuple: Ближайший узел или None, если набор узлов пуст.
-        """
-        if not nodes:
-            return None
-        min_distance = float('inf')
-        nearest_node = None
-        target_x, target_y = target
-        for node in nodes:
-            x, y = node
-            distance = abs(target_x - x) + abs(target_y - y)
-            if distance < min_distance:
-                min_distance = distance
-                nearest_node = node
-        return nearest_node
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py
deleted file mode 100644
index 9baaab4895..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py
+++ /dev/null
@@ -1,126 +0,0 @@
-import torch
-import random
-from abc import ABC, abstractmethod
-# ПРИМЕЧАНИЕ: ObstacleGraph больше не используется. Вместо него передаются тензоры
-# из нового VectorizedSceneManager.
-
-class PlacementStrategy(ABC):
-    def __init__(self, device: str, **kwargs):
-        self.device = device
-
-    @abstractmethod
-    def apply(self, env_ids: torch.Tensor, object_indices: torch.Tensor, scene_data: dict, mess: bool):
-        # scene_data будет словарем с тензорами: 'positions', 'active' и т.д.
-        pass
-
-class FixedPlacement(PlacementStrategy):
-    def __init__(self, device: str, positions_dict: dict):
-        super().__init__(device)
-        self.positions_dict = positions_dict  # { "chair": [[x,y,z], ...], "table": [...], ... }
-
-    def apply(self, env_ids: torch.Tensor, obj_indices: torch.Tensor, scene_data: dict, mess: bool):
-        # obj_indices: (num_envs, num_to_place) — индексы объектов, которые нужно разместить
-        # Для фиксированного размещения мы просто берём позиции из словаря
-        for env_i, env_id in enumerate(env_ids.tolist()):
-            for j, obj_idx in enumerate(obj_indices[env_i].tolist()):
-                # Определяем имя по индексу (scene_manager.names[obj_idx])
-                obj_name = scene_data["names"][obj_idx] if "names" in scene_data else f"obj_{obj_idx}"
-                if obj_name.split("_")[0] in self.positions_dict:
-                    pos_list = self.positions_dict[obj_name.split("_")[0]]
-                    if j < len(pos_list):
-                        scene_data["positions"][env_id, obj_idx] = torch.tensor(pos_list[j], device=self.device)
-                        scene_data["active"][env_id, obj_idx] = True
-                        scene_data["on_surface_idx"][env_id, obj_idx] = -1
-                        scene_data["surface_level"][env_id, obj_idx] = 0
-
-
-class GridPlacement(PlacementStrategy):
-    def __init__(self, device: str, grid_coordinates: list[list[float]]):
-        super().__init__(device)
-        self.grid = torch.tensor(grid_coordinates, device=self.device, dtype=torch.float32)
-
-    def apply(
-        self,
-        env_ids: torch.Tensor,
-        obj_indices_to_place: torch.Tensor, # Форма: (num_envs, num_to_place)
-        scene_data: dict,
-        mess: bool
-    ):
-        num_to_place = obj_indices_to_place.shape[1]
-        if num_to_place == 0:
-            return
-
-        num_grid_points = len(self.grid)
-        
-        # Генерируем случайные перестановки позиций на сетке для каждого окружения
-        # torch.rand(...).argsort() - эффективный способ получить батч перестановок
-        pos_indices = torch.rand(len(env_ids), num_grid_points, device=self.device).argsort(dim=1)[:, :num_to_place]
-        # print("pos_indices ", pos_indices)
-        # Выбираем позиции из сетки согласно сгенерированным индексам
-        selected_positions = self.grid[pos_indices] # Форма: (num_envs, num_to_place, 3)
-        # print("selected_positions ", selected_positions)
-        # Обновляем глобальные тензоры позиций и состояний
-        # Используем scatter_ для эффективного обновления по индексам
-        env_idx_tensor = env_ids.view(-1, 1).expand_as(obj_indices_to_place)
-        scene_data['positions'][env_idx_tensor, obj_indices_to_place] = selected_positions
-        scene_data['active'][env_idx_tensor, obj_indices_to_place] = True
-        scene_data['on_surface_idx'][env_idx_tensor, obj_indices_to_place] = -1 # Размещение на сетке = на полу
-        scene_data['surface_level'][env_idx_tensor, obj_indices_to_place] = 0
-
-class OnSurfacePlacement(PlacementStrategy):
-    def __init__(self, device: str, surface_indices: list[int], margin: float):
-        super().__init__(device)
-        self.surface_indices = torch.tensor(surface_indices, device=self.device, dtype=torch.long)
-        self.margin = margin
-
-    def apply(
-        self,
-        env_ids: torch.Tensor,
-        obj_indices_to_place: torch.Tensor, # Форма: (num_envs, num_to_place)
-        scene_data: dict,
-        mess: bool
-    ):
-        num_to_place = obj_indices_to_place.shape[1]
-        if num_to_place == 0:
-            return
-
-        num_envs_to_process = len(env_ids)
-
-        # 1. Находим все активные поверхности в нужных окружениях
-        # print("scene_data ", scene_data)
-        active_mask = scene_data['active'][env_ids][:, self.surface_indices] # (num_envs, num_surfaces)
-        
-        # 2. Для каждого окружения выбираем случайную активную поверхность
-        # Используем torch.multinomial для векторизованного выбора
-        probs = active_mask.float()
-        chosen_surface_rel_idx = torch.multinomial(probs, num_to_place, replacement=False) # (num_envs, num_to_place)
-        target_surface_idx = self.surface_indices[chosen_surface_rel_idx] # (num_envs, num_to_place)
-        # print("chosen_surface_rel_idx:  ", target_surface_idx)
-        # 3. Собираем данные о выбранных поверхностях
-        env_idx_tensor = env_ids.view(-1, 1).expand_as(target_surface_idx)
-        surface_pos = scene_data['positions'][env_idx_tensor, target_surface_idx]
-        # print("positions:  ", surface_pos)
-        surface_size = scene_data['sizes'][env_idx_tensor, target_surface_idx]
-        
-        # Собираем размеры размещаемых объектов
-        obj_size = scene_data['sizes'][env_idx_tensor, obj_indices_to_place]
-
-        # 4. Вычисляем новые позиции объектов
-        rand_xy = torch.zeros(num_envs_to_process, num_to_place, 2, device=self.device)
-        # print("rand_xy:  ", rand_xy)
-        if mess:
-            print("mess")
-            # Генерируем случайные смещения сразу для всех
-            margin_tensor = torch.tensor([self.margin * 2, self.margin * 2], device=self.device)
-            max_offsets = (surface_size[..., :2] - margin_tensor)
-            rand_xy = (torch.rand_like(rand_xy) - 0.5) * max_offsets
-        
-        pos_z = surface_pos[..., 2] + surface_size[..., 2] + obj_size[..., 2] / 2.0
-        new_pos_xy = surface_pos[..., :2] + rand_xy
-        # print("new_pos_xy: ", new_pos_xy)
-        # 5. Обновляем глобальные тензоры
-        scene_data['positions'][env_idx_tensor, obj_indices_to_place] = torch.cat([new_pos_xy, pos_z.unsqueeze(-1)], dim=-1)
-        scene_data['active'][env_idx_tensor, obj_indices_to_place] = True
-        scene_data['on_surface_idx'][env_idx_tensor, obj_indices_to_place] = target_surface_idx
-        surface_levels = scene_data['surface_level'][env_idx_tensor, target_surface_idx]
-        scene_data['surface_level'][env_idx_tensor, obj_indices_to_place] = surface_levels + 1
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies_for_pg.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies_for_pg.py
deleted file mode 100644
index 9b450a2358..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies_for_pg.py
+++ /dev/null
@@ -1,80 +0,0 @@
-from abc import ABC, abstractmethod
-import torch
-import random
-import importlib.util
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py"
-ObstacleGraph = import_class_from_path(module_path, "ObstacleGraph")
-
-class PlacementStrategy(ABC):
-    def __init__(self, device: str, **kwargs):
-        self.device = device
-
-    @abstractmethod
-    def apply(self, graph: ObstacleGraph, object_indices: list[int], num_to_place: int, mess: bool):
-        pass
-
-class GridPlacement(PlacementStrategy):
-    def __init__(self, device: str, grid_coordinates: list[list[float]]):
-        super().__init__(device)
-        self.grid = torch.tensor(grid_coordinates, device=self.device, dtype=torch.float32)
-
-    def apply(self, graph: ObstacleGraph, object_indices: list[int], num_to_place: int, mess: bool):
-        num_to_place = min(num_to_place, len(object_indices), len(self.grid))
-        if num_to_place == 0: return
-
-        active_indices = random.sample(object_indices, num_to_place)
-        pos_indices = torch.randperm(len(self.grid), device=self.device)[:num_to_place]
-
-        graph.positions[active_indices] = self.grid[pos_indices]
-        graph.active[active_indices] = True
-        graph.on_surface_idx[active_indices] = -1 # Размещение на сетке = на полу
-        graph.surface_level[active_indices] = 0
-
-class OnSurfacePlacement(PlacementStrategy):
-    def __init__(self, device: str, surface_types: list[str], margin: float):
-        super().__init__(device)
-        self.surface_types = set(surface_types)
-        self.margin = margin
-
-    def apply(self, graph: ObstacleGraph, object_indices: list[int], num_to_place: int, mess: bool):
-        num_to_place = min(num_to_place, len(object_indices))
-        if num_to_place == 0: return
-        
-        # Находим все активные поверхности нужного типа
-        available_surfaces = [
-            i for i, data in graph.graph.nodes(data=True)
-            if data['active'] and not self.surface_types.isdisjoint(data['types'])
-        ]
-        if not available_surfaces: return
-
-        active_indices = random.sample(object_indices, num_to_place)
-        for i, obj_idx in enumerate(active_indices):
-            # Выбираем поверхность для размещения
-            if mess:
-                target_surface_idx = random.choice(available_surfaces)
-            else: # В режиме не-mess, распределяем по одному на поверхность
-                target_surface_idx = available_surfaces[i % len(available_surfaces)]
-            surface_pos = graph.positions[target_surface_idx]
-            surface_size = graph.sizes[target_surface_idx]
-            # Размещаем либо случайно на поверхности (mess), либо в центре
-            if mess:
-                rand_x = (torch.rand(1).item() - 0.5) * (surface_size[0] - self.margin * 2)
-                rand_y = (torch.rand(1).item() - 0.5) * (surface_size[1] - self.margin * 2)
-            else:
-                rand_x, rand_y = 0.0, 0.0
-            
-            pos_z = surface_pos[2] + surface_size[2] + graph.sizes[obj_idx][2] / 2.0
-            graph.positions[obj_idx] = torch.tensor([surface_pos[0] + rand_x, surface_pos[1] + rand_y, pos_z], device=self.device)
-            # Устанавливаем иерархию
-            graph.active[obj_idx] = True
-            graph.on_surface_idx[obj_idx] = target_surface_idx
-            graph.surface_level[obj_idx] = graph.surface_level[target_surface_idx] + 1
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items copy.json b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items copy.json
deleted file mode 100644
index 3d166e613e..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items copy.json	
+++ /dev/null
@@ -1,95 +0,0 @@
-{
-  "objects": [
-    {
-      "name": "table",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["static_obstacle", "surface_provider", "changeable_color"],
-      "count": 6,
-      "size": [0.5, 0.5, 0.75],
-      "usd_paths": ["scenes/scenes_sber_kitchen_for_BBQ/table/table.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-4.5, -1.1, 0.0],
-            [-4.5, 0.0, 0.0],
-            [-4.5, 1.1, 0.0],
-            [4.5, -1.1, 0.0],
-            [4.5, 0.0, 0.0],
-            [4.5, 1.1, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "chair",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["movable_obstacle"],
-      "count": 6,
-      "size": [0.4, 0.4, 0.9],
-      "usd_paths": ["scenes/obstacles/chair2.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-2.2, -1.1, 0.0],
-            [-2.2, 0.0, 0.0],
-            [-2.2, 1.1, 0.0],
-            [2.2, -1.1, 0.0],
-            [2.2, 0.0, 0.0],
-            [2.2, 1.1, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "bowl",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["possible_goal", "surface_only"],
-      "count": 1,
-      "size": [0.15, 0.15, 0.05],
-      "usd_paths": ["objects/bowl.usd"],
-      "placement": [
-        {
-          "strategy": "on_surface",
-          "surface_types": ["surface_provider"],
-          "margin": 0.1
-        }
-      ]
-    },
-    {
-      "name": "wall",
-      "info": {
-          "color": "red",
-          "add_info": ["surface_provider"],
-          "position": [-5.0, 3, 1.0]
-        },
-      "type": ["info"],
-      "count": 1,
-      "size": [0.1, 6, 2]
-    },
-    {
-      "name": "wall",
-      "info": {
-          "color": "green",
-          "add_info": ["surface_provider"],
-          "position": [5.0, 3, 1.0]
-        },
-      "type": ["info"],
-      "count": 1,
-      "size": [0.1, 6, 2]
-    }
-  ]
-}
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json
deleted file mode 100644
index 790ca0f334..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items.json
+++ /dev/null
@@ -1,92 +0,0 @@
-{
-  "objects": [
-    {
-      "name": "table",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["static_obstacle", "surface_provider", "changeable_color"],
-      "count": 3,
-      "size": [0.5, 0.5, 0.75],
-      "usd_paths": ["scenes/scenes_sber_kitchen_for_BBQ/table/table.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-4.5, -1.1, 0.0],
-            [-4.5, 0.0, 0.0],
-            [-4.5, 1.1, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "chair",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["movable_obstacle"],
-      "count": 3,
-      "size": [0.4, 0.4, 0.9],
-      "usd_paths": ["scenes/obstacles/chair2.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-2.2, -1.1, 0.0],
-            [-2.2, 0.0, 0.0],
-            [-2.2, 1.1, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "cabinet",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["staff_obstacle"],
-      "count": 3,
-      "size": [0.4, 0.4, 0.9],
-      "usd_paths": ["scenes/obstacles/cabinet.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-0.5, -3.2, 0.0],
-            [-2, -3.2, 0.0],
-            [-3.5, -3.2, 0.0],
-            [-0.5, 3.2, 0.0],
-            [-2, 3.2, 0.0],
-            [-3.5, 3.2, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "bowl",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["possible_goal", "surface_only"],
-      "count": 1,
-      "size": [0.15, 0.15, 0.05],
-      "usd_paths": ["objects/bowl.usd"],
-      "placement": [
-        {
-          "strategy": "on_surface",
-          "surface_types": ["surface_provider"],
-          "margin": 0.1
-        }
-      ]
-    }
-  ]
-}
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items_cutted.json b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items_cutted.json
deleted file mode 100644
index b54b624b96..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_items_cutted.json
+++ /dev/null
@@ -1,86 +0,0 @@
-{
-  "objects": [
-    {
-      "name": "table",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["static_obstacle", "surface_provider", "changeable_color"],
-      "count": 3,
-      "size": [0.5, 0.5, 0.75],
-      "usd_paths": ["scenes/scenes_sber_kitchen_for_BBQ/table/table.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-4.5, 0.0, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "chair",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["movable_obstacle"],
-      "count": 3,
-      "size": [0.4, 0.4, 0.9],
-      "usd_paths": ["scenes/obstacles/chair2.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-2.2, -1.1, 0.0],
-            [-2.2, 0.0, 0.0],
-            [-2.2, 1.1, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "cabinet",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["staff_obstacle"],
-      "count": 3,
-      "size": [0.4, 0.4, 0.9],
-      "usd_paths": ["scenes/obstacles/cabinet.usd"],
-      "placement": [
-        {
-          "strategy": "grid",
-          "grid_coordinates": [
-            [-3.5, -3.2, 0.0],
-            [-3.5, 3.2, 0.0]
-          ]
-        }
-      ]
-    },
-    {
-      "name": "bowl",
-      "info": {
-          "color": "",
-          "add_info": [],
-          "position": []
-        },
-      "type": ["possible_goal", "surface_only"],
-      "count": 1,
-      "size": [0.15, 0.15, 0.05],
-      "usd_paths": ["objects/bowl.usd"],
-      "placement": [
-        {
-          "strategy": "on_surface",
-          "surface_types": ["surface_provider"],
-          "margin": 0.1
-        }
-      ]
-    }
-  ]
-}
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager copy.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager copy.py
deleted file mode 100644
index ae7ba61cec..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager copy.py	
+++ /dev/null
@@ -1,730 +0,0 @@
-import torch
-import math
-import random
-import json
-from collections import defaultdict
-from tabulate import tabulate
-import importlib.util
-# Импортируем обновленные, векторизованные стратегии
-# from .placement_strategies import PlacementStrategy, GridPlacement, OnSurfacePlacement # Эти классы остаются как в предыдущем ответе
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py"
-PlacementStrategy = import_class_from_path(module_path, "PlacementStrategy")
-GridPlacement = import_class_from_path(module_path, "GridPlacement")
-OnSurfacePlacement = import_class_from_path(module_path, "OnSurfacePlacement")
-
-class SceneManager:
-    def __init__(self, num_envs: int, config_path: str, device: str):
-        self.num_envs = num_envs
-        self.device = device
-        with open(config_path, 'r') as f:
-            self.config = json.load(f)['objects']
-        self.colors_dict = {
-                        "green": [0.0, 1.0, 0.0],
-                        "blue": [0.0, 0.0, 1.0],
-                        "yellow": [1.0, 1.0, 0.0],
-                        "gray": [0.5, 0.5, 0.5],
-                        "red": [1.0, 0.0, 0.0]
-                    }
-        # --- Начало: Векторизованная структура данных ---
-        self.num_total_objects = sum(obj['count'] for obj in self.config)
-        self.object_ids = torch.zeros(1, self.num_total_objects, device=self.device)
-        # Словари для быстрого доступа к метаданным
-        self.object_map = {} # {name: {'indices': tensor, 'types': set, 'count': int}}
-        self.type_map = defaultdict(list) # {type_str: [indices...]}
-        
-        # Глобальные тензоры состояний
-        self.positions = torch.zeros(self.num_envs, self.num_total_objects, 3, device=self.device)
-        self.sizes = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        self.radii = torch.zeros(1, self.num_total_objects, device=self.device)
-        self.colors = torch.ones(1, self.num_total_objects, 3, device=self.device)  # По умолчанию белый для не-changeable
-        self.names = []
-        self.active = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.bool, device=self.device)
-        self.on_surface_idx = torch.full((self.num_envs, self.num_total_objects), -1, dtype=torch.long, device=self.device)
-        self.surface_level = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.long, device=self.device)
-        
-        self._initialize_object_data()
-        self.default_positions = self.positions.clone()
-        # --- Конец: Векторизованная структура данных ---
-
-        self.placement_strategies = self._initialize_strategies()
-
-        self.robot_radius = 0.5
-        self.room_bounds = {'x_min': -4, 'x_max': 4.0, 'y_min': -2.7, 'y_max': 2.7}
-        self.goal_positions = torch.zeros((num_envs, 3), device=self.device)
-
-        n_angles = 36
-        angle_step = 2 * math.pi / n_angles
-        self.discrete_angles = torch.arange(0, 2 * math.pi, angle_step, device=self.device)
-        self.candidate_vectors = torch.stack([torch.cos(self.discrete_angles), torch.sin(self.discrete_angles)], dim=1)
-
-        self.clip_descriptors = {}
-        self.full_names = []
-        # Assign object IDs based on name
-
-    
-    def update_prims(self):
-        pass
-    
-    def get_scene_data_dict(self):
-        return {"positions": self.positions, "sizes": self.sizes.expand(self.num_envs, -1, -1), "radii": self.radii.expand(self.num_envs, -1), "active": self.active, "on_surface_idx": self.on_surface_idx, "surface_level": self.surface_level}
-    
-    def apply_fixed_positions(self, env_ids: torch.Tensor, positions_config: list[dict]):
-        """
-        positions_config: список словарей по числу сред.
-        Каждый словарь: { "chair": [[x,y,z], ...], "table": [...], ... }
-        """
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        scene_data = self.get_scene_data_dict()
-        for env_id in env_ids:
-            env_dict = positions_config[env_id.item()]
-            for obj_name, pos_list in env_dict.items():
-                if obj_name not in self.object_map:
-                    continue
-                indices = self.object_map[obj_name]["indices"]
-                # print(indices)
-                for i, pos in enumerate(pos_list):
-                    if i >= len(indices):
-                        print("errror")
-                        break
-                    scene_data["positions"][env_id.item(), indices[i]] = torch.tensor(pos, device=self.device)
-                    scene_data["active"][env_id.item(), indices[i]] = True
-                    scene_data["on_surface_idx"][env_id.item(), indices[i]] = -1
-                    scene_data["surface_level"][env_id.item(), indices[i]] = 0
-
-        # for i in env_ids:
-        #     self.print_graph_info(i)
-        self.chose_active_goal_state(env_ids)
-
-
-    def _initialize_object_data(self):
-        """Заполняет метаданные об объектах и их начальные/дефолтные состояния."""
-        start_idx = 0
-        
-        # Создаем временный тензор для дефолтных позиций
-        default_pos_tensor = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        
-        # --- Начало: Логика создания "кладбища" ---
-        graveyard_start_x = 0.0
-        graveyard_start_y = 6.0
-        spacing = 1.0 # Расстояние между объектами на кладбище
-        max_per_row = 5 # Сколько объектов в ряду на кладбище
-
-        for i in range(self.num_total_objects):
-            row = i // max_per_row
-            col = i % max_per_row
-            default_pos_tensor[0, i, 0] = graveyard_start_x + col * spacing
-            default_pos_tensor[0, i, 1] = graveyard_start_y + row * spacing
-            default_pos_tensor[0, i, 2] = 0.0
-        # --- Конец: Логика создания "кладбища" ---
-
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            count = obj_cfg['count']
-            indices = torch.arange(start_idx, start_idx + count, device=self.device, dtype=torch.long)
-            types = set(obj_cfg['type'])
-
-            if "changeable_color" in types:
-                colors_dict = self.colors_dict
-                color_names = list(colors_dict.keys())
-                for idx in indices:
-                    color_name = random.choice(color_names)
-                    self.colors[0, idx] = torch.tensor(colors_dict[color_name], device=self.device)
-
-            self.object_map[name] = {'indices': indices, 'types': types, 'count': count}
-            for type_str in types:
-                self.type_map[type_str].extend(indices.tolist())
-            
-            self.names.extend([f"{name}_{i}" for i in range(count)])
-            
-            size_tensor = torch.tensor(obj_cfg['size'], device=self.device)
-            self.sizes[0, indices] = size_tensor
-            self.radii[0, indices] = torch.norm(size_tensor[:2] / 2)
-            start_idx += count
-
-        for type_str, indices in self.type_map.items():
-            self.type_map[type_str] = torch.tensor(sorted(indices), device=self.device, dtype=torch.long)
-
-        # --- Исправленная последовательность ---
-        # 1. Присваиваем правильно созданные "кладбищенские" позиции
-        self.default_positions = default_pos_tensor.expand(self.num_envs, -1, -1)
-        id_map = {"table": 1, "bowl": 2, "chair": 3, "cabinet": 4}
-        for name, data in self.object_map.items():
-            obj_id = id_map.get(name, 0)  # Default to 0 for unmapped objects
-            self.object_ids[0, data['indices']] = obj_id
-        # 2. Инициализируем текущие позиции из дефолтных
-        self.positions = self.default_positions.clone()
-
-    def _initialize_strategies(self):
-        strategies = {}
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            print("name: ", name)
-            placement_cfg_list = obj_cfg.get('placement')
-            if not placement_cfg_list: continue
-            placement_cfg = placement_cfg_list[0]
-            strategy_type = placement_cfg['strategy']
-            if strategy_type == 'grid':
-                strategies[name] = GridPlacement(self.device, placement_cfg['grid_coordinates'])
-            elif strategy_type == 'on_surface':
-                surface_indices = self.type_map.get(placement_cfg['surface_types'][0], torch.tensor([], dtype=torch.long))
-                strategies[name] = OnSurfacePlacement(self.device, surface_indices.tolist(), placement_cfg['margin'])
-        return strategies
-
-    def randomize_scene(self, env_ids: torch.Tensor, mess: bool = False, use_obstacles: bool = False, all_defoult: bool = False):
-        """Абстрактная, векторизованная рандомизация сцены на основе ТИПОВ объектов."""
-        num_to_randomize = len(env_ids)
-        
-        # 1. Сброс состояния
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        if all_defoult:
-            return
-
-        # 2. Определение количества объектов для размещения по типам
-        num_surface_only = len(self.type_map.get("surface_only", []))
-        num_providers = len(self.type_map.get("surface_provider", []))
-        num_floor_obs = len(self.type_map.get("movable_obstacle", [])) - num_surface_only
-        num_static_floor_obs = len(self.type_map.get("staff_obstacle", [])) - num_surface_only
-
-        num_surface_only_to_place = torch.randint(1, num_surface_only + 1, (num_to_randomize,), device=self.device) if num_surface_only > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        
-        if num_providers > 0:
-            # Нижняя граница: минимум 1, и не меньше чем количество surface_only объектов.
-            low_bound = torch.max(
-                torch.tensor(1, device=self.device), 
-                num_surface_only_to_place
-            )
-            # Верхняя граница (исключающая для генерации)
-            high_bound = num_providers + 1
-            
-            # Генерируем случайные числа с плавающей точкой и масштабируем их до нужного диапазона
-            rand_float = torch.rand(num_to_randomize, device=self.device)
-            num_providers_to_place = (low_bound + rand_float * (high_bound - low_bound)).long()
-        else:
-            num_providers_to_place = torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_floor_obstacles_to_place = torch.randint(0, num_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_static_floor_obstacles_to_place = torch.randint(0, num_static_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_static_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-
-        # 3. Применение стратегий в правильном порядке (сначала поверхности)
-        scene_data = self.get_scene_data_dict()
-        placement_order = ["surface_provider", "movable_obstacle", "surface_only", "staff_obstacle"]
-        
-        # Определяем, сколько объектов какого типа нужно разместить
-        counts_by_type = {
-            "surface_provider": num_providers_to_place,
-            "movable_obstacle": num_floor_obstacles_to_place,
-            "surface_only": num_surface_only_to_place,
-            "staff_obstacle": num_static_floor_obstacles_to_place,
-        }
-        
-        for p_type in placement_order:
-            # Находим все объекты, имеющие данный тип
-            for name, data in self.object_map.items():
-                if p_type in data['types'] and name in self.placement_strategies:
-                    # Некоторые типы могут иметь несколько ролей, например, movable_obstacle может быть не surface_only
-                    # Проверяем, что мы еще не разместили этот объект в рамках другой роли
-                    if p_type == "movable_obstacle" and "surface_only" in data['types']:
-                        continue
-
-                    obj_indices = data['indices']
-                    num_to_place_per_env = counts_by_type.get(p_type, torch.zeros(num_to_randomize, dtype=torch.long))
-
-                    # Векторизованный выбор случайных экземпляров
-                    rand_indices = torch.rand(num_to_randomize, len(obj_indices), device=self.device).argsort(dim=1)
-                    
-                    max_num_to_place = int(num_to_place_per_env.max())
-                    if max_num_to_place == 0: continue
-                    
-                    indices_to_place = obj_indices[rand_indices[:, :max_num_to_place]]
-
-                    # Маска, чтобы применять стратегию только к тем env, где нужно разместить > 0 объектов
-                    valid_envs_mask = num_to_place_per_env > 0
-                    
-                    # Фильтруем env_ids и indices_to_place
-                    active_env_ids = env_ids[valid_envs_mask]
-                    if len(active_env_ids) == 0: continue
-
-                    active_indices_to_place = indices_to_place[valid_envs_mask]
-                    # print("active_indices_to_place ", active_indices_to_place)
-                    self.placement_strategies[name].apply(active_env_ids, active_indices_to_place, scene_data, mess)
-
-        self.chose_active_goal_state(env_ids)
-
-    def get_active_obstacle_positions_for_path_planning(self, env_ids: torch.Tensor) -> list:
-        """
-        Возвращает позиции активных препятствий в формате списка списков,
-        специально для генерации строкового ключа в path_manager.
-        """
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return [[] for _ in env_ids]
-            
-        active_mask = self.active[env_ids][:, obs_indices] # (num_envs, num_obstacles)
-        positions = self.positions[env_ids][:, obs_indices].cpu().numpy() # (num_envs, num_obstacles, 3)
-        
-        output_list = []
-        for i in range(len(env_ids)):
-            # Выбираем только активные позиции для i-й среды
-            active_positions = positions[i, active_mask[i].cpu().numpy()]
-            # Округляем и сортируем для консистентности ключа
-            rounded_pos = [(round(p[0], 1), round(p[1], 1), round(p[2], 1)) for p in active_positions]
-            output_list.append(sorted(rounded_pos))
-            
-        return output_list
-
-    def get_graph_embedding(self, env_ids: torch.Tensor) -> torch.Tensor:
-        """Создает тензорный эмбеддинг фиксированного размера для текущего состояния сцены."""
-        # [is_active, pos_x, pos_y, pos_z, size_x, size_y, size_z, radius, object_id]
-        # Размер фичи: 1 + 3 + 3 + 1 + 1 = 9
-        num_features = 9
-        embedding = torch.zeros(len(env_ids), self.num_total_objects, num_features, device=self.device)
-        # print("bbbb ", len(embedding[0]))
-        env_positions = self.positions[env_ids] + 5
-        env_active = self.active[env_ids].float().unsqueeze(-1)
-        env_sizes = self.sizes.expand(len(env_ids), -1, -1)
-        env_radii = self.radii.expand(len(env_ids), -1).unsqueeze(-1)
-        env_object_ids = self.object_ids.expand(len(env_ids), -1).unsqueeze(-1)
-
-        embedding[..., 0:1] = env_active
-        embedding[..., 1:4] = env_positions * env_active
-        embedding[..., 4:7] = env_sizes * env_active
-        embedding[..., 7:8] = env_radii * env_active
-        embedding[..., 8:9] = env_object_ids * env_active
-
-        # Нормализация для лучшего обучения (применяется ко всем, но неактивные останутся 0)
-        embedding[..., 1:4] /= 5.0  # Делим позиции на примерный масштаб комнаты
-        embedding[..., 4:7] /= 1.0  # Размеры уже примерно в этом диапазоне
-        embedding[..., 7:8] /= 2.0  # Радиусы
-        embedding[..., 8:9] /= 3.0  # Нормализация ID (максимум 3 для chair)
-        # Возвращаем "плоский" тензор
-        return embedding.view(len(env_ids), -1)
-
-    def print_graph_info(self, env_id: int):
-        """Печатает детальную информацию о сцене для ОДНОГО окружения."""
-        print(f"\n=== Scene Information (Env ID: {env_id}) ===")
-        
-        # Данные для указанного env_id
-        positions = self.positions[env_id]
-        active_states = self.active[env_id]
-        surface_indices = self.on_surface_idx[env_id]
-        surface_levels = self.surface_level[env_id]
-        
-        table_data = []
-        for i in range(self.num_total_objects):
-            name = self.names[i]
-            pos = positions[i]
-            # Ищем типы по индексу
-            types = ", ".join([t for t, inds in self.type_map.items() if i in inds])
-
-            row = [
-                i, name, types,
-                f"({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})",
-                f"{self.radii[0, i]:.2f}",
-                str(active_states[i].item()),
-                surface_indices[i].item(),
-                surface_levels[i].item()
-            ]
-            table_data.append(row)
-            
-        headers = ["ID", "Name", "Types", "Position", "Radius", "Active", "On Surface", "Surface Level"]
-        print(tabulate(table_data, headers=headers, tablefmt="grid"))
-    
-    def chose_active_goal_state(self, env_ids: torch.Tensor):
-        goal_indices = self.type_map.get("possible_goal", torch.tensor([], dtype=torch.long))
-        if len(goal_indices) == 0:
-            print(f"[WARNING] No objects of type 'possible_goal' found in config.")
-            self.goal_positions[env_ids] = torch.tensor([-4.5, 0.0, 0.75], device=self.device)
-            return
-        
-        active_goal_mask = self.active[env_ids][:, goal_indices].float()
-        
-        # Fallback если ни одна цель не активна
-        any_active = active_goal_mask.sum(dim=1) > 0
-        if not all(any_active):
-            print("NO GOAL", any_active)
-            # # Для env где нет активных целей, активируем первую попавшуюся
-            # fallback_mask = ~any_active
-            # active_goal_mask[fallback_mask, 0] = 1.0
-
-        chosen_goal_rel_idx = torch.multinomial(active_goal_mask + 1e-9, 1).squeeze(-1)
-        chosen_goal_idx = goal_indices[chosen_goal_rel_idx]
-        
-        env_indices = env_ids
-        self.goal_positions[env_indices] = self.positions[env_indices, chosen_goal_idx]
-
-    def get_active_goal_state(self, env_ids: torch.Tensor):
-        return self.goal_positions[env_ids]
-
-    def place_robot_for_goal(self, env_ids: torch.Tensor, mean_dist: float, min_dist: float, max_dist: float, angle_error: float):
-        """Размещает робота относительно цели, избегая препятствий и границ."""
-        # Этап 1: Получение числа сред
-        num_envs = len(env_ids)
-
-        # Этап 2: Извлечение позиций целей
-        goal_pos = self.goal_positions[env_ids]
-
-        # Этап 3: Определение активных препятствий на полу
-        is_floor_obstacle = (self.active[env_ids] == True) & (self.on_surface_idx[env_ids] == -1)
-
-        # Этап 4: Извлечение позиций и радиусов препятствий
-        obstacle_pos_all = self.positions[env_ids, :, :2].clone()
-
-        obstacle_radii_all = self.radii.expand(self.num_envs, -1)[env_ids]
-        # Этап 5: Фильтрация неактивных препятствий
-        inf_pos = torch.full_like(obstacle_pos_all, 999.0)
-
-        obstacle_pos = torch.where(is_floor_obstacle.unsqueeze(-1), obstacle_pos_all, inf_pos)
-        # Этап 6: Генерация радиусов для размещения робота
-        mean_dist_with_shift = mean_dist + 1.31
-        radii = torch.normal(mean=mean_dist_with_shift, std=mean_dist * 0.1, size=(num_envs, 1), device=self.device).clamp_(min_dist, max_dist)
-        # Этап 7: Генерация кандидатов для позиций робота
-        candidates = goal_pos[:, None, :2] + radii.unsqueeze(1) * self.candidate_vectors
-        # Этап 8: Проверка границ комнаты
-        # Этап 8: Проверка границ комнаты (только границы, без коллизий)
-        bounds = self.room_bounds
-        in_bounds_mask = (
-            (candidates[..., 0] >= bounds['x_min'] + self.robot_radius) &
-            (candidates[..., 0] <= bounds['x_max'] - self.robot_radius) &
-            (candidates[..., 1] >= bounds['y_min'] + self.robot_radius) &
-            (candidates[..., 1] <= bounds['y_max'] - self.robot_radius)
-        )
-        # print(candidates)
-        # print(in_bounds_mask)
-        # Этап 9: Выбор углов только по границам
-        in_bounds_mask_float = in_bounds_mask.float() + 1e-9
-        chosen_angle_idx = torch.multinomial(in_bounds_mask_float, 1).squeeze(-1)
-        # print(chosen_angle_idx)
-        # Этап 10: Выбор финальных позиций робота
-        batch_indices = torch.arange(num_envs, device=self.device)
-        final_robot_positions = candidates[batch_indices, chosen_angle_idx]
-
-        # Этап 11: fallback если ни одна позиция не в границах
-        no_valid_pos_mask = ~in_bounds_mask.any(dim=1)
-        if torch.any(no_valid_pos_mask):
-            fallback_pos = goal_pos[:, :2] + torch.tensor([max_dist, 0.0], device=self.device) # 0.0!
-            final_robot_positions[no_valid_pos_mask] = fallback_pos[no_valid_pos_mask]
-
-        # Этап 15: Вычисление ориентации робота (yaw)
-        direction_to_goal = goal_pos[:, :2] - final_robot_positions
-        base_yaw = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-        error = (torch.rand(num_envs, device=self.device) - 0.5) * 2 * angle_error
-        final_yaw = base_yaw + error
-        # Этап 16: Формирование кватернионов ориентации
-        robot_quats = torch.zeros(num_envs, 4, device=self.device)
-        robot_quats[:, 0] = torch.cos(final_yaw / 2.0)
-        robot_quats[:, 3] = torch.sin(final_yaw / 2.0)
-        # Этап 17: Возврат результатов
-        # Проверяем пересечения с препятствиями
-        self.remove_colliding_obstacles(env_ids, final_robot_positions)
-
-        return final_robot_positions, robot_quats
-    
-
-    def remove_colliding_obstacles(self, env_ids: torch.Tensor, robot_positions: torch.Tensor):
-        """Ставит в дефолт все препятствия, пересекающиеся с роботом."""
-        # TODO There can be obstacles with suface providing and we should delete alse items on that
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return
-
-        # позиции и радиусы препятствий
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r + 0.2)
-        if coll_mask.any():
-            # print("coll_mask: ",  coll_mask)
-            # for i in env_ids:
-            #     self.print_graph_info(i)
-            # переносим такие препятствия в дефолт
-            default_pos = self.default_positions[env_ids][:, obs_indices]
-            batch_idx, obs_idx = torch.where(coll_mask)                 # индексы элементов с коллизией
-            env_batch_idx = env_ids[batch_idx]                           # индексы env_ids для batch
-            obs_indices_sel = obs_indices[obs_idx]                       # индексы obstacles
-
-            # Присваиваем значения дефолтных позиций в исходный тензор
-            self.positions[env_batch_idx, obs_indices_sel] = default_pos[batch_idx, obs_idx]
-
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-            # print(default_pos[coll_mask])
-            
-            # self.positions[env_ids][:, obs_indices][coll_mask] = default_pos[coll_mask]
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-
-            # деактивируем их
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-            self.active[env_batch_idx, obs_indices_sel] = False
-
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r)
-        if coll_mask.any():
-            
-            # print("coll_mask 2: ",  coll_mask)
-            
-            for i in env_ids:
-                self.print_graph_info(i)
-    
-    def init_graph_descriptor(self, clip_processor, clip_model):
-        names = ["bowl", "cabinet", "chair", "table"]
-        i = 0.0
-        for name in names:
-            i += 1
-            self.clip_descriptors[name] = i
-
-        print(f"[ info ] Inited clip_descriptors: {self.clip_descriptors} and full_name: {self.full_names}")
-
-    def get_graph_obs(self, env_ids=None) -> list:
-        """Returns a list of length num_envs, where each element is a list of node dictionaries representing the scene graph.
-        
-        Each node dict has format:
-        {"node_id": int, "clip_descriptor": list[float] (512), "edges_vl_sat": list[dict], "bbox_center": list[float] (3), "bbox_extent": list[float] (3), "class_name": str}
-        
-        edges_vl_sat contains dicts for relations to other nodes: {"id_1": int, "class_name_1": str, "rel_id": int, "id_2": int}
-        Relations are positional only, based on main difference in x/y/z axes from viewpoint (0,0 facing -y).
-        """
-        if env_ids is None:
-            env_ids = torch.arange(self.num_envs, device=self.device)
-        
-        output = []
-        for eid in env_ids:
-            active_mask = self.active[eid]
-            active_indices = torch.nonzero(active_mask).squeeze(-1)
-            num_active = len(active_indices)
-            
-            if num_active == 0:
-                output.append([])
-                continue
-            
-            # Map global indices to local node_ids (0 to num_active-1)
-            node_id_map = {int(active_indices[j]): j for j in range(num_active)}
-            
-            # Gather data for active objects
-            active_pos = self.positions[eid, active_indices]
-            active_sizes = self.sizes[0, active_indices]
-            active_names = [self.names[int(idx)] for idx in active_indices]
-            # Fetch CLIP descriptors based on names
-            active_clip = torch.stack([self.clip_descriptors[name.split('_')[0]] for name in active_names]).to(self.device)
-            
-            env_graph = []
-            for j in range(num_active):
-                idx = active_indices[j]
-                node_id = j
-                clip_desc = active_clip[j].tolist()
-                bbox_center = active_pos[j].tolist()
-                bbox_extent = active_sizes[j].tolist()
-                name = active_names[j]
-                
-                edges = []
-                for k in range(num_active):
-                    if k == j:
-                        continue
-                    
-                    diff = active_pos[k] - active_pos[j]
-                    abs_diff = torch.abs(diff)
-                    max_idx = abs_diff.argmax().item()
-                    
-                    if max_idx == 0:  # x (left/right; assume +x is right)
-                        rel_id = 19 if diff[0] > 0 else 14  # right / left
-                    elif max_idx == 1:  # y (front/behind; facing -y, +y behind, -y front)
-                        rel_id = 1 if diff[1] > 0 else 8  # behind / front
-                    elif max_idx == 2:  # z (higher/lower)
-                        rel_id = 11 if diff[2] > 0 else 15  # higher than / lower than
-                    
-                    edge_dict = {
-                        "id_1": node_id,
-                        "rel_id": rel_id,
-                        "id_2": k
-                    }
-                    edges.append(edge_dict)
-                
-                node_dict = {
-                    "node_id": node_id,
-                    "clip_descriptor": clip_desc,
-                    "edges_vl_sat": edges,
-                    "bbox_center": bbox_center,
-                    "bbox_extent": bbox_extent,
-                }
-                env_graph.append(node_dict)
-            
-            # Выполняем вытеснение неактивных объектов: заполняем до self.num_total_objects копиями случайных активных объектов
-            while len(env_graph) < self.num_total_objects and num_active > 0:
-                new_id = len(env_graph)
-                original_j = random.randint(0, num_active - 1)  # Выбираем из оригинальных активных
-                original_node = env_graph[original_j]
-                
-                new_node = original_node.copy()
-                new_node["node_id"] = new_id
-                new_node["edges_vl_sat"] = []
-                
-                # Добавляем edges между существующими нодами и новой
-                for existing_id in range(new_id):
-                    # Находим edge от existing к original_j и копируем для existing к new_id
-                    for edge in env_graph[existing_id]["edges_vl_sat"]:
-                        if edge["id_2"] == original_j:
-                            new_edge = edge.copy()
-                            new_edge["id_2"] = new_id
-                            env_graph[existing_id]["edges_vl_sat"].append(new_edge)
-                            break
-                    
-                    # Находим edge от original_j к existing и копируем для new_id к existing
-                    for edge in original_node["edges_vl_sat"]:
-                        if edge["id_2"] == existing_id:
-                            new_edge = edge.copy()
-                            new_edge["id_1"] = new_id
-                            new_node["edges_vl_sat"].append(new_edge)
-                            break
-                
-                env_graph.append(new_node)
-            
-            output.append(env_graph)
-        
-        return output
-
-    def add_noise_to_graph_obs(self, graph_obs_list: list[list[dict]]) -> list[list[dict]]:
-        """Adds noise to the graph observations: small perturbations to centers and extents mostly,
-        larger rarely (via normal dist with occasional outliers), and rarely change some edge rel_ids.
-        
-        Operates vectorized internally for efficiency.
-        """
-        num_envs = len(graph_obs_list)
-        if num_envs == 0:
-            return graph_obs_list
-        
-        # Find max_nodes across envs
-        max_nodes = self.num_total_objects
-        
-        # Tensors for centers, extents, rel_ids
-        centers = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        extents = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        rel_ids = torch.full((num_envs, max_nodes, max_nodes), -1, dtype=torch.long, device=self.device)  # -1 invalid
-        
-        # Fill tensors from list
-        for e, graph in enumerate(graph_obs_list):
-            n = len(graph)
-            if n == 0:
-                continue
-            for j, node in enumerate(graph):
-                centers[e, j] = torch.tensor(node["bbox_center"], device=self.device)
-                extents[e, j] = torch.tensor(node["bbox_extent"], device=self.device)
-                for edge in node["edges_vl_sat"]:
-                    k = edge["id_2"]
-                    rel_ids[e, j, k] = edge["rel_id"]
-        
-        # Add noise to centers: normal dist, sigma=0.05 mostly, but with 10% chance sigma=0.2, 1% sigma=1.0
-        noise_levels = torch.rand(num_envs, max_nodes, 1, device=self.device)
-        sigmas = torch.where(noise_levels < 0.01, 1.0, torch.where(noise_levels < 0.1, 0.2, 0.05))
-        center_noise = torch.randn(num_envs, max_nodes, 3, device=self.device) * sigmas
-        centers += center_noise
-        
-        # Add noise to extents: multiplicative, normal around 1, same sigma logic
-        extent_factors = 1 + torch.randn(num_envs, max_nodes, 3, device=self.device) * sigmas
-        extent_factors.clamp_(0.5, 2.0)  # Prevent negative or extreme
-        extents *= extent_factors
-        
-        # Rarely change rel_ids: per edge with prob 0.01, set to random rel_id from {1, 8, 11, 14, 15, 19}
-        valid_rel_ids = torch.tensor([1, 8, 11, 14, 15, 19], device=self.device)  # Only positional relations
-        change_mask = (rel_ids >= 0) & (torch.rand_like(rel_ids.float()) < 0.01)
-        random_indices = torch.randint(0, len(valid_rel_ids), change_mask.shape, device=self.device)
-        new_rels = valid_rel_ids[random_indices]
-        rel_ids[change_mask] = new_rels[change_mask]
-        
-        # Convert back to list[list[dict]]
-        noisy_graph_obs = []
-        for e in range(num_envs):
-            n = len(graph_obs_list[e])
-            if n == 0:
-                noisy_graph_obs.append([])
-                continue
-            env_graph = []
-            for j in range(n):
-                node = graph_obs_list[e][j].copy()  # Copy original
-                node["bbox_center"] = centers[e, j].tolist()
-                node["bbox_extent"] = extents[e, j].tolist()
-                edges = []
-                for k in range(n):
-                    if k == j:
-                        continue
-                    rel_id = int(rel_ids[e, j, k])
-                    if rel_id < 0:
-                        continue
-                    edge_dict = {
-                        "id_1": j,
-                        "class_name_1": node["class_name"],
-                        "rel_id": rel_id,
-                        "id_2": k
-                    }
-                    edges.append(edge_dict)
-                node["edges_vl_sat"] = edges
-                env_graph.append(node)
-            noisy_graph_obs.append(env_graph)
-        
-        return noisy_graph_obs
-
-    def tensorize_graph_obs(self, graph_obs_list: list[list[dict]]) -> dict[str, torch.Tensor]:
-        """
-        Converts a list of scene graphs (one per environment) into a dictionary of batched tensors.
-        ф
-        Args:
-            graph_obs_list: List of length num_envs, each element is a list of node dictionaries for that env.
-        
-        Returns:
-            Dict with keys: 'node_clip' (num_envs, max_nodes, 512),
-                            'node_center' (num_envs, max_nodes, 3),
-                            'node_extent' (num_envs, max_nodes, 3),
-                            'rel_ids' (num_envs, max_nodes, max_nodes, long)
-        """
-        num_envs = len(graph_obs_list)
-        max_nodes = self.num_total_objects
-        clip_dim = 512
-        
-        node_clip = torch.zeros(num_envs, max_nodes, clip_dim, device=self.device)
-        node_center = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        node_extent = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        rel_ids = torch.zeros(num_envs, max_nodes, max_nodes, dtype=torch.long, device=self.device)
-        
-        for e in range(num_envs):
-            graph = graph_obs_list[e]
-            n = len(graph)
-            if n == 0:
-                continue
-            
-            for j in range(n):
-                node = graph[j]
-                node_clip[e, j] = torch.tensor(node["clip_descriptor"], device=self.device)
-                node_center[e, j] = torch.tensor(node["bbox_center"], device=self.device)
-                node_extent[e, j] = torch.tensor(node["bbox_extent"], device=self.device)
-                
-                for edge in node["edges_vl_sat"]:
-                    k = edge["id_2"]
-                    rel_id = edge["rel_id"]
-                    rel_ids[e, j, k] = rel_id
-        return {
-            "node_clip": node_clip,
-            "node_center": node_center,
-            "node_extent": node_extent,
-            "rel_ids": rel_ids
-        }
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager.py
deleted file mode 100644
index aff63acaf1..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager.py
+++ /dev/null
@@ -1,729 +0,0 @@
-import torch
-import math
-import random
-import json
-from collections import defaultdict
-from tabulate import tabulate
-import importlib.util
-# Импортируем обновленные, векторизованные стратегии
-# from .placement_strategies import PlacementStrategy, GridPlacement, OnSurfacePlacement # Эти классы остаются как в предыдущем ответе
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py"
-PlacementStrategy = import_class_from_path(module_path, "PlacementStrategy")
-GridPlacement = import_class_from_path(module_path, "GridPlacement")
-OnSurfacePlacement = import_class_from_path(module_path, "OnSurfacePlacement")
-
-class SceneManager:
-    def __init__(self, num_envs: int, config_path: str, device: str):
-        self.num_envs = num_envs
-        self.device = device
-        with open(config_path, 'r') as f:
-            self.config = json.load(f)['objects']
-        self.colors_dict = {
-                        "green": [0.0, 1.0, 0.0],
-                        "blue": [0.0, 0.0, 1.0],
-                        "yellow": [1.0, 1.0, 0.0],
-                        "gray": [0.5, 0.5, 0.5],
-                        "red": [1.0, 0.0, 0.0]
-                    }
-        # --- Начало: Векторизованная структура данных ---
-        self.num_total_objects = sum(obj['count'] for obj in self.config)
-        self.object_ids = torch.zeros(1, self.num_total_objects, device=self.device)
-        # Словари для быстрого доступа к метаданным
-        self.object_map = {} # {name: {'indices': tensor, 'types': set, 'count': int}}
-        self.type_map = defaultdict(list) # {type_str: [indices...]}
-        
-        # Глобальные тензоры состояний
-        self.positions = torch.zeros(self.num_envs, self.num_total_objects, 3, device=self.device)
-        self.sizes = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        self.radii = torch.zeros(1, self.num_total_objects, device=self.device)
-        self.colors = torch.ones(1, self.num_total_objects, 3, device=self.device)  # По умолчанию белый для не-changeable
-        self.names = []
-        self.active = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.bool, device=self.device)
-        self.on_surface_idx = torch.full((self.num_envs, self.num_total_objects), -1, dtype=torch.long, device=self.device)
-        self.surface_level = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.long, device=self.device)
-        
-        self._initialize_object_data()
-        self.default_positions = self.positions.clone()
-        # --- Конец: Векторизованная структура данных ---
-
-        self.placement_strategies = self._initialize_strategies()
-
-        self.robot_radius = 0.5
-        self.room_bounds = {'x_min': -4, 'x_max': 4.0, 'y_min': -2.7, 'y_max': 2.7}
-        self.goal_positions = torch.zeros((num_envs, 3), device=self.device)
-
-        n_angles = 36
-        angle_step = 2 * math.pi / n_angles
-        self.discrete_angles = torch.arange(0, 2 * math.pi, angle_step, device=self.device)
-        self.candidate_vectors = torch.stack([torch.cos(self.discrete_angles), torch.sin(self.discrete_angles)], dim=1)
-
-        self.clip_descriptors = {}
-        self.full_names = []
-        # Assign object IDs based on name
-
-    
-    def update_prims(self):
-        pass
-    
-    def get_scene_data_dict(self):
-        return {"positions": self.positions, "sizes": self.sizes.expand(self.num_envs, -1, -1), "radii": self.radii.expand(self.num_envs, -1), "active": self.active, "on_surface_idx": self.on_surface_idx, "surface_level": self.surface_level}
-    
-    def apply_fixed_positions(self, env_ids: torch.Tensor, positions_config: list[dict]):
-        """
-        positions_config: список словарей по числу сред.
-        Каждый словарь: { "chair": [[x,y,z], ...], "table": [...], ... }
-        """
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        scene_data = self.get_scene_data_dict()
-        for env_id in env_ids:
-            env_dict = positions_config[env_id.item()]
-            for obj_name, pos_list in env_dict.items():
-                if obj_name not in self.object_map:
-                    continue
-                indices = self.object_map[obj_name]["indices"]
-                # print(indices)
-                for i, pos in enumerate(pos_list):
-                    if i >= len(indices):
-                        print("errror")
-                        break
-                    scene_data["positions"][env_id.item(), indices[i]] = torch.tensor(pos, device=self.device)
-                    scene_data["active"][env_id.item(), indices[i]] = True
-                    scene_data["on_surface_idx"][env_id.item(), indices[i]] = -1
-                    scene_data["surface_level"][env_id.item(), indices[i]] = 0
-
-        # for i in env_ids:
-        #     self.print_graph_info(i)
-        self.chose_active_goal_state(env_ids)
-
-
-    def _initialize_object_data(self):
-        """Заполняет метаданные об объектах и их начальные/дефолтные состояния."""
-        start_idx = 0
-        
-        # Создаем временный тензор для дефолтных позиций
-        default_pos_tensor = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        
-        # --- Начало: Логика создания "кладбища" ---
-        graveyard_start_x = 0.0
-        graveyard_start_y = 6.0
-        spacing = 1.0 # Расстояние между объектами на кладбище
-        max_per_row = 5 # Сколько объектов в ряду на кладбище
-
-        for i in range(self.num_total_objects):
-            row = i // max_per_row
-            col = i % max_per_row
-            default_pos_tensor[0, i, 0] = graveyard_start_x + col * spacing
-            default_pos_tensor[0, i, 1] = graveyard_start_y + row * spacing
-            default_pos_tensor[0, i, 2] = 0.0
-        # --- Конец: Логика создания "кладбища" ---
-
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            count = obj_cfg['count']
-            indices = torch.arange(start_idx, start_idx + count, device=self.device, dtype=torch.long)
-            types = set(obj_cfg['type'])
-
-            if "changeable_color" in types:
-                colors_dict = self.colors_dict
-                color_names = list(colors_dict.keys())
-                for idx in indices:
-                    color_name = random.choice(color_names)
-                    self.colors[0, idx] = torch.tensor(colors_dict[color_name], device=self.device)
-
-            self.object_map[name] = {'indices': indices, 'types': types, 'count': count}
-            for type_str in types:
-                self.type_map[type_str].extend(indices.tolist())
-            
-            self.names.extend([f"{name}_{i}" for i in range(count)])
-            
-            size_tensor = torch.tensor(obj_cfg['size'], device=self.device)
-            self.sizes[0, indices] = size_tensor
-            self.radii[0, indices] = torch.norm(size_tensor[:2] / 2)
-            start_idx += count
-
-        for type_str, indices in self.type_map.items():
-            self.type_map[type_str] = torch.tensor(sorted(indices), device=self.device, dtype=torch.long)
-
-        # --- Исправленная последовательность ---
-        # 1. Присваиваем правильно созданные "кладбищенские" позиции
-        self.default_positions = default_pos_tensor.expand(self.num_envs, -1, -1)
-        id_map = {"table": 1, "bowl": 2, "chair": 3, "cabinet": 4}
-        for name, data in self.object_map.items():
-            obj_id = id_map.get(name, 0)  # Default to 0 for unmapped objects
-            self.object_ids[0, data['indices']] = obj_id
-        # 2. Инициализируем текущие позиции из дефолтных
-        self.positions = self.default_positions.clone()
-
-    def _initialize_strategies(self):
-        strategies = {}
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            print("name: ", name)
-            placement_cfg_list = obj_cfg.get('placement')
-            if not placement_cfg_list: continue
-            placement_cfg = placement_cfg_list[0]
-            strategy_type = placement_cfg['strategy']
-            if strategy_type == 'grid':
-                strategies[name] = GridPlacement(self.device, placement_cfg['grid_coordinates'])
-            elif strategy_type == 'on_surface':
-                surface_indices = self.type_map.get(placement_cfg['surface_types'][0], torch.tensor([], dtype=torch.long))
-                strategies[name] = OnSurfacePlacement(self.device, surface_indices.tolist(), placement_cfg['margin'])
-        return strategies
-
-    def randomize_scene(self, env_ids: torch.Tensor, mess: bool = False, use_obstacles: bool = False, all_defoult: bool = False):
-        """Абстрактная, векторизованная рандомизация сцены на основе ТИПОВ объектов."""
-        num_to_randomize = len(env_ids)
-        
-        # 1. Сброс состояния
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        if all_defoult:
-            return
-
-        # 2. Определение количества объектов для размещения по типам
-        num_surface_only = len(self.type_map.get("surface_only", []))
-        num_providers = len(self.type_map.get("surface_provider", []))
-        num_floor_obs = len(self.type_map.get("movable_obstacle", [])) - num_surface_only
-        num_static_floor_obs = len(self.type_map.get("staff_obstacle", [])) - num_surface_only
-
-        num_surface_only_to_place = torch.randint(1, num_surface_only + 1, (num_to_randomize,), device=self.device) if num_surface_only > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        
-        if num_providers > 0:
-            # Нижняя граница: минимум 1, и не меньше чем количество surface_only объектов.
-            low_bound = torch.max(
-                torch.tensor(1, device=self.device), 
-                num_surface_only_to_place
-            )
-            # Верхняя граница (исключающая для генерации)
-            high_bound = num_providers + 1
-            
-            # Генерируем случайные числа с плавающей точкой и масштабируем их до нужного диапазона
-            rand_float = torch.rand(num_to_randomize, device=self.device)
-            num_providers_to_place = (low_bound + rand_float * (high_bound - low_bound)).long()
-        else:
-            num_providers_to_place = torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_floor_obstacles_to_place = torch.randint(0, num_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_static_floor_obstacles_to_place = torch.randint(0, num_static_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_static_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-
-        # 3. Применение стратегий в правильном порядке (сначала поверхности)
-        scene_data = self.get_scene_data_dict()
-        placement_order = ["surface_provider", "movable_obstacle", "surface_only", "staff_obstacle"]
-        
-        # Определяем, сколько объектов какого типа нужно разместить
-        counts_by_type = {
-            "surface_provider": num_providers_to_place,
-            "movable_obstacle": num_floor_obstacles_to_place,
-            "surface_only": num_surface_only_to_place,
-            "staff_obstacle": num_static_floor_obstacles_to_place,
-        }
-        
-        for p_type in placement_order:
-            # Находим все объекты, имеющие данный тип
-            for name, data in self.object_map.items():
-                if p_type in data['types'] and name in self.placement_strategies:
-                    # Некоторые типы могут иметь несколько ролей, например, movable_obstacle может быть не surface_only
-                    # Проверяем, что мы еще не разместили этот объект в рамках другой роли
-                    if p_type == "movable_obstacle" and "surface_only" in data['types']:
-                        continue
-
-                    obj_indices = data['indices']
-                    num_to_place_per_env = counts_by_type.get(p_type, torch.zeros(num_to_randomize, dtype=torch.long))
-
-                    # Векторизованный выбор случайных экземпляров
-                    rand_indices = torch.rand(num_to_randomize, len(obj_indices), device=self.device).argsort(dim=1)
-                    
-                    max_num_to_place = int(num_to_place_per_env.max())
-                    if max_num_to_place == 0: continue
-                    
-                    indices_to_place = obj_indices[rand_indices[:, :max_num_to_place]]
-
-                    # Маска, чтобы применять стратегию только к тем env, где нужно разместить > 0 объектов
-                    valid_envs_mask = num_to_place_per_env > 0
-                    
-                    # Фильтруем env_ids и indices_to_place
-                    active_env_ids = env_ids[valid_envs_mask]
-                    if len(active_env_ids) == 0: continue
-
-                    active_indices_to_place = indices_to_place[valid_envs_mask]
-                    # print("active_indices_to_place ", active_indices_to_place)
-                    self.placement_strategies[name].apply(active_env_ids, active_indices_to_place, scene_data, mess)
-
-        self.chose_active_goal_state(env_ids)
-
-    def get_active_obstacle_positions_for_path_planning(self, env_ids: torch.Tensor) -> list:
-        """
-        Возвращает позиции активных препятствий в формате списка списков,
-        специально для генерации строкового ключа в path_manager.
-        """
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return [[] for _ in env_ids]
-            
-        active_mask = self.active[env_ids][:, obs_indices] # (num_envs, num_obstacles)
-        positions = self.positions[env_ids][:, obs_indices].cpu().numpy() # (num_envs, num_obstacles, 3)
-        
-        output_list = []
-        for i in range(len(env_ids)):
-            # Выбираем только активные позиции для i-й среды
-            active_positions = positions[i, active_mask[i].cpu().numpy()]
-            # Округляем и сортируем для консистентности ключа
-            rounded_pos = [(round(p[0], 1), round(p[1], 1), round(p[2], 1)) for p in active_positions]
-            output_list.append(sorted(rounded_pos))
-            
-        return output_list
-
-    def get_graph_embedding(self, env_ids: torch.Tensor) -> torch.Tensor:
-        """Создает тензорный эмбеддинг фиксированного размера для текущего состояния сцены."""
-        # [is_active, pos_x, pos_y, pos_z, size_x, size_y, size_z, radius, object_id]
-        # Размер фичи: 1 + 3 + 3 + 1 + 1 = 9
-        num_features = 9
-        embedding = torch.zeros(len(env_ids), self.num_total_objects, num_features, device=self.device)
-        # print("bbbb ", len(embedding[0]))
-        env_positions = self.positions[env_ids] + 5
-        env_active = self.active[env_ids].float().unsqueeze(-1)
-        env_sizes = self.sizes.expand(len(env_ids), -1, -1)
-        env_radii = self.radii.expand(len(env_ids), -1).unsqueeze(-1)
-        env_object_ids = self.object_ids.expand(len(env_ids), -1).unsqueeze(-1)
-
-        embedding[..., 0:1] = env_active
-        embedding[..., 1:4] = env_positions * env_active
-        embedding[..., 4:7] = env_sizes * env_active
-        embedding[..., 7:8] = env_radii * env_active
-        embedding[..., 8:9] = env_object_ids * env_active
-
-        # Нормализация для лучшего обучения (применяется ко всем, но неактивные останутся 0)
-        embedding[..., 1:4] /= 5.0  # Делим позиции на примерный масштаб комнаты
-        embedding[..., 4:7] /= 1.0  # Размеры уже примерно в этом диапазоне
-        embedding[..., 7:8] /= 2.0  # Радиусы
-        embedding[..., 8:9] /= 3.0  # Нормализация ID (максимум 3 для chair)
-        # Возвращаем "плоский" тензор
-        return embedding.view(len(env_ids), -1)
-
-    def print_graph_info(self, env_id: int):
-        """Печатает детальную информацию о сцене для ОДНОГО окружения."""
-        print(f"\n=== Scene Information (Env ID: {env_id}) ===")
-        
-        # Данные для указанного env_id
-        positions = self.positions[env_id]
-        active_states = self.active[env_id]
-        surface_indices = self.on_surface_idx[env_id]
-        surface_levels = self.surface_level[env_id]
-        
-        table_data = []
-        for i in range(self.num_total_objects):
-            name = self.names[i]
-            pos = positions[i]
-            # Ищем типы по индексу
-            types = ", ".join([t for t, inds in self.type_map.items() if i in inds])
-
-            row = [
-                i, name, types,
-                f"({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})",
-                f"{self.radii[0, i]:.2f}",
-                str(active_states[i].item()),
-                surface_indices[i].item(),
-                surface_levels[i].item()
-            ]
-            table_data.append(row)
-            
-        headers = ["ID", "Name", "Types", "Position", "Radius", "Active", "On Surface", "Surface Level"]
-        print(tabulate(table_data, headers=headers, tablefmt="grid"))
-    
-    def chose_active_goal_state(self, env_ids: torch.Tensor):
-        goal_indices = self.type_map.get("possible_goal", torch.tensor([], dtype=torch.long))
-        if len(goal_indices) == 0:
-            print(f"[WARNING] No objects of type 'possible_goal' found in config.")
-            self.goal_positions[env_ids] = torch.tensor([-4.5, 0.0, 0.75], device=self.device)
-            return
-        
-        active_goal_mask = self.active[env_ids][:, goal_indices].float()
-        
-        # Fallback если ни одна цель не активна
-        any_active = active_goal_mask.sum(dim=1) > 0
-        if not all(any_active):
-            print("NO GOAL", any_active)
-            # # Для env где нет активных целей, активируем первую попавшуюся
-            # fallback_mask = ~any_active
-            # active_goal_mask[fallback_mask, 0] = 1.0
-
-        chosen_goal_rel_idx = torch.multinomial(active_goal_mask + 1e-9, 1).squeeze(-1)
-        chosen_goal_idx = goal_indices[chosen_goal_rel_idx]
-        
-        env_indices = env_ids
-        self.goal_positions[env_indices] = self.positions[env_indices, chosen_goal_idx]
-
-    def get_active_goal_state(self, env_ids: torch.Tensor):
-        return self.goal_positions[env_ids]
-
-    def place_robot_for_goal(self, env_ids: torch.Tensor, mean_dist: float, min_dist: float, max_dist: float, angle_error: float):
-        """Размещает робота относительно цели, избегая препятствий и границ."""
-        # Этап 1: Получение числа сред
-        num_envs = len(env_ids)
-
-        # Этап 2: Извлечение позиций целей
-        goal_pos = self.goal_positions[env_ids]
-
-        # Этап 3: Определение активных препятствий на полу
-        is_floor_obstacle = (self.active[env_ids] == True) & (self.on_surface_idx[env_ids] == -1)
-
-        # Этап 4: Извлечение позиций и радиусов препятствий
-        obstacle_pos_all = self.positions[env_ids, :, :2].clone()
-
-        obstacle_radii_all = self.radii.expand(self.num_envs, -1)[env_ids]
-        # Этап 5: Фильтрация неактивных препятствий
-        inf_pos = torch.full_like(obstacle_pos_all, 999.0)
-
-        obstacle_pos = torch.where(is_floor_obstacle.unsqueeze(-1), obstacle_pos_all, inf_pos)
-        # Этап 6: Генерация радиусов для размещения робота
-        mean_dist_with_shift = mean_dist + 1.31
-        radii = torch.normal(mean=mean_dist_with_shift, std=mean_dist * 0.1, size=(num_envs, 1), device=self.device).clamp_(min_dist, max_dist)
-        # Этап 7: Генерация кандидатов для позиций робота
-        candidates = goal_pos[:, None, :2] + radii.unsqueeze(1) * self.candidate_vectors
-        # Этап 8: Проверка границ комнаты
-        # Этап 8: Проверка границ комнаты (только границы, без коллизий)
-        bounds = self.room_bounds
-        in_bounds_mask = (
-            (candidates[..., 0] >= bounds['x_min'] + self.robot_radius) &
-            (candidates[..., 0] <= bounds['x_max'] - self.robot_radius) &
-            (candidates[..., 1] >= bounds['y_min'] + self.robot_radius) &
-            (candidates[..., 1] <= bounds['y_max'] - self.robot_radius)
-        )
-        # print(candidates)
-        # print(in_bounds_mask)
-        # Этап 9: Выбор углов только по границам
-        in_bounds_mask_float = in_bounds_mask.float() + 1e-9
-        chosen_angle_idx = torch.multinomial(in_bounds_mask_float, 1).squeeze(-1)
-        # print(chosen_angle_idx)
-        # Этап 10: Выбор финальных позиций робота
-        batch_indices = torch.arange(num_envs, device=self.device)
-        final_robot_positions = candidates[batch_indices, chosen_angle_idx]
-
-        # Этап 11: fallback если ни одна позиция не в границах
-        no_valid_pos_mask = ~in_bounds_mask.any(dim=1)
-        if torch.any(no_valid_pos_mask):
-            fallback_pos = goal_pos[:, :2] + torch.tensor([max_dist, 0.0], device=self.device) # 0.0!
-            final_robot_positions[no_valid_pos_mask] = fallback_pos[no_valid_pos_mask]
-
-        # Этап 15: Вычисление ориентации робота (yaw)
-        direction_to_goal = goal_pos[:, :2] - final_robot_positions
-        base_yaw = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-        error = (torch.rand(num_envs, device=self.device) - 0.5) * 2 * angle_error
-        final_yaw = base_yaw + error
-        # Этап 16: Формирование кватернионов ориентации
-        robot_quats = torch.zeros(num_envs, 4, device=self.device)
-        robot_quats[:, 0] = torch.cos(final_yaw / 2.0)
-        robot_quats[:, 3] = torch.sin(final_yaw / 2.0)
-        # Этап 17: Возврат результатов
-        # Проверяем пересечения с препятствиями
-        self.remove_colliding_obstacles(env_ids, final_robot_positions)
-
-        return final_robot_positions, robot_quats
-    
-
-    def remove_colliding_obstacles(self, env_ids: torch.Tensor, robot_positions: torch.Tensor):
-        """Ставит в дефолт все препятствия, пересекающиеся с роботом."""
-        # TODO There can be obstacles with suface providing and we should delete alse items on that
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return
-
-        # позиции и радиусы препятствий
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r + 0.2)
-        if coll_mask.any():
-            # print("coll_mask: ",  coll_mask)
-            # for i in env_ids:
-            #     self.print_graph_info(i)
-            # переносим такие препятствия в дефолт
-            default_pos = self.default_positions[env_ids][:, obs_indices]
-            batch_idx, obs_idx = torch.where(coll_mask)                 # индексы элементов с коллизией
-            env_batch_idx = env_ids[batch_idx]                           # индексы env_ids для batch
-            obs_indices_sel = obs_indices[obs_idx]                       # индексы obstacles
-
-            # Присваиваем значения дефолтных позиций в исходный тензор
-            self.positions[env_batch_idx, obs_indices_sel] = default_pos[batch_idx, obs_idx]
-
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-            # print(default_pos[coll_mask])
-            
-            # self.positions[env_ids][:, obs_indices][coll_mask] = default_pos[coll_mask]
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-
-            # деактивируем их
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-            self.active[env_batch_idx, obs_indices_sel] = False
-
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r)
-        if coll_mask.any():
-            
-            # print("coll_mask 2: ",  coll_mask)
-            
-            for i in env_ids:
-                self.print_graph_info(i)
-    
-    def init_graph_descriptor(self, clip_processor, clip_model):
-        names = ["bowl", "cabinet", "chair", "table"]
-        i = 0.0
-        for name in names:
-            i += 1
-            self.clip_descriptors[name] = torch.tensor(i,device=self.device)
-        print("self.clip_descriptors", self.clip_descriptors)
-        print(f"[ info ] Inited clip_descriptors: {self.clip_descriptors} and full_name: {self.full_names}")
-
-    def get_graph_obs(self, env_ids=None) -> list:
-        """Returns a list of length num_envs, where each element is a list of node dictionaries representing the scene graph.
-        
-        Each node dict has format:
-        {"node_id": int, "clip_descriptor": list[float] (512), "edges_vl_sat": list[dict], "bbox_center": list[float] (3), "bbox_extent": list[float] (3), "class_name": str}
-        
-        edges_vl_sat contains dicts for relations to other nodes: {"id_1": int, "class_name_1": str, "rel_id": int, "id_2": int}
-        Relations are positional only, based on main difference in x/y/z axes from viewpoint (0,0 facing -y).
-        """
-        if env_ids is None:
-            env_ids = torch.arange(self.num_envs, device=self.device)
-        
-        output = []
-        for eid in env_ids:
-            active_mask = self.active[eid]
-            active_indices = torch.nonzero(active_mask).squeeze(-1)
-            num_active = len(active_indices)
-            
-            if num_active == 0:
-                output.append([])
-                continue
-            
-            # Map global indices to local node_ids (0 to num_active-1)
-            node_id_map = {int(active_indices[j]): j for j in range(num_active)}
-            
-            # Gather data for active objects
-            active_pos = self.positions[eid, active_indices]
-            active_sizes = self.sizes[0, active_indices]
-            active_names = [self.names[int(idx)] for idx in active_indices]
-            # Fetch CLIP descriptors based on names
-            active_clip = torch.stack([self.clip_descriptors[name.split('_')[0]] for name in active_names]).to(self.device)
-            
-            env_graph = []
-            for j in range(num_active):
-                idx = active_indices[j]
-                node_id = j
-                clip_desc = active_clip[j].tolist()
-                bbox_center = active_pos[j].tolist()
-                bbox_extent = active_sizes[j].tolist()
-                name = active_names[j]
-                
-                edges = []
-                for k in range(num_active):
-                    if k == j:
-                        continue
-                    
-                    diff = active_pos[k] - active_pos[j]
-                    abs_diff = torch.abs(diff)
-                    max_idx = abs_diff.argmax().item()
-                    
-                    if max_idx == 0:  # x (left/right; assume +x is right)
-                        rel_id = 19 if diff[0] > 0 else 14  # right / left
-                    elif max_idx == 1:  # y (front/behind; facing -y, +y behind, -y front)
-                        rel_id = 1 if diff[1] > 0 else 8  # behind / front
-                    elif max_idx == 2:  # z (higher/lower)
-                        rel_id = 11 if diff[2] > 0 else 15  # higher than / lower than
-                    
-                    edge_dict = {
-                        "id_1": node_id,
-                        "rel_id": rel_id,
-                        "id_2": k
-                    }
-                    edges.append(edge_dict)
-                
-                node_dict = {
-                    "node_id": node_id,
-                    "clip_descriptor": clip_desc,
-                    "edges_vl_sat": edges,
-                    "bbox_center": bbox_center,
-                    "bbox_extent": bbox_extent,
-                }
-                env_graph.append(node_dict)
-            
-            # Выполняем вытеснение неактивных объектов: заполняем до self.num_total_objects копиями случайных активных объектов
-            while len(env_graph) < self.num_total_objects and num_active > 0:
-                new_id = len(env_graph)
-                original_j = random.randint(0, num_active - 1)  # Выбираем из оригинальных активных
-                original_node = env_graph[original_j]
-                
-                new_node = original_node.copy()
-                new_node["node_id"] = new_id
-                new_node["edges_vl_sat"] = []
-                
-                # Добавляем edges между существующими нодами и новой
-                for existing_id in range(new_id):
-                    # Находим edge от existing к original_j и копируем для existing к new_id
-                    for edge in env_graph[existing_id]["edges_vl_sat"]:
-                        if edge["id_2"] == original_j:
-                            new_edge = edge.copy()
-                            new_edge["id_2"] = new_id
-                            env_graph[existing_id]["edges_vl_sat"].append(new_edge)
-                            break
-                    
-                    # Находим edge от original_j к existing и копируем для new_id к existing
-                    for edge in original_node["edges_vl_sat"]:
-                        if edge["id_2"] == existing_id:
-                            new_edge = edge.copy()
-                            new_edge["id_1"] = new_id
-                            new_node["edges_vl_sat"].append(new_edge)
-                            break
-                
-                env_graph.append(new_node)
-            
-            output.append(env_graph)
-        
-        return output
-
-    def add_noise_to_graph_obs(self, graph_obs_list: list[list[dict]]) -> list[list[dict]]:
-        """Adds noise to the graph observations: small perturbations to centers and extents mostly,
-        larger rarely (via normal dist with occasional outliers), and rarely change some edge rel_ids.
-        
-        Operates vectorized internally for efficiency.
-        """
-        num_envs = len(graph_obs_list)
-        if num_envs == 0:
-            return graph_obs_list
-        
-        # Find max_nodes across envs
-        max_nodes = self.num_total_objects
-        
-        # Tensors for centers, extents, rel_ids
-        centers = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        extents = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        rel_ids = torch.full((num_envs, max_nodes, max_nodes), -1, dtype=torch.long, device=self.device)  # -1 invalid
-        
-        # Fill tensors from list
-        for e, graph in enumerate(graph_obs_list):
-            n = len(graph)
-            if n == 0:
-                continue
-            for j, node in enumerate(graph):
-                centers[e, j] = torch.tensor(node["bbox_center"], device=self.device)
-                extents[e, j] = torch.tensor(node["bbox_extent"], device=self.device)
-                for edge in node["edges_vl_sat"]:
-                    k = edge["id_2"]
-                    rel_ids[e, j, k] = edge["rel_id"]
-        
-        # Add noise to centers: normal dist, sigma=0.05 mostly, but with 10% chance sigma=0.2, 1% sigma=1.0
-        noise_levels = torch.rand(num_envs, max_nodes, 1, device=self.device)
-        sigmas = torch.where(noise_levels < 0.01, 1.0, torch.where(noise_levels < 0.1, 0.2, 0.05))
-        center_noise = torch.randn(num_envs, max_nodes, 3, device=self.device) * sigmas
-        centers += center_noise
-        
-        # Add noise to extents: multiplicative, normal around 1, same sigma logic
-        extent_factors = 1 + torch.randn(num_envs, max_nodes, 3, device=self.device) * sigmas
-        extent_factors.clamp_(0.5, 2.0)  # Prevent negative or extreme
-        extents *= extent_factors
-        
-        # Rarely change rel_ids: per edge with prob 0.01, set to random rel_id from {1, 8, 11, 14, 15, 19}
-        valid_rel_ids = torch.tensor([1, 8, 11, 14, 15, 19], device=self.device)  # Only positional relations
-        change_mask = (rel_ids >= 0) & (torch.rand_like(rel_ids.float()) < 0.01)
-        random_indices = torch.randint(0, len(valid_rel_ids), change_mask.shape, device=self.device)
-        new_rels = valid_rel_ids[random_indices]
-        rel_ids[change_mask] = new_rels[change_mask]
-        
-        # Convert back to list[list[dict]]
-        noisy_graph_obs = []
-        for e in range(num_envs):
-            n = len(graph_obs_list[e])
-            if n == 0:
-                noisy_graph_obs.append([])
-                continue
-            env_graph = []
-            for j in range(n):
-                node = graph_obs_list[e][j].copy()  # Copy original
-                node["bbox_center"] = centers[e, j].tolist()
-                node["bbox_extent"] = extents[e, j].tolist()
-                edges = []
-                for k in range(n):
-                    if k == j:
-                        continue
-                    rel_id = int(rel_ids[e, j, k])
-                    if rel_id < 0:
-                        continue
-                    edge_dict = {
-                        "id_1": j,
-                        "rel_id": rel_id,
-                        "id_2": k
-                    }
-                    edges.append(edge_dict)
-                node["edges_vl_sat"] = edges
-                env_graph.append(node)
-            noisy_graph_obs.append(env_graph)
-        
-        return noisy_graph_obs
-
-    def tensorize_graph_obs(self, graph_obs_list: list[list[dict]]) -> dict[str, torch.Tensor]:
-        """
-        Converts a list of scene graphs (one per environment) into a dictionary of batched tensors.
-        ф
-        Args:
-            graph_obs_list: List of length num_envs, each element is a list of node dictionaries for that env.
-        
-        Returns:
-            Dict with keys: 'node_clip' (num_envs, max_nodes, 512),
-                            'node_center' (num_envs, max_nodes, 3),
-                            'node_extent' (num_envs, max_nodes, 3),
-                            'rel_ids' (num_envs, max_nodes, max_nodes, long)
-        """
-        num_envs = len(graph_obs_list)
-        max_nodes = self.num_total_objects
-        clip_dim = 512
-        
-        node_clip = torch.zeros(num_envs, max_nodes, 1, device=self.device)
-        node_center = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        node_extent = torch.zeros(num_envs, max_nodes, 3, device=self.device)
-        rel_ids = torch.zeros(num_envs, max_nodes, max_nodes, dtype=torch.long, device=self.device)
-        
-        for e in range(num_envs):
-            graph = graph_obs_list[e]
-            n = len(graph)
-            if n == 0:
-                continue
-            
-            for j in range(n):
-                node = graph[j]
-                node_clip[e, j] = torch.tensor(node["clip_descriptor"], device=self.device)
-                node_center[e, j] = torch.tensor(node["bbox_center"], device=self.device)
-                node_extent[e, j] = torch.tensor(node["bbox_extent"], device=self.device)
-                
-                for edge in node["edges_vl_sat"]:
-                    k = edge["id_2"]
-                    rel_id = edge["rel_id"]
-                    rel_ids[e, j, k] = rel_id
-        return {
-            "node_clip": node_clip,
-            "node_center": node_center,
-            "node_extent": node_extent,
-            "rel_ids": rel_ids
-        }
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_for_pg.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_for_pg.py
deleted file mode 100644
index f580ac6363..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_for_pg.py
+++ /dev/null
@@ -1,436 +0,0 @@
-# ЗАМЕНИТЬ ФАЙЛ: scene_manager.py
-
-import torch
-import math
-import random
-import json
-# from .graph_manager import ObstacleGraph
-# from .placement_strategies import PlacementStrategy, GridPlacement, OnSurfacePlacement
-
-import importlib.util
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/graph_manager.py"
-ObstacleGraph = import_class_from_path(module_path, "ObstacleGraph")
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies_for_pg.py"
-PlacementStrategy = import_class_from_path(module_path, "PlacementStrategy")
-GridPlacement = import_class_from_path(module_path, "GridPlacement")
-OnSurfacePlacement = import_class_from_path(module_path, "OnSurfacePlacement")
-
-class SceneManager:
-    def __init__(self, num_envs: int, config_path: str, device: str):
-        self.num_envs = num_envs
-        self.device = device
-        with open(config_path, 'r') as f:
-            self.config = json.load(f)['objects']
-
-        self.object_indices = {}
-        self.placement_strategies = {}
-        self.graphs = [ObstacleGraph(self.config, device) for _ in range(num_envs)]
-        self._initialize_strategies()
-
-        # Параметры робота и комнаты
-        self.robot_radius = 0.5
-        self.room_bounds = {'x_min': -4, 'x_max': 4.0, 'y_min': -3.0, 'y_max': 3.0}
-        self.robot_placement_bounds = {
-            'x_min': self.room_bounds['x_min'] + self.robot_radius,
-            'x_max': self.room_bounds['x_max'] - self.robot_radius,
-            'y_min': self.room_bounds['y_min'] + self.robot_radius,
-            'y_max': self.room_bounds['y_max'] - self.robot_radius
-        }
-        # Для дискретизации углов
-        self.n_angles = 36
-        self.angle_step = math.pi / self.n_angles
-        self.discrete_angles = torch.arange(0, 2 * math.pi, self.angle_step, device=self.device)
-        self.goal_positions = torch.zeros((num_envs,3),device=self.device)
-
-
-    def _initialize_strategies(self):
-        """Создает и настраивает стратегии размещения на основе конфига."""
-        start_idx = 0
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            count = obj_cfg['count']
-            self.object_indices[name] = list(range(start_idx, start_idx + count))
-            start_idx += count
-
-            placement_cfg = obj_cfg.get('placement')
-            if not placement_cfg: continue
-
-            placement_cfg = placement_cfg[0]
-            strategy_type = placement_cfg['strategy']
-            if strategy_type == 'grid':
-                print(placement_cfg['grid_coordinates'])
-                self.placement_strategies[name] = GridPlacement(
-                    self.device, placement_cfg['grid_coordinates']
-                )
-            elif strategy_type == 'on_surface':
-                surface_types = placement_cfg['surface_types']
-                # surface_indices = self.object_indices[placement_cfg['surface_object_name']]
-                self.placement_strategies[name] = OnSurfacePlacement(
-                    self.device, surface_types, placement_cfg['margin']
-                )
-
-    def randomize_scene(self, env_ids: torch.Tensor, mess: bool = False, use_obstacles: bool = False, all_defoult = False):
-        """
-        По-настоящему унифицированная, иерархическая рандомизация сцены.
-        Логика оперирует исключительно ТИПАМИ объектов, а не их именами.
-        """
-        from collections import defaultdict
-        for env_id in env_ids:
-            graph = self.graphs[env_id.item()]
-            # 1. Сброс состояния графа
-            graph.active[:] = False
-            graph.positions = graph.default_positions.clone()
-            # print("def pos ", graph.default_positions)
-            graph.on_surface_idx[:] = -1
-            graph.surface_level[:] = 0
-            if all_defoult:
-                pass
-            # 2. Сбор всех доступных объектов по их РОЛЯМ (типам)
-            all_providers = graph.get_nodes_by_type("surface_provider")
-            all_surface_only_objects = graph.get_nodes_by_type("surface_only")
-            # print("all_providers :", all_providers)
-            # print("all_surface_only_objects :", all_surface_only_objects)
-            # Находим препятствия, которые могут стоять на полу
-            all_floor_obstacles = [
-                i for i in graph.get_nodes_by_type("movable_obstacle")
-                if i not in all_surface_only_objects
-            ]
-            # print("all_floor_obstacles :", all_floor_obstacles)
-            # 3. ОПРЕДЕЛЕНИЕ КОЛИЧЕСТВА НА ОСНОВЕ РОЛЕЙ (логика "задом наперед")
-            
-            # Шаг А: Сколько объектов, которые могут быть только на поверхностях, мы хотим разместить?
-            # Сюда входят и цели (possible_goal), и просто декор (stuff), если они surface_only.
-            # Для простоты и для гарантии наличия цели, будем размещать хотя бы один такой объект.
-            num_surface_only_to_place = random.randint(1, len(all_surface_only_objects)) if len(all_surface_only_objects) > 0 else 0
-            # Шаг Б: Сколько поверхностей нам нужно под эти объекты?
-            min_providers_needed = num_surface_only_to_place
-            num_providers_to_place = 0
-            if all_providers:
-                max_providers = len(all_providers)
-                num_providers_to_place = random.randint(min_providers_needed, max_providers) if min_providers_needed <= max_providers else max_providers
-
-            # Шаг В: Сколько объектов будет стоять на полу?
-            num_floor_obstacles_to_place = random.randint(0, len(all_floor_obstacles)) if all_floor_obstacles and use_obstacles else 0
-
-            # 4. ВЫБОР КОНКРЕТНЫХ ЭКЗЕМПЛЯРОВ И ИХ РАЗМЕЩЕНИЕ
-            
-            # Выбираем случайные экземпляры провайдеров для активации
-            providers_to_place = random.sample(all_providers, num_providers_to_place)
-            
-            # Выбираем случайные экземпляры surface-only объектов для активации
-            surface_only_to_place = random.sample(all_surface_only_objects, num_surface_only_to_place)
-            # Выбираем случайные экземпляры напольных препятствий для активации
-            floor_obstacles_to_place = random.sample(all_floor_obstacles, num_floor_obstacles_to_place)
-
-            # Собираем все объекты, которые нужно разместить, в один список
-            all_objects_to_place = providers_to_place + surface_only_to_place + floor_obstacles_to_place
-            # Группируем их по имени, чтобы применить правильную стратегию
-            grouped_by_name = defaultdict(list)
-            
-            for node_idx in all_objects_to_place:
-                name = graph.graph.nodes[node_idx]['name']
-                grouped_by_name[name].append(node_idx)
-            # Применяем стратегии в правильном порядке: сначала поверхности, потом все остальное
-            placement_order = ["surface_provider", "surface_only", "movable_obstacle"]
-            
-            for p_type in placement_order:
-                for name, indices in grouped_by_name.items():
-                    # Проверяем, относится ли группа объектов к текущему этапу размещения
-                    node_types = graph.graph.nodes[indices[0]]['types']
-                    if p_type in node_types:
-                        strategy = self.placement_strategies.get(name)
-                        if strategy:
-                            strategy.apply(graph, indices, len(indices), mess)
-
-                # 5. Финальное обновление состояния графа
-                graph.update_graph_state()
-            # graph.print_graph_info(f"______END ________")
-        self.chose_active_goal_state(env_ids=env_ids)
-
-    def chose_active_goal_state(self, env_ids: torch.Tensor):
-        """Находит случайную активную цель для каждой среды и возвращает ее локальную позицию."""
-        goal_positions = torch.zeros(len(env_ids), 3, device=self.device)
-        
-        for i, env_id in enumerate(env_ids):
-            graph = self.graphs[env_id.item()]
-            
-            # ИЗМЕНЕНИЕ: Используем новый метод get_nodes_by_type, который ищет строку в множестве типов.
-            
-            active_goal_indices = graph.get_nodes_by_type("possible_goal", only_active=True)
-            # print("chose_active_goal_state: ", env_id, active_goal_indices)
-            if active_goal_indices:
-                # Логика выбора цели остается той же
-                chosen_idx = random.choice(active_goal_indices)
-                goal_positions[i] = graph.positions[chosen_idx]
-            else:
-                # Аварийный случай, если нет активных целей
-                print(f"[WARNING] Env {env_id.item()}: No active goals found. Using fallback position.")
-                goal_positions[i] = torch.tensor([-4.5, 0.0, 0.75], device=self.device)
-        self.goal_positions[env_ids] = goal_positions
-
-    def get_active_goal_state(self, env_ids: torch.Tensor):
-        return self.goal_positions[env_ids]
-
-    def place_robot_for_goal(self, env_ids: torch.Tensor, mean_dist: float, min_dist: float, max_dist: float, angle_error: float, max_attempts: int = 50):
-        """
-        Размещает робота на полу (уровень 0) относительно цели,
-        проверяя коллизии только с объектами на полу.
-        """
-        goal_positions = self.goal_positions[env_ids]
-        num_envs = len(env_ids)
-        final_robot_positions = torch.zeros(num_envs, 2, device=self.device)
-        valid_positions_found = torch.zeros(num_envs, dtype=torch.bool, device=self.device)
-
-        # Шаг 1: Собираем препятствия на ПОЛУ (surface_idx == -1)
-        floor_obstacles_per_env = []
-        for env_id in env_ids:
-            graph = self.graphs[env_id.item()]
-            floor_nodes = [
-                (data['position'][:2], data['radius']) for i, data in graph.graph.nodes(data=True)
-                if data['active'] and data['on_surface_idx'] == -1 and data['radius'] > 0
-            ]
-            floor_obstacles_per_env.append(floor_nodes)
-        
-        # ... (код для создания тензоров all_obstacle_pos, all_obstacle_radii, obstacle_valid_mask
-        # ... остается таким же, как в моем предыдущем ответе, он уже векторизован и корректен)
-        max_obstacles = max(len(obs) for obs in floor_obstacles_per_env) if floor_obstacles_per_env else 0
-        all_obstacle_pos = torch.zeros(num_envs, max_obstacles, 2, device=self.device)
-        all_obstacle_radii = torch.zeros(num_envs, max_obstacles, device=self.device)
-        obstacle_valid_mask = torch.zeros(num_envs, max_obstacles, dtype=torch.bool, device=self.device)
-        for i in range(num_envs):
-            if floor_obstacles_per_env[i]:
-                num_obs = len(floor_obstacles_per_env[i])
-                positions, radii = zip(*floor_obstacles_per_env[i])
-                all_obstacle_pos[i, :num_obs] = torch.tensor(positions, device=self.device)
-                all_obstacle_radii[i, :num_obs] = torch.tensor(radii, device=self.device)
-                obstacle_valid_mask[i, :num_obs] = True
-        # print("all_obstacle_pos", all_obstacle_pos)
-        # Шаг 2: Итеративный поиск валидной позиции
-        # Генерируем все возможные кандидаты сразу для всех сред
-        # [num_envs, num_angles, 2]
-        std_dev = mean_dist*0.1
-        base = 1.3
-        radii_not_clamp = torch.randn(num_envs) * std_dev + mean_dist + base
-        radii = torch.clamp(radii_not_clamp, min=min_dist, max=max_dist)
-        candidate_vectors = torch.stack([torch.cos(self.discrete_angles), torch.sin(self.discrete_angles)], dim=1) # [num_angles, 2]
-        candidates = candidates = goal_positions[:, None, :2].to(self.device) + radii[:, None, None].to(self.device) * candidate_vectors[None, :, :].to(self.device)
-        # Проверка 1: Границы комнаты
-        bounds = self.robot_placement_bounds
-        in_bounds_mask = (candidates[..., 0] >= bounds['x_min']) & (candidates[..., 0] <= bounds['x_max']) & \
-                        (candidates[..., 1] >= bounds['y_min']) & (candidates[..., 1] <= bounds['y_max'])
-        # Проверка 2: Коллизии с препятствиями на полу
-        dists = torch.norm(candidates.unsqueeze(2) - all_obstacle_pos.unsqueeze(1), dim=3) # [envs, angles, obs, 2] -> [envs, angles, obs]
-        required_dists = self.robot_radius + all_obstacle_radii.unsqueeze(1)
-        collisions = (dists < required_dists) & obstacle_valid_mask.unsqueeze(1)
-        no_collision_mask = ~torch.any(collisions, dim=2)
-        
-        # Финальная маска валидных углов
-        valid_angle_mask = in_bounds_mask & no_collision_mask
-        # Шаг 3: Выбор позиции и итеративное выталкивание
-        for i in range(num_envs):
-            valid_indices = torch.where(valid_angle_mask[i])[0]
-            if valid_indices.numel() > 0:
-                has_collisions = True
-                while has_collisions:
-                    chosen_angle_idx = valid_indices[torch.randint(0, valid_indices.numel(), (1,))]
-                    pos = candidates[i, chosen_angle_idx].squeeze()
-                    
-                    # Итеративное выталкивание (упрощенная версия для примера)
-                    # В полноценной реализации здесь должен быть цикл, как в вашем исходном коде
-
-                    pos, has_collisions = self._resolve_collisions_iteratively(pos, all_obstacle_pos[i], all_obstacle_radii[i], obstacle_valid_mask[i])
-                    final_robot_positions[i] = pos
-            else:
-                # Fallback, если не найдено ни одного валидного угла
-                print("[ OH NO ]not valid angle")
-                print("in_bounds_mask: ", in_bounds_mask)
-                print("in_bounds_mask: ", no_collision_mask)
-                final_robot_positions[i] = goal_positions[i, :2] + torch.tensor([max_dist, 0.0], device=self.device)
-        # Шаг 4: Вычисление ориентации
-        robot_quats = torch.zeros(num_envs, 4, device=self.device)
-        direction_to_goal = goal_positions[:, :2] - final_robot_positions
-        base_yaw = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-        error = (torch.rand(num_envs, device=self.device) - 0.5) * 2 * angle_error
-        final_yaw = base_yaw + error
-        robot_quats[:, 0] = torch.cos(final_yaw / 2.0)
-        robot_quats[:, 3] = torch.sin(final_yaw / 2.0)
-        # final_robot_positions = torch.zeros(num_envs, 2, device=self.device)
-        return final_robot_positions, robot_quats
-    
-    def _resolve_collisions_iteratively(self, robot_pos, obs_pos, obs_radii, obs_mask, max_iter=15, safety_margin=0.1):
-        """Итеративно отодвигает робота от препятствий, проверяя границы области."""
-        # Вычисляем начальные расстояния до препятствий
-        robot_pos = self._clamp_to_room_bounds(robot_pos)
-        dists = torch.norm(robot_pos - obs_pos, dim=1)
-        required = self.robot_radius + obs_radii + safety_margin
-        collisions = (dists < required) & obs_mask
-        step = 0
-        istep = 0
-        pos_start = robot_pos
-        while torch.any(collisions):
-            # Пересчитываем расстояния и коллизии
-            dists = torch.norm(robot_pos - obs_pos, dim=1)
-            required = self.robot_radius + obs_radii + safety_margin
-            
-            collisions = (dists < required) & obs_mask
-            
-            # Проверяем, нет ли коллизий
-            if not torch.any(collisions):
-                # Проверяем границы комнаты
-                # print("rp: ", robot_pos)
-                return robot_pos, False  # Нет коллизий, позиция скорректирована
-
-            # Находим ближайшее препятствие с коллизией
-            colliding_dists = dists.clone()
-            colliding_dists[~collisions] = float('inf')
-            closest_idx = torch.argmin(colliding_dists)
-            old_robot_pos = robot_pos
-            # Вектор от центра препятствия к роботу
-            vec_to_robot = robot_pos - obs_pos[closest_idx]
-            vec_norm = torch.norm(vec_to_robot)
-            
-            # Вычисляем расстояние для сдвига
-            move_dist = required[closest_idx] - vec_norm
-            
-            # Сдвигаем робота
-            if vec_norm > 1e-6 and step < 3:
-                robot_pos += (vec_to_robot / vec_norm) * move_dist
-            else:  # Робот в центре препятствия — случайный сдвиг
-                robot_pos += torch.randn(2, device=self.device) * move_dist
-            robot_pos = self._clamp_to_room_bounds(robot_pos)
-            step += 1
-            if step > max_iter:
-                step = 0
-                istep += 1
-                print(f"[DEBUG] COLLISION IN _resolve_collisions_iteratively after {istep} iterations")
-                print(f"old robot pos: {old_robot_pos} obs: {obs_pos}")
-                print(f"dist for obs: {dists} required: {required}")
-
-
-
-                return robot_pos, True  # Нет коллизий, позиция скорректирована
-        return robot_pos, False  # Нет коллизий, позиция скорректирована
-        
-    def _clamp_to_room_bounds(self, robot_pos):
-        """Ограничивает позицию робота границами комнаты."""
-        bounds = self.robot_placement_bounds
-        x = torch.clamp(robot_pos[0], bounds['x_min'], bounds['x_max'])
-        y = torch.clamp(robot_pos[1], bounds['y_min'], bounds['y_max'])
-        return torch.tensor([x, y], device=self.device)
-    
-    def get_selected_indices(self, env_id: int | torch.Tensor) -> list[int] | None:
-        """
-        Возвращает отсортированный список индексов активных препятствий (тип 'movable_obstacle') для заданной среды.
-        Аналог старого метода для совместимости с path_manager.
-        
-        Args:
-            env_id: Индекс среды (int или torch.Tensor).
-        
-        Returns:
-            list[int]: Sorted indices активных movable_obstacle, или None если ошибка.
-        """
-        if torch.is_tensor(env_id):
-            env_id = env_id.item()
-        
-        graph = self.graphs[env_id]
-        active_obstacles = graph.get_nodes_by_type("movable_obstacle", only_active=True)
-        
-        if not active_obstacles:
-            print(f"[WARNING] No active movable_obstacles in env {env_id}")
-            return None
-        
-        return sorted(active_obstacles)
-    
-    def set_obstacle_positions(self, env_ids: torch.Tensor, positions: list[list[float]]):
-        """
-        Детерминировано устанавливает позиции для активных препятствий (movable_obstacle).
-        Активирует первые len(positions) movable nodes, устанавливает их позиции, остальным — default и inactive.
-        USD выбирается автоматически (как в randomize).
-        
-        Args:
-            env_ids: Индексы сред.
-            positions: List of [x,y,z] для каждого активного препятствия (len <= count movable).
-        """
-        # movable_nodes = self.get_nodes_by_type("movable_obstacle")  # Через graph.get_nodes_by_type, но self нет graph, так что:
-        # Найдем movable indices из object_indices
-        movable_indices = self.object_indices.get('chair', [])  # По name, предполагая 'chair' — movable
-        if len(positions) > len(movable_indices):
-            raise ValueError(f"Too many positions: {len(positions)} > movable count {len(movable_indices)}")
-        
-        for env_id in env_ids:
-            graph = self.graphs[env_id.item()]
-            graph.active[:] = False
-            graph.positions = graph.default_positions.clone()
-            graph.on_surface_idx[:] = -1
-            graph.surface_level[:] = 0
-            
-            # Активируем и устанавливаем первые len(positions)
-            for i, pos in enumerate(positions):
-                node_idx = movable_indices[i]
-                graph.positions[node_idx] = torch.tensor(pos, device=self.device)
-                graph.active[node_idx] = True
-                graph.on_surface_idx[node_idx] = -1  # На полу
-                graph.surface_level[node_idx] = 0
-            
-            graph.update_graph_state()  # Синхронизируем NetworkX
-
-    def get_active_obstacle_positions(self, env_id: int | torch.Tensor) -> list[tuple[float, float, float]]:
-        """
-        Возвращает отсортированный список позиций активных movable_obstacle для env_id.
-        Positions rounded to 1 decimal для matching с generator.
-        """
-        if torch.is_tensor(env_id):
-            env_id = env_id.item()
-        graph = self.graphs[env_id]
-        obst_indices = graph.get_nodes_by_type("movable_obstacle", only_active=True)
-        positions = [(round(p[0].item(), 1), round(p[1].item(), 1), round(p[2].item(), 1)) for p in graph.positions[obst_indices]]
-        return sorted(positions)
-
-    def set_goal_position(self, env_ids: torch.Tensor, position: list[float]):
-        """
-        Детерминировано устанавливает позицию цели (possible_goal).
-        Если surface_only, размещает на surface_provider (первом) с z=height_surface + half_size_goal_z, x/y = position (игнор margin для фикса).
-        
-        Args:
-            env_ids: Индексы сред.
-            position: [x,y,z] для цели (z игнорируется если on_surface).
-        """
-        # goal_nodes = self.get_nodes_by_type("possible_goal")  # Через graph, но аналогично
-        goal_idx = self.object_indices.get('bowl', [])[0]  # Предполагая один bowl
-        
-        surface_idx = None
-        for obj_cfg in self.config:
-            if "possible_goal" in obj_cfg['type'] and "surface_only" in obj_cfg['type']:
-                # Находим surface_provider index (active)
-                surface_nodes = self.graphs[0].get_nodes_by_type("surface_provider", only_active=False)
-                if surface_nodes:
-                    surface_idx = surface_nodes[0]  # Первый активный (table)
-        
-        for env_id in env_ids:
-            graph = self.graphs[env_id.item()]
-            pos_tensor = torch.tensor(position, device=self.device)
-            
-            if surface_idx is not None:
-                # On surface: z = surface_z + surface_size_z/2 + goal_size_z/2
-                surf_pos = graph.positions[surface_idx]
-                surf_size = graph.sizes[surface_idx]
-                goal_size = graph.sizes[goal_idx]
-                # TODO CHANGE FIXED  surf_pos[2] + surf_size[2]/2 + goal_size[2]/2
-                pos_tensor = torch.tensor([position[0], position[1], 0.75], device=self.device )  #(pos_tensor, 0.75))
-                graph.on_surface_idx[goal_idx] = surface_idx
-                graph.surface_level[goal_idx] = graph.surface_level[surface_idx] + 1
-            
-            graph.positions[goal_idx] = pos_tensor
-            graph.active[goal_idx] = True
-            self.goal_positions[env_id] = pos_tensor  # Обновляем self.goal_positions
-            
-            graph.update_graph_state()
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_img.py b/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_img.py
deleted file mode 100644
index ac614dd48f..0000000000
--- a/source/isaaclab_tasks/isaaclab_tasks/direct/aloha/scene_manager_img.py
+++ /dev/null
@@ -1,549 +0,0 @@
-import torch
-import math
-import random
-import json
-from collections import defaultdict
-from tabulate import tabulate
-import importlib.util
-# Импортируем обновленные, векторизованные стратегии
-# from .placement_strategies import PlacementStrategy, GridPlacement, OnSurfacePlacement # Эти классы остаются как в предыдущем ответе
-def import_class_from_path(module_path, class_name):
-    print(f"[DEBUG] Importing class '{class_name}' from module: {module_path}")
-    spec = importlib.util.spec_from_file_location("custom_module", module_path)
-    module = importlib.util.module_from_spec(spec)
-    spec.loader.exec_module(module)
-    class_obj = getattr(module, class_name)
-    print(f"[DEBUG] Successfully imported class: {class_obj}")
-    return class_obj
-
-module_path = "source/isaaclab_tasks/isaaclab_tasks/direct/aloha/placement_strategies.py"
-PlacementStrategy = import_class_from_path(module_path, "PlacementStrategy")
-GridPlacement = import_class_from_path(module_path, "GridPlacement")
-OnSurfacePlacement = import_class_from_path(module_path, "OnSurfacePlacement")
-
-class SceneManager:
-    def __init__(self, num_envs: int, config_path: str, device: str):
-        self.num_envs = num_envs
-        self.device = device
-        with open(config_path, 'r') as f:
-            self.config = json.load(f)['objects']
-        self.colors_dict = {
-                        "green": [0.0, 1.0, 0.0],
-                        "blue": [0.0, 0.0, 1.0],
-                        "yellow": [1.0, 1.0, 0.0],
-                        "gray": [0.5, 0.5, 0.5],
-                        "red": [1.0, 0.0, 0.0]
-                    }
-        # --- Начало: Векторизованная структура данных ---
-        self.num_total_objects = sum(obj['count'] for obj in self.config)
-        self.object_ids = torch.zeros(1, self.num_total_objects, device=self.device)
-        # Словари для быстрого доступа к метаданным
-        self.object_map = {} # {name: {'indices': tensor, 'types': set, 'count': int}}
-        self.type_map = defaultdict(list) # {type_str: [indices...]}
-        
-        # Глобальные тензоры состояний
-        self.positions = torch.zeros(self.num_envs, self.num_total_objects, 3, device=self.device)
-        self.sizes = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        self.radii = torch.zeros(1, self.num_total_objects, device=self.device)
-        self.colors = torch.ones(1, self.num_total_objects, 3, device=self.device)  # По умолчанию белый для не-changeable
-        self.names = [] # Список имен для print_graph_info
-        self.active = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.bool, device=self.device)
-        self.on_surface_idx = torch.full((self.num_envs, self.num_total_objects), -1, dtype=torch.long, device=self.device)
-        self.surface_level = torch.zeros(self.num_envs, self.num_total_objects, dtype=torch.long, device=self.device)
-        
-        self._initialize_object_data()
-        self.default_positions = self.positions.clone()
-        # --- Конец: Векторизованная структура данных ---
-
-        self.placement_strategies = self._initialize_strategies()
-
-        self.robot_radius = 0.5
-        self.room_bounds = {'x_min': -4, 'x_max': 4.0, 'y_min': -2.7, 'y_max': 2.7}
-        self.goal_positions = torch.zeros((num_envs, 3), device=self.device)
-
-        n_angles = 36
-        angle_step = 2 * math.pi / n_angles
-        self.discrete_angles = torch.arange(0, 2 * math.pi, angle_step, device=self.device)
-        self.candidate_vectors = torch.stack([torch.cos(self.discrete_angles), torch.sin(self.discrete_angles)], dim=1)
-        # Assign object IDs based on name
-
-    
-    def update_prims(self):
-        pass
-    
-    def get_scene_data_dict(self):
-        return {"positions": self.positions, "sizes": self.sizes.expand(self.num_envs, -1, -1), "radii": self.radii.expand(self.num_envs, -1), "active": self.active, "on_surface_idx": self.on_surface_idx, "surface_level": self.surface_level}
-    
-    def apply_fixed_positions(self, env_ids: torch.Tensor, positions_config: list[dict]):
-        """
-        positions_config: список словарей по числу сред.
-        Каждый словарь: { "chair": [[x,y,z], ...], "table": [...], ... }
-        """
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        scene_data = self.get_scene_data_dict()
-        for env_id in env_ids:
-            env_dict = positions_config[env_id.item()]
-            for obj_name, pos_list in env_dict.items():
-                if obj_name not in self.object_map:
-                    continue
-                indices = self.object_map[obj_name]["indices"]
-                # print(indices)
-                for i, pos in enumerate(pos_list):
-                    if i >= len(indices):
-                        print("errror")
-                        break
-                    scene_data["positions"][env_id.item(), indices[i]] = torch.tensor(pos, device=self.device)
-                    scene_data["active"][env_id.item(), indices[i]] = True
-                    scene_data["on_surface_idx"][env_id.item(), indices[i]] = -1
-                    scene_data["surface_level"][env_id.item(), indices[i]] = 0
-
-        # for i in env_ids:
-        #     self.print_graph_info(i)
-        self.chose_active_goal_state(env_ids)
-
-
-    def _initialize_object_data(self):
-        """Заполняет метаданные об объектах и их начальные/дефолтные состояния."""
-        start_idx = 0
-        
-        # Создаем временный тензор для дефолтных позиций
-        default_pos_tensor = torch.zeros(1, self.num_total_objects, 3, device=self.device)
-        
-        # --- Начало: Логика создания "кладбища" ---
-        graveyard_start_x = 0.0
-        graveyard_start_y = 6.0
-        spacing = 1.0 # Расстояние между объектами на кладбище
-        max_per_row = 5 # Сколько объектов в ряду на кладбище
-
-        for i in range(self.num_total_objects):
-            row = i // max_per_row
-            col = i % max_per_row
-            default_pos_tensor[0, i, 0] = graveyard_start_x + col * spacing
-            default_pos_tensor[0, i, 1] = graveyard_start_y + row * spacing
-            default_pos_tensor[0, i, 2] = 0.0
-        # --- Конец: Логика создания "кладбища" ---
-
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            count = obj_cfg['count']
-            indices = torch.arange(start_idx, start_idx + count, device=self.device, dtype=torch.long)
-            types = set(obj_cfg['type'])
-
-            if "changeable_color" in types:
-                colors_dict = self.colors_dict
-                color_names = list(colors_dict.keys())
-                for idx in indices:
-                    color_name = random.choice(color_names)
-                    self.colors[0, idx] = torch.tensor(colors_dict[color_name], device=self.device)
-
-            self.object_map[name] = {'indices': indices, 'types': types, 'count': count}
-            for type_str in types:
-                self.type_map[type_str].extend(indices.tolist())
-            
-            self.names.extend([f"{name}_{i}" for i in range(count)])
-            
-            size_tensor = torch.tensor(obj_cfg['size'], device=self.device)
-            self.sizes[0, indices] = size_tensor
-            self.radii[0, indices] = torch.norm(size_tensor[:2] / 2)
-            start_idx += count
-
-        for type_str, indices in self.type_map.items():
-            self.type_map[type_str] = torch.tensor(sorted(indices), device=self.device, dtype=torch.long)
-
-        # --- Исправленная последовательность ---
-        # 1. Присваиваем правильно созданные "кладбищенские" позиции
-        self.default_positions = default_pos_tensor.expand(self.num_envs, -1, -1)
-        id_map = {"table": 1, "bowl": 2, "chair": 3, "cabinet": 4}
-        for name, data in self.object_map.items():
-            obj_id = id_map.get(name, 0)  # Default to 0 for unmapped objects
-            self.object_ids[0, data['indices']] = obj_id
-        # 2. Инициализируем текущие позиции из дефолтных
-        self.positions = self.default_positions.clone()
-
-    def _initialize_strategies(self):
-        strategies = {}
-        for obj_cfg in self.config:
-            name = obj_cfg['name']
-            print("name: ", name)
-            placement_cfg_list = obj_cfg.get('placement')
-            if not placement_cfg_list: continue
-            placement_cfg = placement_cfg_list[0]
-            strategy_type = placement_cfg['strategy']
-            if strategy_type == 'grid':
-                strategies[name] = GridPlacement(self.device, placement_cfg['grid_coordinates'])
-            elif strategy_type == 'on_surface':
-                surface_indices = self.type_map.get(placement_cfg['surface_types'][0], torch.tensor([], dtype=torch.long))
-                strategies[name] = OnSurfacePlacement(self.device, surface_indices.tolist(), placement_cfg['margin'])
-        return strategies
-
-    def randomize_scene(self, env_ids: torch.Tensor, mess: bool = False, use_obstacles: bool = False, all_defoult: bool = False):
-        """Абстрактная, векторизованная рандомизация сцены на основе ТИПОВ объектов."""
-        num_to_randomize = len(env_ids)
-        
-        # 1. Сброс состояния
-        self.active[env_ids] = False
-        self.positions[env_ids] = self.default_positions[env_ids]
-        self.on_surface_idx[env_ids] = -1
-        self.surface_level[env_ids] = 0
-        if all_defoult:
-            return
-
-        # 2. Определение количества объектов для размещения по типам
-        num_surface_only = len(self.type_map.get("surface_only", []))
-        num_providers = len(self.type_map.get("surface_provider", []))
-        num_floor_obs = len(self.type_map.get("movable_obstacle", [])) - num_surface_only
-        num_static_floor_obs = len(self.type_map.get("staff_obstacle", [])) - num_surface_only
-
-        num_surface_only_to_place = torch.randint(1, num_surface_only + 1, (num_to_randomize,), device=self.device) if num_surface_only > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        
-        if num_providers > 0:
-            # Нижняя граница: минимум 1, и не меньше чем количество surface_only объектов.
-            low_bound = torch.max(
-                torch.tensor(1, device=self.device), 
-                num_surface_only_to_place
-            )
-            # Верхняя граница (исключающая для генерации)
-            high_bound = num_providers + 1
-            
-            # Генерируем случайные числа с плавающей точкой и масштабируем их до нужного диапазона
-            rand_float = torch.rand(num_to_randomize, device=self.device)
-            num_providers_to_place = (low_bound + rand_float * (high_bound - low_bound)).long()
-        else:
-            num_providers_to_place = torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_floor_obstacles_to_place = torch.randint(2, num_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-        num_static_floor_obstacles_to_place = torch.randint(0, num_static_floor_obs + 1, (num_to_randomize,), device=self.device) if use_obstacles and num_static_floor_obs > 0 else torch.zeros(num_to_randomize, dtype=torch.long, device=self.device)
-
-        # 3. Применение стратегий в правильном порядке (сначала поверхности)
-        scene_data = self.get_scene_data_dict()
-        placement_order = ["surface_provider", "movable_obstacle", "surface_only", "staff_obstacle"]
-        
-        # Определяем, сколько объектов какого типа нужно разместить
-        counts_by_type = {
-            "surface_provider": num_providers_to_place,
-            "movable_obstacle": num_floor_obstacles_to_place,
-            "surface_only": num_surface_only_to_place,
-            "staff_obstacle": num_static_floor_obstacles_to_place,
-        }
-        
-        for p_type in placement_order:
-            # Находим все объекты, имеющие данный тип
-            for name, data in self.object_map.items():
-                if p_type in data['types'] and name in self.placement_strategies:
-                    # Некоторые типы могут иметь несколько ролей, например, movable_obstacle может быть не surface_only
-                    # Проверяем, что мы еще не разместили этот объект в рамках другой роли
-                    if p_type == "movable_obstacle" and "surface_only" in data['types']:
-                        continue
-
-                    obj_indices = data['indices']
-                    num_to_place_per_env = counts_by_type.get(p_type, torch.zeros(num_to_randomize, dtype=torch.long))
-
-                    # Векторизованный выбор случайных экземпляров
-                    rand_indices = torch.rand(num_to_randomize, len(obj_indices), device=self.device).argsort(dim=1)
-                    
-                    max_num_to_place = int(num_to_place_per_env.max())
-                    if max_num_to_place == 0: continue
-                    
-                    indices_to_place = obj_indices[rand_indices[:, :max_num_to_place]]
-
-                    # Маска, чтобы применять стратегию только к тем env, где нужно разместить > 0 объектов
-                    valid_envs_mask = num_to_place_per_env > 0
-                    
-                    # Фильтруем env_ids и indices_to_place
-                    active_env_ids = env_ids[valid_envs_mask]
-                    if len(active_env_ids) == 0: continue
-
-                    active_indices_to_place = indices_to_place[valid_envs_mask]
-                    # print("active_indices_to_place ", active_indices_to_place)
-                    self.placement_strategies[name].apply(active_env_ids, active_indices_to_place, scene_data, mess)
-
-        self.chose_active_goal_state(env_ids)
-
-    def get_active_obstacle_positions_for_path_planning(self, env_ids: torch.Tensor) -> list:
-        """
-        Возвращает позиции активных препятствий в формате списка списков,
-        специально для генерации строкового ключа в path_manager.
-        """
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return [[] for _ in env_ids]
-            
-        active_mask = self.active[env_ids][:, obs_indices] # (num_envs, num_obstacles)
-        positions = self.positions[env_ids][:, obs_indices].cpu().numpy() # (num_envs, num_obstacles, 3)
-        
-        output_list = []
-        for i in range(len(env_ids)):
-            # Выбираем только активные позиции для i-й среды
-            active_positions = positions[i, active_mask[i].cpu().numpy()]
-            # Округляем и сортируем для консистентности ключа
-            rounded_pos = [(round(p[0], 1), round(p[1], 1), round(p[2], 1)) for p in active_positions]
-            output_list.append(sorted(rounded_pos))
-            
-        return output_list
-
-    def get_graph_embedding(self, env_ids: torch.Tensor) -> torch.Tensor:
-        """Создает тензорный эмбеддинг фиксированного размера для текущего состояния сцены."""
-        # [is_active, pos_x, pos_y, pos_z, size_x, size_y, size_z, radius, object_id]
-        # Размер фичи: 1 + 3 + 3 + 1 + 1 = 9
-        num_features = 9
-        embedding = torch.zeros(len(env_ids), self.num_total_objects, num_features, device=self.device)
-        # print("bbbb ", len(embedding[0]))
-        env_positions = self.positions[env_ids] + 5
-        env_active = self.active[env_ids].float().unsqueeze(-1)
-        env_sizes = self.sizes.expand(len(env_ids), -1, -1)
-        env_radii = self.radii.expand(len(env_ids), -1).unsqueeze(-1)
-        env_object_ids = self.object_ids.expand(len(env_ids), -1).unsqueeze(-1)
-
-        embedding[..., 0:1] = env_active
-        embedding[..., 1:4] = env_positions * env_active
-        embedding[..., 4:7] = env_sizes * env_active
-        embedding[..., 7:8] = env_radii * env_active
-        embedding[..., 8:9] = env_object_ids * env_active
-
-        # Нормализация для лучшего обучения (применяется ко всем, но неактивные останутся 0)
-        embedding[..., 1:4] /= 5.0  # Делим позиции на примерный масштаб комнаты
-        embedding[..., 4:7] /= 1.0  # Размеры уже примерно в этом диапазоне
-        embedding[..., 7:8] /= 2.0  # Радиусы
-        embedding[..., 8:9] /= 3.0  # Нормализация ID (максимум 3 для chair)
-        # Возвращаем "плоский" тензор
-        return embedding.view(len(env_ids), -1)
-
-    def print_graph_info(self, env_id: int):
-        """Печатает детальную информацию о сцене для ОДНОГО окружения."""
-        print(f"\n=== Scene Information (Env ID: {env_id}) ===")
-        
-        # Данные для указанного env_id
-        positions = self.positions[env_id]
-        active_states = self.active[env_id]
-        surface_indices = self.on_surface_idx[env_id]
-        surface_levels = self.surface_level[env_id]
-        
-        table_data = []
-        for i in range(self.num_total_objects):
-            name = self.names[i]
-            pos = positions[i]
-            # Ищем типы по индексу
-            types = ", ".join([t for t, inds in self.type_map.items() if i in inds])
-
-            row = [
-                i, name, types,
-                f"({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})",
-                f"{self.radii[0, i]:.2f}",
-                str(active_states[i].item()),
-                surface_indices[i].item(),
-                surface_levels[i].item()
-            ]
-            table_data.append(row)
-            
-        headers = ["ID", "Name", "Types", "Position", "Radius", "Active", "On Surface", "Surface Level"]
-        print(tabulate(table_data, headers=headers, tablefmt="grid"))
-    
-    def chose_active_goal_state(self, env_ids: torch.Tensor):
-        goal_indices = self.type_map.get("possible_goal", torch.tensor([], dtype=torch.long))
-        if len(goal_indices) == 0:
-            print(f"[WARNING] No objects of type 'possible_goal' found in config.")
-            self.goal_positions[env_ids] = torch.tensor([-4.5, 0.0, 0.75], device=self.device)
-            return
-        
-        active_goal_mask = self.active[env_ids][:, goal_indices].float()
-        
-        # Fallback если ни одна цель не активна
-        any_active = active_goal_mask.sum(dim=1) > 0
-        if not all(any_active):
-            print("NO GOAL", any_active)
-            # # Для env где нет активных целей, активируем первую попавшуюся
-            # fallback_mask = ~any_active
-            # active_goal_mask[fallback_mask, 0] = 1.0
-
-        chosen_goal_rel_idx = torch.multinomial(active_goal_mask + 1e-9, 1).squeeze(-1)
-        chosen_goal_idx = goal_indices[chosen_goal_rel_idx]
-        
-        env_indices = env_ids
-        self.goal_positions[env_indices] = self.positions[env_indices, chosen_goal_idx]
-
-    def get_active_goal_state(self, env_ids: torch.Tensor):
-        return self.goal_positions[env_ids]
-
-    def place_robot_for_goal(self, env_ids: torch.Tensor, mean_dist: float, min_dist: float, max_dist: float, angle_error: float):
-        """Размещает робота относительно цели, избегая препятствий и границ."""
-        # Этап 1: Получение числа сред
-        num_envs = len(env_ids)
-
-        # Этап 2: Извлечение позиций целей
-        goal_pos = self.goal_positions[env_ids]
-
-        # Этап 3: Определение активных препятствий на полу
-        is_floor_obstacle = (self.active[env_ids] == True) & (self.on_surface_idx[env_ids] == -1)
-
-        # Этап 4: Извлечение позиций и радиусов препятствий
-        obstacle_pos_all = self.positions[env_ids, :, :2].clone()
-
-        obstacle_radii_all = self.radii.expand(self.num_envs, -1)[env_ids]
-        # Этап 5: Фильтрация неактивных препятствий
-        inf_pos = torch.full_like(obstacle_pos_all, 999.0)
-
-        obstacle_pos = torch.where(is_floor_obstacle.unsqueeze(-1), obstacle_pos_all, inf_pos)
-        # Этап 6: Генерация радиусов для размещения робота
-        mean_dist_with_shift = mean_dist + 1.31
-        radii = torch.normal(mean=mean_dist_with_shift, std=mean_dist * 0.1, size=(num_envs, 1), device=self.device).clamp_(min_dist, max_dist)
-        # Этап 7: Генерация кандидатов для позиций робота
-        candidates = goal_pos[:, None, :2] + radii.unsqueeze(1) * self.candidate_vectors
-        # Этап 8: Проверка границ комнаты
-        # Этап 8: Проверка границ комнаты (только границы, без коллизий)
-        bounds = self.room_bounds
-        in_bounds_mask = (
-            (candidates[..., 0] >= bounds['x_min'] + self.robot_radius) &
-            (candidates[..., 0] <= bounds['x_max'] - self.robot_radius) &
-            (candidates[..., 1] >= bounds['y_min'] + self.robot_radius) &
-            (candidates[..., 1] <= bounds['y_max'] - self.robot_radius)
-        )
-        # print(candidates)
-        # print(in_bounds_mask)
-        # Этап 9: Выбор углов только по границам
-        in_bounds_mask_float = in_bounds_mask.float() + 1e-9
-        chosen_angle_idx = torch.multinomial(in_bounds_mask_float, 1).squeeze(-1)
-        # print(chosen_angle_idx)
-        # Этап 10: Выбор финальных позиций робота
-        batch_indices = torch.arange(num_envs, device=self.device)
-        final_robot_positions = candidates[batch_indices, chosen_angle_idx]
-
-        # Этап 11: fallback если ни одна позиция не в границах
-        no_valid_pos_mask = ~in_bounds_mask.any(dim=1)
-        if torch.any(no_valid_pos_mask):
-            fallback_pos = goal_pos[:, :2] + torch.tensor([max_dist, 0.0], device=self.device) # 0.0!
-            final_robot_positions[no_valid_pos_mask] = fallback_pos[no_valid_pos_mask]
-
-        # Этап 15: Вычисление ориентации робота (yaw)
-        direction_to_goal = goal_pos[:, :2] - final_robot_positions
-        base_yaw = torch.atan2(direction_to_goal[:, 1], direction_to_goal[:, 0])
-        error = (torch.rand(num_envs, device=self.device) - 0.5) * 2 * angle_error
-        final_yaw = base_yaw + error
-        # Этап 16: Формирование кватернионов ориентации
-        robot_quats = torch.zeros(num_envs, 4, device=self.device)
-        robot_quats[:, 0] = torch.cos(final_yaw / 2.0)
-        robot_quats[:, 3] = torch.sin(final_yaw / 2.0)
-        # Этап 17: Возврат результатов
-        # Проверяем пересечения с препятствиями
-        self.remove_colliding_obstacles(env_ids, final_robot_positions)
-
-        return final_robot_positions, robot_quats
-    
-
-    def remove_colliding_obstacles(self, env_ids: torch.Tensor, robot_positions: torch.Tensor):
-        """Ставит в дефолт все препятствия, пересекающиеся с роботом."""
-        # TODO There can be obstacles with suface providing and we should delete alse items on that
-        obs_indices = self.type_map.get("movable_obstacle", torch.tensor([], dtype=torch.long))
-        if len(obs_indices) == 0:
-            return
-
-        # позиции и радиусы препятствий
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r + 0.2)
-        if coll_mask.any():
-            # print("coll_mask: ",  coll_mask)
-            # for i in env_ids:
-            #     self.print_graph_info(i)
-            # переносим такие препятствия в дефолт
-            default_pos = self.default_positions[env_ids][:, obs_indices]
-            batch_idx, obs_idx = torch.where(coll_mask)                 # индексы элементов с коллизией
-            env_batch_idx = env_ids[batch_idx]                           # индексы env_ids для batch
-            obs_indices_sel = obs_indices[obs_idx]                       # индексы obstacles
-
-            # Присваиваем значения дефолтных позиций в исходный тензор
-            self.positions[env_batch_idx, obs_indices_sel] = default_pos[batch_idx, obs_idx]
-
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-            # print(default_pos[coll_mask])
-            
-            # self.positions[env_ids][:, obs_indices][coll_mask] = default_pos[coll_mask]
-            # print(self.positions[env_ids][:, obs_indices][coll_mask])
-
-            # деактивируем их
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-            self.active[env_batch_idx, obs_indices_sel] = False
-
-            # print(self.active[env_ids][:, obs_indices][coll_mask] )
-
-        obs_pos = self.positions[env_ids][:, obs_indices, :2]
-        obs_r = self.radii.expand(len(env_ids), -1)[:, obs_indices]
-
-        # расстояния от робота до препятствий
-        dists = torch.norm(obs_pos - robot_positions[:, None, :2], dim=2)
-        
-        coll_mask = dists < (self.robot_radius + obs_r)
-        if coll_mask.any():
-            
-            # print("coll_mask 2: ",  coll_mask)
-            
-            for i in env_ids:
-                self.print_graph_info(i)
-
-    def get_graph_obs(self, env_ids=None) -> dict[str, torch.Tensor]:
-        """Returns a dictionary with full tensorized graph representation for observations.
-        
-        - node_features: tensor (num_envs, num_objects, 14) - per object: [pos(3), size(3), radius(1), color(3), id(1), active(1), parent_id(1), level(1)].
-        - edge_features: tensor (num_envs, num_objects, 6) - per possible edge (from child to parent): [exists(1), z_diff(1), level_diff(1), dist(1), color_diff_norm(1), id_diff(1)]; 0 if no edge.
-        """
-        if env_ids is None:
-            env_ids = torch.arange(self.num_envs, device=self.device)
-        num_selected = len(env_ids)
-        
-        # Select data for env_ids
-        positions = self.positions[env_ids]/10  # (num_selected, num_objects, 3)
-        sizes = self.sizes.expand(num_selected, -1, -1)/10  # (num_selected, num_objects, 3)
-        radii = self.radii.expand(num_selected, -1).unsqueeze(-1)/10  # (num_selected, num_objects, 1)
-        colors = self.colors.expand(num_selected, -1, -1)/10  # (num_selected, num_objects, 3)
-        object_ids = self.object_ids.expand(num_selected, -1).unsqueeze(-1).float()/10  # (num_selected, num_objects, 1)
-        active = self.active[env_ids].unsqueeze(-1).float()/10  # (num_selected, num_objects, 1)
-        parents = self.on_surface_idx[env_ids].unsqueeze(-1).float()/10  # (num_selected, num_objects, 1); -1 for no parent
-        levels = self.surface_level[env_ids].unsqueeze(-1).float()/10  # (num_selected, num_objects, 1)
-        
-        # Node features: concat all per object
-        node_features = torch.cat([
-            positions, sizes, radii, colors, object_ids, active, parents, levels
-        ], dim=-1)  # (num_selected, num_objects, 14)
-        
-        # Edge features: per object (potential edge to parent)
-        edge_exists = (parents >= 0).float()  # (num_selected, num_objects, 1)
-        
-        # For valid parents: z_diff = child_z - parent_z
-        valid_mask = (parents >= 0).squeeze(-1)  # (num_selected, num_objects)
-        z_diff = torch.zeros(num_selected, self.num_total_objects, 1, device=self.device)
-        batch_idx = torch.arange(num_selected, device=self.device)[:, None].expand(-1, self.num_total_objects)[valid_mask]
-        obj_idx = torch.arange(self.num_total_objects, device=self.device)[None, :].expand(num_selected, -1)[valid_mask]
-        parent_idx = parents.squeeze(-1)[valid_mask].long()
-        z_diff[valid_mask] = positions[batch_idx, obj_idx, 2:3] - positions[batch_idx, parent_idx, 2:3]
-        
-        # level_diff = child_level - parent_level (should be 1 usually)
-        level_diff = torch.zeros_like(z_diff)
-        level_diff[valid_mask] = levels[batch_idx, obj_idx] - levels[batch_idx, parent_idx]
-        
-        # dist = norm(child_pos_xy - parent_pos_xy)
-        dist = torch.zeros_like(z_diff)
-        child_xy = positions[batch_idx, obj_idx, :2]
-        parent_xy = positions[batch_idx, parent_idx, :2]
-        dist[valid_mask] = torch.norm(child_xy - parent_xy, dim=-1, keepdim=True)
-        
-        # color_diff_norm = norm(child_color - parent_color)
-        color_diff_norm = torch.zeros_like(z_diff)
-        child_color = colors[batch_idx, obj_idx]
-        parent_color = colors[batch_idx, parent_idx]
-        color_diff_norm[valid_mask] = torch.norm(child_color - parent_color, dim=-1, keepdim=True)
-        
-        # id_diff = child_id - parent_id
-        id_diff = torch.zeros_like(z_diff)
-        child_id = object_ids[batch_idx, obj_idx]
-        parent_id = object_ids[batch_idx, parent_idx]
-        id_diff[valid_mask] = child_id - parent_id
-        
-        # Edge features: [exists, z_diff, level_diff, dist, color_diff_norm, id_diff]
-        edge_features = torch.cat([edge_exists, z_diff, level_diff, dist, color_diff_norm, id_diff], dim=-1)  # (num_selected, num_objects, 6)
-        
-        return {"node_features": node_features, "edge_features": edge_features}
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/ant/__pycache__/__init__.cpython-311.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/ant/__pycache__/__init__.cpython-311.pyc
index d6dc6c4350..21e4e64b19 100644
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/ant/__pycache__/__init__.cpython-311.pyc and b/source/isaaclab_tasks/isaaclab_tasks/direct/ant/__pycache__/__init__.cpython-311.pyc differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/__pycache__/__init__.cpython-311.pyc b/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/__pycache__/__init__.cpython-311.pyc
index 97c946c54a..674d0c9dd3 100644
Binary files a/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/__pycache__/__init__.cpython-311.pyc and b/source/isaaclab_tasks/isaaclab_tasks/direct/cartpole/__pycache__/__init__.cpython-311.pyc differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
index c078bc3e5d..0f318ce98d 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
@@ -30,6 +30,7 @@ def position_command_error(env: ManagerBasedRLEnv, command_name: str, asset_cfg:
     des_pos_b = command[:, :3]
     des_pos_w, _ = combine_frame_transforms(asset.data.root_pos_w, asset.data.root_quat_w, des_pos_b)
     curr_pos_w = asset.data.body_pos_w[:, asset_cfg.body_ids[0]]  # type: ignore
+
     return torch.norm(curr_pos_w - des_pos_w, dim=1)
 
 
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py
index 8890010a71..63b66d1a2c 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/reach_env_cfg.py
@@ -40,16 +40,16 @@ class ReachSceneCfg(InteractiveSceneCfg):
     ground = AssetBaseCfg(
         prim_path="/World/ground",
         spawn=sim_utils.GroundPlaneCfg(),
-        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, -1.05)),
+        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.0, 0.0, 0.0)),
     )
 
-    table = AssetBaseCfg(
-        prim_path="{ENV_REGEX_NS}/Table",
-        spawn=sim_utils.UsdFileCfg(
-            usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd",
-        ),
-        init_state=AssetBaseCfg.InitialStateCfg(pos=(0.55, 0.0, 0.0), rot=(0.70711, 0.0, 0.0, 0.70711)),
-    )
+    # table = AssetBaseCfg(
+    #     prim_path="{ENV_REGEX_NS}/Table",
+    #     spawn=sim_utils.UsdFileCfg(
+    #         usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd",
+    #     ),
+    #     init_state=AssetBaseCfg.InitialStateCfg(pos=(0.55, 0.0, 0.0), rot=(0.70711, 0.0, 0.0, 0.70711)),
+    # )
 
     # robots
     robot: ArticulationCfg = MISSING
@@ -76,12 +76,12 @@ class CommandsCfg:
         resampling_time_range=(4.0, 4.0),
         debug_vis=True,
         ranges=mdp.UniformPoseCommandCfg.Ranges(
-            pos_x=(0.35, 0.65),
+            pos_x=(0.8,0.9),
             pos_y=(-0.2, 0.2),
-            pos_z=(0.15, 0.5),
+            pos_z=(0.7, 0.8),
             roll=(0.0, 0.0),
             pitch=MISSING,  # depends on end-effector axis
-            yaw=(-3.14, 3.14),
+            yaw=(0, 0),
         ),
     )
 
@@ -116,6 +116,27 @@ class ObservationsCfg:
     policy: PolicyCfg = PolicyCfg()
 
 
+# @configclass
+# class EventCfg:
+#     """Configuration for events."""
+
+#     reset_robot_joints = EventTerm(
+#         func=mdp.reset_root_state_uniform,
+#         mode="reset",
+#         params={
+#             "pose_range": {
+#                 "x": (0.1, 0.1),      
+#                 "y": (0.1, 0.1), 
+#                 "z": (0.2, 0.2)
+#                 # "roll":(3.14/2,3.14/2)     
+
+#         },
+#         "velocity_range": {
+#         "x": (0.0, 0.0),      # линейная скорость по X (м/с)
+#         "y": (0.0, 0.0),      # линейная скорость по Y (м/с)
+#         "z": (0.0, 0.0),      # линейная скорость по Z (м/с)
+#     }}
+#     )
 @configclass
 class EventCfg:
     """Configuration for events."""
@@ -129,7 +150,6 @@ class EventCfg:
         },
     )
 
-
 @configclass
 class RewardsCfg:
     """Reward terms for the MDP."""
@@ -137,17 +157,17 @@ class RewardsCfg:
     # task terms
     end_effector_position_tracking = RewTerm(
         func=mdp.position_command_error,
-        weight=-0.2,
+        weight=-0.15,
         params={"asset_cfg": SceneEntityCfg("robot", body_names=MISSING), "command_name": "ee_pose"},
     )
     end_effector_position_tracking_fine_grained = RewTerm(
         func=mdp.position_command_error_tanh,
-        weight=0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", body_names=MISSING), "std": 0.1, "command_name": "ee_pose"},
+        weight=0.15,
+        params={"asset_cfg": SceneEntityCfg("robot", body_names=MISSING), "std": 0.3, "command_name": "ee_pose"},
     )
     end_effector_orientation_tracking = RewTerm(
         func=mdp.orientation_command_error,
-        weight=-0.1,
+        weight=-0.3,
         params={"asset_cfg": SceneEntityCfg("robot", body_names=MISSING), "command_name": "ee_pose"},
     )
 
@@ -155,7 +175,7 @@ class RewardsCfg:
     action_rate = RewTerm(func=mdp.action_rate_l2, weight=-0.0001)
     joint_vel = RewTerm(
         func=mdp.joint_vel_l2,
-        weight=-0.0001,
+        weight=-0.001,
         params={"asset_cfg": SceneEntityCfg("robot")},
     )
 
@@ -170,13 +190,12 @@ class TerminationsCfg:
 @configclass
 class CurriculumCfg:
     """Curriculum terms for the MDP."""
-
     action_rate = CurrTerm(
-        func=mdp.modify_reward_weight, params={"term_name": "action_rate", "weight": -0.005, "num_steps": 4500}
+        func=mdp.modify_reward_weight, params={"term_name": "action_rate", "weight": -0.05, "num_steps": 4000}
     )
 
     joint_vel = CurrTerm(
-        func=mdp.modify_reward_weight, params={"term_name": "joint_vel", "weight": -0.001, "num_steps": 4500}
+        func=mdp.modify_reward_weight, params={"term_name": "joint_vel", "weight": -0.01, "num_steps": 4000}
     )
 
 
@@ -206,24 +225,24 @@ class ReachEnvCfg(ManagerBasedRLEnvCfg):
         # general settings
         self.decimation = 2
         self.sim.render_interval = self.decimation
-        self.episode_length_s = 12.0
+        self.episode_length_s = 4.0
         self.viewer.eye = (3.5, 3.5, 3.5)
         # simulation settings
         self.sim.dt = 1.0 / 60.0
 
-        self.teleop_devices = DevicesCfg(
-            devices={
-                "keyboard": Se3KeyboardCfg(
-                    gripper_term=False,
-                    sim_device=self.sim.device,
-                ),
-                "gamepad": Se3GamepadCfg(
-                    gripper_term=False,
-                    sim_device=self.sim.device,
-                ),
-                "spacemouse": Se3SpaceMouseCfg(
-                    gripper_term=False,
-                    sim_device=self.sim.device,
-                ),
-            },
-        )
+        # self.teleop_devices = DevicesCfg(
+        #     devices={
+        #         "keyboard": Se3KeyboardCfg(
+        #             gripper_term=False,
+        #             sim_device=self.sim.device,
+        #         ),
+        #         "gamepad": Se3GamepadCfg(
+        #             gripper_term=False,
+        #             sim_device=self.sim.device,
+        #         ),
+        #         "spacemouse": Se3SpaceMouseCfg(
+        #             gripper_term=False,
+        #             sim_device=self.sim.device,
+        #         ),
+        #     },
+        # )
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__init__.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__init__.py
index 6afd5be019..1daa308814 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__init__.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__init__.py
@@ -5,4 +5,5 @@
 
 """Navigation environments."""
 
-from .config import anymal_c
+from .config import anymal_c 
+from .config import husky
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__pycache__/__init__.cpython-311.pyc b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__pycache__/__init__.cpython-311.pyc
index a1c3bc63cb..9bbdb990a8 100644
Binary files a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__pycache__/__init__.cpython-311.pyc and b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/__pycache__/__init__.cpython-311.pyc differ
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/__pycache__/__init__.cpython-311.pyc b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/__pycache__/__init__.cpython-311.pyc
index 61be6fe733..a507ec8777 100644
Binary files a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/__pycache__/__init__.cpython-311.pyc and b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/__pycache__/__init__.cpython-311.pyc differ